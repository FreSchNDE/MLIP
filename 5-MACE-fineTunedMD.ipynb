{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57a883f2-b6b6-4807-8ebd-fd481651b4a2",
   "metadata": {},
   "source": [
    "# MD Fine tuned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75870c8a-14c1-4a77-aa29-29e81ab9fe17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ase.io import read, write\n",
    "db = read('data/solvent_xtb.xyz', ':')\n",
    "write('data/solvent_xtb_train_400.xyz', db[:403]) #first 400 configs plus the 3 E0s\n",
    "write('data/solvent_xtb_test.xyz', db[-1000:]) #last 1000 configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b6ab8ccf-bc97-4f97-9d3f-becaba08cf15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n",
      "2025-06-22 11:57:47.929 INFO: MACE version: 0.3.6\n",
      "2025-06-22 11:57:47.929 INFO: Configuration: Namespace(config=None, name='finetuned_MACE', seed=3, log_dir='logs', model_dir='.', checkpoints_dir='checkpoints', results_dir='results', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=5.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=3, correlation=3, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=None, max_L=None, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=1.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=False, start_swa=None, ema=True, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model='small', foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-22 11:57:48.038 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-22 11:57:48.204 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-22 11:57:48.204 INFO: Using foundation model mace-mp-0 small as initial checkpoint.\n",
      "Using Materials Project MACE for MACECalculator with /home/user4/.cache/mace/46jrkm3v\n",
      "Using float64 for MACECalculator, which is slower but more accurate. Recommended for geometry optimization.\n",
      "/usr/local/lib/python3.10/dist-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "2025-06-22 11:57:48.435 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-22 11:57:48.509 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-22 11:57:48.509 INFO: Using isolated atom energies from training file\n",
      "2025-06-22 11:57:48.514 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-22 11:57:48.514 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-22 11:57:48.793 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-22 11:57:48.906 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-22 11:57:48.906 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-22 11:57:48.907 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-22 11:57:48.907 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-22 11:57:49.017 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=1.000)\n",
      "2025-06-22 11:57:49.060 INFO: Average number of neighbors: 15.98296224588577\n",
      "2025-06-22 11:57:49.060 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-22 11:57:49.099 INFO: Building model\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "2025-06-22 11:57:50.186 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 128x0e | 384 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=6.0, num_basis=10, trainable=True)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=6.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0-1): 2 x RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "      (conv_tp): TensorProduct(128x0e x 1x0e+1x1o+1x2e+1x3o -> 128x0e+128x1o+128x2e+128x3o | 512 paths | 512 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[10, 64, 64, 64, 512]\n",
      "      (linear): Linear(128x0e+128x1o+128x2e+128x3o -> 128x0e+128x1o+128x2e+128x3o | 65536 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(128x0e x 3x0e -> 128x0e | 49152 paths | 49152 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x128 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x128 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(128x0e -> 1x0e | 128 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(128x0e -> 16x0e | 2048 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=0.804154, shift=0.164097)\n",
      ")\n",
      "2025-06-22 11:57:50.188 INFO: Number of parameters: 402202\n",
      "2025-06-22 11:57:50.188 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-22 11:57:50.188 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-22 11:57:50.188 INFO: Started training\n",
      "2025-06-22 11:57:50.914 INFO: Epoch None: loss=3.9875, RMSE_E_per_atom=6262.6 meV, RMSE_F=802.8 meV / A\n",
      "2025-06-22 11:58:08.223 INFO: Epoch 0: loss=0.0546, RMSE_E_per_atom=243.5 meV, RMSE_F=692.8 meV / A\n",
      "2025-06-22 11:58:15.255 INFO: Epoch 2: loss=0.0078, RMSE_E_per_atom=63.0 meV, RMSE_F=267.6 meV / A\n",
      "2025-06-22 11:58:20.743 INFO: Epoch 4: loss=0.0047, RMSE_E_per_atom=34.8 meV, RMSE_F=208.6 meV / A\n",
      "2025-06-22 11:58:26.313 INFO: Epoch 6: loss=0.0036, RMSE_E_per_atom=24.5 meV, RMSE_F=183.7 meV / A\n",
      "2025-06-22 11:58:31.888 INFO: Epoch 8: loss=0.0029, RMSE_E_per_atom=18.7 meV, RMSE_F=166.5 meV / A\n",
      "2025-06-22 11:58:37.471 INFO: Epoch 10: loss=0.0025, RMSE_E_per_atom=15.8 meV, RMSE_F=153.3 meV / A\n",
      "2025-06-22 11:58:43.062 INFO: Epoch 12: loss=0.0022, RMSE_E_per_atom=12.7 meV, RMSE_F=145.1 meV / A\n",
      "2025-06-22 11:58:48.790 INFO: Epoch 14: loss=0.0020, RMSE_E_per_atom=10.7 meV, RMSE_F=138.3 meV / A\n",
      "2025-06-22 11:58:55.639 INFO: Epoch 16: loss=0.0019, RMSE_E_per_atom=9.2 meV, RMSE_F=133.1 meV / A\n",
      "2025-06-22 11:59:03.205 INFO: Epoch 18: loss=0.0017, RMSE_E_per_atom=7.2 meV, RMSE_F=128.8 meV / A\n",
      "2025-06-22 11:59:09.857 INFO: Epoch 20: loss=0.0017, RMSE_E_per_atom=6.6 meV, RMSE_F=125.4 meV / A\n",
      "2025-06-22 11:59:16.232 INFO: Epoch 22: loss=0.0016, RMSE_E_per_atom=6.3 meV, RMSE_F=122.3 meV / A\n",
      "2025-06-22 11:59:22.390 INFO: Epoch 24: loss=0.0015, RMSE_E_per_atom=6.2 meV, RMSE_F=119.7 meV / A\n",
      "2025-06-22 11:59:28.223 INFO: Epoch 26: loss=0.0015, RMSE_E_per_atom=6.1 meV, RMSE_F=117.4 meV / A\n",
      "2025-06-22 11:59:34.505 INFO: Epoch 28: loss=0.0014, RMSE_E_per_atom=6.0 meV, RMSE_F=115.0 meV / A\n",
      "2025-06-22 11:59:42.205 INFO: Epoch 30: loss=0.0013, RMSE_E_per_atom=6.1 meV, RMSE_F=112.9 meV / A\n",
      "2025-06-22 11:59:50.235 INFO: Epoch 32: loss=0.0013, RMSE_E_per_atom=6.2 meV, RMSE_F=111.2 meV / A\n",
      "2025-06-22 11:59:57.290 INFO: Epoch 34: loss=0.0013, RMSE_E_per_atom=6.3 meV, RMSE_F=109.5 meV / A\n",
      "2025-06-22 12:00:03.703 INFO: Epoch 36: loss=0.0012, RMSE_E_per_atom=6.5 meV, RMSE_F=107.8 meV / A\n",
      "2025-06-22 12:00:10.243 INFO: Epoch 38: loss=0.0012, RMSE_E_per_atom=6.6 meV, RMSE_F=106.5 meV / A\n",
      "2025-06-22 12:00:16.103 INFO: Epoch 40: loss=0.0012, RMSE_E_per_atom=6.4 meV, RMSE_F=105.3 meV / A\n",
      "2025-06-22 12:00:21.948 INFO: Epoch 42: loss=0.0011, RMSE_E_per_atom=6.3 meV, RMSE_F=103.7 meV / A\n",
      "2025-06-22 12:00:28.954 INFO: Epoch 44: loss=0.0011, RMSE_E_per_atom=6.3 meV, RMSE_F=101.8 meV / A\n",
      "2025-06-22 12:00:37.549 INFO: Epoch 46: loss=0.0011, RMSE_E_per_atom=6.4 meV, RMSE_F=100.9 meV / A\n",
      "2025-06-22 12:00:46.269 INFO: Epoch 48: loss=0.0011, RMSE_E_per_atom=6.2 meV, RMSE_F=100.2 meV / A\n",
      "2025-06-22 12:00:50.287 INFO: Training complete\n",
      "2025-06-22 12:00:50.287 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-22 12:00:50.784 INFO: Loading checkpoint: checkpoints/finetuned_MACE_run-3_epoch-48.pt\n",
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n",
      "2025-06-22 12:00:50.941 INFO: Loaded model from epoch 48\n",
      "2025-06-22 12:00:50.942 INFO: Evaluating train ...\n",
      "2025-06-22 12:00:51.915 INFO: Evaluating valid ...\n",
      "2025-06-22 12:00:52.038 INFO: Evaluating Default ...\n",
      "2025-06-22 12:00:57.391 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         5.5         |       99.5       |        4.35       |\n",
      "|    valid    |         6.2         |      100.2       |        5.99       |\n",
      "|   Default   |         6.4         |      152.2       |        6.65       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-22 12:00:57.391 INFO: Saving model to checkpoints/finetuned_MACE_run-3.model\n",
      "2025-06-22 12:00:57.551 INFO: Compiling model, saving metadata to finetuned_MACE_compiled.model\n",
      "2025-06-22 12:00:58.183 INFO: Done\n",
      "Fine-tuning time: 193.65 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/calculators/mace.py:128: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=model_path, map_location=device)\n",
      "/usr/local/lib/python3.10/dist-packages/torch/serialization.py:1074: UserWarning: 'torch.load' received a zip file that looks like a TorchScript archive dispatching to 'torch.jit.load' (call 'torch.jit.load' directly to silence this warning)\n",
      "  warnings.warn(\"'torch.load' received a zip file that looks like a TorchScript archive\"\n",
      "/tmp/ipykernel_2518140/119758153.py:44: DeprecationWarning: Please use atoms.calc = calc\n",
      "  conf.set_calculator(calc)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "test\n",
      "Saved moldyn/comparison_rdf_HO_inter.png\n",
      "Saved moldyn/comparison_rdf_OO_inter.png\n",
      "Saved moldyn/comparison_rdf_CC_inter.png\n",
      "✔  wrote moldyn/msd_mace_input2.dat  with 501 lines\n",
      "✔  wrote moldyn/_temp.png\n",
      "✔  wrote moldyn/msd_mace_finetuned.dat  with 501 lines\n",
      "✔  wrote moldyn/_temp.png\n"
     ]
    }
   ],
   "source": [
    "# 5-MACE-fineTunedMD.ipynb\n",
    "\n",
    "import time\n",
    "from ase.io import read\n",
    "from mace.calculators import MACECalculator\n",
    "from aseMolec import anaAtoms as aa\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "\n",
    "# === 1. Fine-tune model with mace_run_train ===\n",
    "start = time.time()\n",
    "\n",
    "!mace_run_train \\\n",
    "    --name=\"finetuned_MACE\" \\\n",
    "    --foundation_model=\"small\" \\\n",
    "    --train_file=\"data/solvent_xtb_train_200.xyz\" \\\n",
    "    --valid_fraction=0.10 \\\n",
    "    --test_file=\"data/solvent_xtb_test.xyz\" \\\n",
    "    --energy_weight=1.0 \\\n",
    "    --forces_weight=1.0 \\\n",
    "    --E0s=\"average\" \\\n",
    "    --energy_key=\"energy_xtb\" \\\n",
    "    --forces_key=\"forces_xtb\" \\\n",
    "    --lr=0.01 \\\n",
    "    --scaling=\"rms_forces_scaling\" \\\n",
    "    --batch_size=10 \\\n",
    "    --max_num_epochs=50 \\\n",
    "    --ema \\\n",
    "    --ema_decay=0.99 \\\n",
    "    --amsgrad \\\n",
    "    --default_dtype=\"float64\" \\\n",
    "    --device=cuda \\\n",
    "    --seed=3\n",
    "\n",
    "end = time.time()\n",
    "print(f\"Fine-tuning time: {end - start:.2f} seconds\")\n",
    "\n",
    "# === 2. Run MD with fine-tuned model on the same liquid config ===\n",
    "\n",
    "# Load the fine-tuned model\n",
    "calc_finetuned = MACECalculator(\n",
    "    model_paths=[\"finetuned_MACE_compiled.model\"],\n",
    "    device=\"cuda\", default_dtype=\"float64\"\n",
    ")\n",
    "\n",
    "# Load the input2.xyz configuration\n",
    "init_conf = read(\"data/input2.xyz\")\n",
    "init_conf.center()\n",
    "\n",
    "# Function to run MD (same as in 4-MACE-MD)\n",
    "# Define the MD function (copied from 4-MACE-MD)\n",
    "def run_md(name, init_conf, temp, calc, steps, interval):\n",
    "    from ase import units\n",
    "    from ase.md.langevin import Langevin\n",
    "    from ase.md.velocitydistribution import Stationary, ZeroRotation, MaxwellBoltzmannDistribution\n",
    "    import os\n",
    "    import numpy as np\n",
    "    from ase.io import write\n",
    "\n",
    "    output_dir = \"moldyn\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    conf = init_conf.copy()\n",
    "    conf.set_calculator(calc)\n",
    "\n",
    "    # Initialize velocities and remove translation/rotation\n",
    "    MaxwellBoltzmannDistribution(conf, temperature_K=300)\n",
    "    Stationary(conf)\n",
    "    ZeroRotation(conf)\n",
    "\n",
    "    dyn = Langevin(conf, 1.0 * units.fs, temperature_K=temp, friction=0.1)\n",
    "    traj_file = os.path.join(output_dir, f\"{name}.xyz\")\n",
    "    if os.path.exists(traj_file):\n",
    "        os.remove(traj_file)\n",
    "\n",
    "    time_fs, temperatures, energies = [], [], []\n",
    "\n",
    "    def log():\n",
    "        print(\"test\")\n",
    "        dyn.atoms.write(traj_file, append=True)\n",
    "        time_fs.append(dyn.get_time() / units.fs)\n",
    "        temperatures.append(dyn.atoms.get_temperature())\n",
    "        energies.append(dyn.atoms.get_potential_energy() / len(dyn.atoms))\n",
    "\n",
    "    dyn.attach(log, interval=interval)\n",
    "    dyn.run(steps)\n",
    "\n",
    "    return np.array(time_fs), np.array(temperatures), np.array(energies)\n",
    "\n",
    "mace_t, mace_temp, mace_E = run_md(\n",
    "    name=\"mace_finetuned_md\", \n",
    "    init_conf=init_conf, \n",
    "    temp=500, \n",
    "    calc=calc_finetuned, \n",
    "    steps=5000, \n",
    "    interval=10\n",
    ")\n",
    "\n",
    "# === 3. RDF comparison plots ===\n",
    "\n",
    "# Read trajectories (skip initial 50 frames)\n",
    "traj_base = read(\"moldyn/mace_md_input2.xyz\", \"50:\")\n",
    "traj_finetuned = read(\"moldyn/mace_finetuned_md.xyz\", \"50:\")\n",
    "\n",
    "# Assign fake periodic box\n",
    "for traj in [traj_base, traj_finetuned]:\n",
    "    for at in traj:\n",
    "        at.pbc = True\n",
    "        at.cell = [100, 100, 100]\n",
    "\n",
    "# Tags to plot\n",
    "tags = ['HO_inter', 'OO_inter', 'CC_inter']\n",
    "\n",
    "# Plot RDF comparison\n",
    "for tag in tags:\n",
    "    rdf_base = aa.compute_rdfs_traj_avg(traj_base, rmax=5, nbins=50)\n",
    "    rdf_ft = aa.compute_rdfs_traj_avg(traj_finetuned, rmax=5, nbins=50)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.plot(rdf_base[1], rdf_base[0][tag], label='MACE-4000', linewidth=2)\n",
    "    plt.plot(rdf_ft[1], rdf_ft[0][tag], label='Fine-tuned', linewidth=2)\n",
    "    plt.xlabel(r'R ($\\rm \\AA$)')\n",
    "    plt.ylabel(f'RDF {tag}')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"moldyn/comparison_rdf_{tag}.png\", dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"Saved moldyn/comparison_rdf_{tag}.png\")\n",
    "\n",
    "# === 4. MSD comparison ===\n",
    "\n",
    "# Run MSD script on both trajectories\n",
    "!python3 MSD.py moldyn/mace_md_input2.xyz --out moldyn/msd_mace_input2.dat --dt 1 --png moldyn/_temp.png --skip 1\n",
    "!python3 MSD.py moldyn/mace_finetuned_md.xyz --out moldyn/msd_mace_finetuned.dat --dt 1 --png moldyn/_temp.png --skip 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18e5f7e2-ca33-4a1a-9cd5-07c8bbc6f398",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved moldyn/comparison_msd_input2.png\n"
     ]
    }
   ],
   "source": [
    "# Load MSDs from output\n",
    "def load_msd(file):\n",
    "    times, rmsds, msds = [], [], []\n",
    "    with open(file, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.strip() and not line.startswith('#'):\n",
    "                t, rmsd, msd = map(float, line.split())\n",
    "                times.append(t * 1000)  # convert from ps to fs\n",
    "                msds.append(msd)\n",
    "    return times, msds\n",
    "\n",
    "\n",
    "t_4000, msd_4000 = load_msd(\"moldyn/msd_mace_input2.dat\")\n",
    "t_ft, msd_ft = load_msd(\"moldyn/msd_mace_finetuned.dat\")\n",
    "\n",
    "\n",
    "# Plot MSD comparison\n",
    "plt.figure()\n",
    "plt.plot(list(t_4000), list(msd_4000), label='MACE-4000', linewidth=2)\n",
    "plt.plot(list(t_ft), list(msd_ft), label='Fine-tuned', linewidth=2)\n",
    "plt.xlabel('Time [fs]')\n",
    "plt.ylabel('MSD [Å²]')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"moldyn/comparison_msd_input2.png\", dpi=300)\n",
    "plt.close()\n",
    "print(\"Saved moldyn/comparison_msd_input2.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2760c09b-e66c-4c8c-abc1-e29fb4a0dfe8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
