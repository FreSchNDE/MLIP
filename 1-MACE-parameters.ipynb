{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516e26a9",
   "metadata": {},
   "source": [
    "# GPU Performance & Accuracy Benchmarking\n",
    "\n",
    "This script systematically tests how different MACE model parameters affect training time, GPU utilization, and model accuracy. We vary parameters like `batch_size`, `num_channels`, `num_interactions`, `max_L`, and `correlation`.\n",
    "\n",
    "Each configuration is trained on:\n",
    "- `data/solvent_xtb_train_200.xyz`\n",
    "- `data/solvent_xtb_test.xyz`\n",
    "\n",
    "**Metrics collected:**\n",
    "- Training duration\n",
    "- Final validation error (RMSE_E and RMSE_F)\n",
    "- GPU memory usage (via `nvidia-smi`)\n",
    "- GPU utilization (average)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f435756-e921-408c-b584-1404af51f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = read('data/solvent_xtb.xyz', ':')\n",
    "# write('data/solvent_xtb_train_200.xyz', db[:203]) #first 200 configs plus the 3 E0s\n",
    "\n",
    "# write('data/solvent_xtb_test.xyz', db[-1000:]) #last 1000 configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a87125d-09ec-466d-99fc-c8d100b277d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import yaml\n",
    "import re\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "Path(\"benchmark_results\").mkdir(exist_ok=True)\n",
    "Path(\"MACE_models\").mkdir(exist_ok=True)\n",
    "\n",
    "base_config = {\n",
    "    'model': 'MACE',\n",
    "    'num_interactions': 2,\n",
    "    'num_channels': 32,\n",
    "    'max_L': 0,\n",
    "    'correlation': 2,\n",
    "    'r_max': 4.0,\n",
    "    'max_ell': 2,\n",
    "    'model_dir': 'MACE_models',\n",
    "    'log_dir': 'MACE_models',\n",
    "    'checkpoints_dir': 'MACE_models',\n",
    "    'results_dir': 'MACE_models',\n",
    "    'train_file': 'data/solvent_xtb_train_200.xyz',\n",
    "    'valid_fraction': 0.10,\n",
    "    'test_file': 'data/solvent_xtb_test.xyz',\n",
    "    'E0s': 'average',\n",
    "    'energy_key': 'energy_xtb',\n",
    "    'forces_key': 'forces_xtb',\n",
    "    'device': 'cuda',\n",
    "    'max_num_epochs': 50,\n",
    "    'swa': True,\n",
    "    'seed': 123\n",
    "}\n",
    "\n",
    "configs_to_test = [\n",
    "    {'name': 'best_guess_v1', 'batch_size': 10, 'num_channels': 64, 'num_interactions': 3, 'correlation': 3},\n",
    "    {'name': 'best_guess_v2', 'batch_size': 10, 'num_channels': 64, 'num_interactions': 3, 'correlation': 3, 'max_L': 1},\n",
    "    {'name': 'best_guess_v3', 'batch_size': 10, 'num_channels': 64, 'num_interactions': 3, 'correlation': 3, 'max_L': 2},\n",
    "    \n",
    "    {'name': 'baseline', 'batch_size': 10},\n",
    "    {'name': 'batch32', 'batch_size': 32},\n",
    "    {'name': 'batch64', 'batch_size': 64},\n",
    "\n",
    "    {'name': 'channels64', 'batch_size': 10, 'num_channels': 64},\n",
    "    {'name': 'channels128', 'batch_size': 10, 'num_channels': 128},\n",
    "    {'name': 'interactions3', 'batch_size': 10, 'num_interactions': 3},\n",
    "    {'name': 'interactions4', 'batch_size': 10, 'num_interactions': 4},\n",
    "    {'name': 'maxL1', 'batch_size': 10, 'max_L': 1},\n",
    "    {'name': 'maxL2', 'batch_size': 10, 'max_L': 2},\n",
    "    {'name': 'correlation3', 'batch_size': 10, 'correlation': 3},\n",
    "    {'name': 'correlation4', 'batch_size': 10, 'correlation': 4},\n",
    "\n",
    "    {'name': 'chan64_corr3', 'batch_size': 10, 'num_channels': 64, 'correlation': 3},\n",
    "    {'name': 'chan64_inter3', 'batch_size': 10, 'num_channels': 64, 'num_interactions': 3},\n",
    "    {'name': 'inter3_corr3', 'batch_size': 10, 'num_interactions': 3, 'correlation': 3},\n",
    "    {'name': 'maxL1_corr3', 'batch_size': 10, 'max_L': 1, 'correlation': 3}\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "28914804-9343-40c3-8b22-a9db4a4ca4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_old_files(model_prefix):\n",
    "    pattern_extensions = [\n",
    "        \".log\", \".txt\", \".model\", \"_compiled.model\",\n",
    "        \"_run-123.log\", \"_run-123.model\", \"_run-123_train.txt\",\n",
    "        \"_swa.model\", \"_swa_compiled.model\"\n",
    "    ]\n",
    "    pattern_suffixes = [\"_epoch-48_swa.pt\", \"_epoch-*.pt\"]\n",
    "\n",
    "    for ext in pattern_extensions:\n",
    "        path = Path(\"MACE_models\") / f\"{model_prefix}{ext}\"\n",
    "        if path.exists():\n",
    "            path.unlink()\n",
    "\n",
    "    for path in Path(\"MACE_models\").glob(f\"{model_prefix}_run-123_epoch-*.pt\"):\n",
    "        path.unlink()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "70d07cfe-681e-42fc-a265-3e09cf97e5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gpu_stats():\n",
    "    result = subprocess.run(\n",
    "        ['nvidia-smi', '--query-gpu=utilization.gpu,memory.used',\n",
    "         '--format=csv,noheader,nounits'],\n",
    "        stdout=subprocess.PIPE, text=True\n",
    "    )\n",
    "    lines = result.stdout.strip().split('\\n')\n",
    "    utils, mems = zip(*[map(int, line.split(',')) for line in lines])\n",
    "    return sum(utils) / len(utils), sum(mems) / len(mems)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e86c0d9c-f529-40ec-b276-ba3f24cf1d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(log_path):\n",
    "    with open(log_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    rmse_table_lines = [i for i, l in enumerate(lines) if \"+-------------+---------------------+------------------+-------------------+\" in l]\n",
    "    if not rmse_table_lines:\n",
    "        print(\"âŒ No RMSE table found.\")\n",
    "        return None, None\n",
    "\n",
    "    for idx in reversed(rmse_table_lines):\n",
    "        for j in range(idx, min(idx + 10, len(lines))):\n",
    "            if '|    valid    |' in lines[j]:\n",
    "                parts = lines[j].strip().split('|')\n",
    "                try:\n",
    "                    rmse_e = float(parts[2].strip())\n",
    "                    rmse_f = float(parts[3].strip())\n",
    "                    print(f\"âœ… Found: RMSE_E = {rmse_e} meV, RMSE_F = {rmse_f} meV/Ã…\")\n",
    "                    return rmse_e, rmse_f\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error during parsing: {e}\")\n",
    "                    return None, None\n",
    "    print(\"âŒ Found no 'valid' line.\")\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bd558f6-1018-4d77-9a45-61634e7a3709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training started: mace_benchmark_best_guess_v1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:37:01.735 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:37:01.736 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_best_guess_v1.yaml', name='mace_benchmark_best_guess_v1', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:37:01.838 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:37:01.875 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:37:01.945 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:37:01.945 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:37:01.949 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:37:01.949 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:37:02.205 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:37:02.226 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:37:02.226 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:37:02.227 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:37:02.227 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:37:02.430 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:37:02.586 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:37:02.586 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:37:02.624 INFO: Building model\n",
      "2025-06-03 18:37:02.625 INFO: Hidden irreps: 64x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:37:03.683 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:37:03.771 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1-2): 2 x RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-2): 3 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:37:03.773 INFO: Number of parameters: 195856\n",
      "2025-06-03 18:37:03.773 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:37:03.773 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:37:03.773 INFO: Started training\n",
      "2025-06-03 18:37:04.521 INFO: Epoch None: loss=70.1338, RMSE_E_per_atom=6240.4 meV, RMSE_F=2580.7 meV / A\n",
      "2025-06-03 18:37:11.222 INFO: Epoch 0: loss=21.4374, RMSE_E_per_atom=4976.8 meV, RMSE_F=1380.1 meV / A\n",
      "2025-06-03 18:37:15.342 INFO: Epoch 2: loss=6.2869, RMSE_E_per_atom=4193.4 meV, RMSE_F=673.3 meV / A\n",
      "2025-06-03 18:37:18.823 INFO: Epoch 4: loss=5.4451, RMSE_E_per_atom=3807.0 meV, RMSE_F=633.4 meV / A\n",
      "2025-06-03 18:37:22.329 INFO: Epoch 6: loss=4.2819, RMSE_E_per_atom=3480.4 meV, RMSE_F=555.3 meV / A\n",
      "2025-06-03 18:37:25.826 INFO: Epoch 8: loss=3.8852, RMSE_E_per_atom=2649.2 meV, RMSE_F=565.0 meV / A\n",
      "2025-06-03 18:37:29.287 INFO: Epoch 10: loss=2.2204, RMSE_E_per_atom=1767.0 meV, RMSE_F=437.4 meV / A\n",
      "2025-06-03 18:37:32.904 INFO: Epoch 12: loss=1.4716, RMSE_E_per_atom=947.9 meV, RMSE_F=371.8 meV / A\n",
      "2025-06-03 18:37:36.478 INFO: Epoch 14: loss=2.3603, RMSE_E_per_atom=726.3 meV, RMSE_F=481.4 meV / A\n",
      "2025-06-03 18:37:40.470 INFO: Epoch 16: loss=0.9913, RMSE_E_per_atom=444.2 meV, RMSE_F=312.3 meV / A\n",
      "2025-06-03 18:37:44.131 INFO: Epoch 18: loss=1.1071, RMSE_E_per_atom=375.2 meV, RMSE_F=330.9 meV / A\n",
      "2025-06-03 18:37:47.682 INFO: Epoch 20: loss=0.8627, RMSE_E_per_atom=321.9 meV, RMSE_F=292.4 meV / A\n",
      "2025-06-03 18:37:51.365 INFO: Epoch 22: loss=0.7237, RMSE_E_per_atom=234.2 meV, RMSE_F=268.5 meV / A\n",
      "2025-06-03 18:37:55.083 INFO: Epoch 24: loss=0.7416, RMSE_E_per_atom=230.4 meV, RMSE_F=271.4 meV / A\n",
      "2025-06-03 18:37:58.731 INFO: Epoch 26: loss=0.8737, RMSE_E_per_atom=175.5 meV, RMSE_F=295.2 meV / A\n",
      "2025-06-03 18:38:02.284 INFO: Epoch 28: loss=0.8441, RMSE_E_per_atom=188.6 meV, RMSE_F=290.0 meV / A\n",
      "2025-06-03 18:38:05.858 INFO: Epoch 30: loss=0.5486, RMSE_E_per_atom=139.3 meV, RMSE_F=234.0 meV / A\n",
      "2025-06-03 18:38:09.621 INFO: Epoch 32: loss=0.5177, RMSE_E_per_atom=175.5 meV, RMSE_F=226.8 meV / A\n",
      "2025-06-03 18:38:13.382 INFO: Epoch 34: loss=0.8739, RMSE_E_per_atom=118.6 meV, RMSE_F=295.9 meV / A\n",
      "2025-06-03 18:38:15.117 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:38:16.917 INFO: Epoch 36: loss=0.6227, RMSE_E_per_atom=47.2 meV, RMSE_F=200.1 meV / A\n",
      "2025-06-03 18:38:20.553 INFO: Epoch 38: loss=0.4318, RMSE_E_per_atom=34.8 meV, RMSE_F=176.6 meV / A\n",
      "2025-06-03 18:38:24.260 INFO: Epoch 40: loss=0.5563, RMSE_E_per_atom=48.9 meV, RMSE_F=178.2 meV / A\n",
      "2025-06-03 18:38:27.882 INFO: Epoch 42: loss=0.5173, RMSE_E_per_atom=45.0 meV, RMSE_F=177.6 meV / A\n",
      "2025-06-03 18:38:31.489 INFO: Epoch 44: loss=0.4408, RMSE_E_per_atom=38.7 meV, RMSE_F=170.8 meV / A\n",
      "2025-06-03 18:38:35.032 INFO: Epoch 46: loss=0.4441, RMSE_E_per_atom=39.5 meV, RMSE_F=169.9 meV / A\n",
      "2025-06-03 18:38:38.651 INFO: Epoch 48: loss=0.4473, RMSE_E_per_atom=39.8 meV, RMSE_F=170.0 meV / A\n",
      "2025-06-03 18:38:40.321 INFO: Training complete\n",
      "2025-06-03 18:38:40.322 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:38:40.881 INFO: Loading checkpoint: MACE_models/mace_benchmark_best_guess_v1_run-123_epoch-32.pt\n",
      "2025-06-03 18:38:40.921 INFO: Loaded model from epoch 32\n",
      "2025-06-03 18:38:40.921 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:38:41.692 INFO: Evaluating valid ...\n",
      "2025-06-03 18:38:41.782 INFO: Evaluating Default ...\n",
      "2025-06-03 18:38:47.062 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        157.7        |      200.9       |        9.23       |\n",
      "|    valid    |        175.5        |      226.8       |        8.77       |\n",
      "|   Default   |        152.9        |      222.0       |        9.69       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:38:47.062 INFO: Saving model to MACE_models/mace_benchmark_best_guess_v1_run-123.model\n",
      "2025-06-03 18:38:47.202 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_best_guess_v1_compiled.model\n",
      "2025-06-03 18:38:48.094 INFO: Loading checkpoint: MACE_models/mace_benchmark_best_guess_v1_run-123_epoch-38_swa.pt\n",
      "2025-06-03 18:38:48.125 INFO: Loaded model from epoch 38\n",
      "2025-06-03 18:38:48.126 INFO: Evaluating train ...\n",
      "2025-06-03 18:38:48.942 INFO: Evaluating valid ...\n",
      "2025-06-03 18:38:49.039 INFO: Evaluating Default ...\n",
      "2025-06-03 18:38:54.370 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         37.2        |      159.6       |        7.33       |\n",
      "|    valid    |         34.8        |      176.6       |        6.83       |\n",
      "|   Default   |         37.9        |      187.9       |        8.21       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:38:54.370 INFO: Saving model to MACE_models/mace_benchmark_best_guess_v1_run-123_swa.model\n",
      "2025-06-03 18:38:54.532 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_best_guess_v1_swa_compiled.model\n",
      "2025-06-03 18:38:55.538 INFO: Done\n",
      "âœ… Found: RMSE_E = 34.8 meV, RMSE_F = 176.6 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_best_guess_v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:38:59.825 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:38:59.825 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_best_guess_v2.yaml', name='mace_benchmark_best_guess_v2', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=1, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:38:59.921 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:38:59.958 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:39:00.028 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:39:00.028 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:39:00.032 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:39:00.032 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:39:00.288 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:39:00.309 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:39:00.309 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:39:00.310 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:39:00.310 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:39:00.514 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:39:00.668 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:39:00.668 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:39:00.706 INFO: Building model\n",
      "2025-06-03 18:39:00.707 INFO: Hidden irreps: 64x0e+64x1o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:39:01.959 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:39:02.068 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o -> 64x0e+64x1o | 8192 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o x 1x0e+1x1o+1x2e -> 128x0e+192x1o+128x2e | 448 paths | 448 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 448]\n",
      "      (linear): Linear(128x0e+192x1o+128x2e -> 64x0e+64x1o+64x2e | 28672 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o x 3x0e -> 64x0e+64x1o | 24576 paths | 24576 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (2): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o -> 64x0e+64x1o | 8192 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o x 1x0e+1x1o+1x2e -> 128x0e+192x1o+128x2e | 448 paths | 448 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 448]\n",
      "      (linear): Linear(128x0e+192x1o+128x2e -> 64x0e+64x1o+64x2e | 28672 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e+64x1o -> 64x0e+64x1o | 8192 weights)\n",
      "    )\n",
      "    (2): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e+64x1o -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:39:02.071 INFO: Number of parameters: 300048\n",
      "2025-06-03 18:39:02.071 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:39:02.071 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:39:02.071 INFO: Started training\n",
      "2025-06-03 18:39:02.916 INFO: Epoch None: loss=71.2926, RMSE_E_per_atom=6240.3 meV, RMSE_F=2603.1 meV / A\n",
      "2025-06-03 18:39:10.747 INFO: Epoch 0: loss=20.4686, RMSE_E_per_atom=4829.0 meV, RMSE_F=1349.2 meV / A\n",
      "2025-06-03 18:39:16.779 INFO: Epoch 2: loss=6.3562, RMSE_E_per_atom=4064.1 meV, RMSE_F=686.9 meV / A\n",
      "2025-06-03 18:39:22.415 INFO: Epoch 4: loss=5.1433, RMSE_E_per_atom=3697.6 meV, RMSE_F=616.4 meV / A\n",
      "2025-06-03 18:39:28.188 INFO: Epoch 6: loss=3.1404, RMSE_E_per_atom=2948.0 meV, RMSE_F=478.0 meV / A\n",
      "2025-06-03 18:39:33.988 INFO: Epoch 8: loss=1.9877, RMSE_E_per_atom=1530.5 meV, RMSE_F=419.5 meV / A\n",
      "2025-06-03 18:39:39.742 INFO: Epoch 10: loss=1.7425, RMSE_E_per_atom=750.1 meV, RMSE_F=411.7 meV / A\n",
      "2025-06-03 18:39:45.572 INFO: Epoch 12: loss=1.2611, RMSE_E_per_atom=622.3 meV, RMSE_F=351.1 meV / A\n",
      "2025-06-03 18:39:51.308 INFO: Epoch 14: loss=1.3137, RMSE_E_per_atom=357.5 meV, RMSE_F=361.8 meV / A\n",
      "2025-06-03 18:39:56.878 INFO: Epoch 16: loss=0.8664, RMSE_E_per_atom=330.9 meV, RMSE_F=293.5 meV / A\n",
      "2025-06-03 18:40:02.640 INFO: Epoch 18: loss=0.7257, RMSE_E_per_atom=243.1 meV, RMSE_F=269.3 meV / A\n",
      "2025-06-03 18:40:08.449 INFO: Epoch 20: loss=1.0131, RMSE_E_per_atom=251.7 meV, RMSE_F=318.4 meV / A\n",
      "2025-06-03 18:40:14.020 INFO: Epoch 22: loss=0.6563, RMSE_E_per_atom=267.3 meV, RMSE_F=255.7 meV / A\n",
      "2025-06-03 18:40:19.827 INFO: Epoch 24: loss=0.9983, RMSE_E_per_atom=237.5 meV, RMSE_F=315.8 meV / A\n",
      "2025-06-03 18:40:25.425 INFO: Epoch 26: loss=0.6915, RMSE_E_per_atom=204.5 meV, RMSE_F=263.0 meV / A\n",
      "2025-06-03 18:40:31.002 INFO: Epoch 28: loss=0.7307, RMSE_E_per_atom=224.4 meV, RMSE_F=270.1 meV / A\n",
      "2025-06-03 18:40:36.558 INFO: Epoch 30: loss=0.5509, RMSE_E_per_atom=240.0 meV, RMSE_F=234.2 meV / A\n",
      "2025-06-03 18:40:42.341 INFO: Epoch 32: loss=0.8338, RMSE_E_per_atom=191.1 meV, RMSE_F=288.5 meV / A\n",
      "2025-06-03 18:40:47.905 INFO: Epoch 34: loss=0.5588, RMSE_E_per_atom=230.4 meV, RMSE_F=235.6 meV / A\n",
      "2025-06-03 18:40:50.621 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:40:53.471 INFO: Epoch 36: loss=3.4351, RMSE_E_per_atom=173.0 meV, RMSE_F=210.6 meV / A\n",
      "2025-06-03 18:40:59.380 INFO: Epoch 38: loss=0.4283, RMSE_E_per_atom=13.6 meV, RMSE_F=203.1 meV / A\n",
      "2025-06-03 18:41:05.160 INFO: Epoch 40: loss=0.3301, RMSE_E_per_atom=6.3 meV, RMSE_F=181.3 meV / A\n",
      "2025-06-03 18:41:10.884 INFO: Epoch 42: loss=0.3351, RMSE_E_per_atom=18.2 meV, RMSE_F=174.5 meV / A\n",
      "2025-06-03 18:41:16.427 INFO: Epoch 44: loss=0.3058, RMSE_E_per_atom=9.4 meV, RMSE_F=173.1 meV / A\n",
      "2025-06-03 18:41:22.228 INFO: Epoch 46: loss=0.3219, RMSE_E_per_atom=21.9 meV, RMSE_F=166.2 meV / A\n",
      "2025-06-03 18:41:27.800 INFO: Epoch 48: loss=0.3133, RMSE_E_per_atom=19.4 meV, RMSE_F=166.7 meV / A\n",
      "2025-06-03 18:41:30.540 INFO: Training complete\n",
      "2025-06-03 18:41:30.540 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:41:30.995 INFO: Loading checkpoint: MACE_models/mace_benchmark_best_guess_v2_run-123_epoch-30.pt\n",
      "2025-06-03 18:41:31.035 INFO: Loaded model from epoch 30\n",
      "2025-06-03 18:41:31.035 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:41:32.077 INFO: Evaluating valid ...\n",
      "2025-06-03 18:41:32.204 INFO: Evaluating Default ...\n",
      "2025-06-03 18:41:38.436 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        222.4        |      200.7       |        9.22       |\n",
      "|    valid    |        240.0        |      234.2       |        9.06       |\n",
      "|   Default   |        214.5        |      237.0       |       10.35       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:41:38.436 INFO: Saving model to MACE_models/mace_benchmark_best_guess_v2_run-123.model\n",
      "2025-06-03 18:41:38.605 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_best_guess_v2_compiled.model\n",
      "2025-06-03 18:41:39.642 INFO: Loading checkpoint: MACE_models/mace_benchmark_best_guess_v2_run-123_epoch-44_swa.pt\n",
      "2025-06-03 18:41:39.688 INFO: Loaded model from epoch 44\n",
      "2025-06-03 18:41:39.688 INFO: Evaluating train ...\n",
      "2025-06-03 18:41:40.681 INFO: Evaluating valid ...\n",
      "2025-06-03 18:41:40.811 INFO: Evaluating Default ...\n",
      "2025-06-03 18:41:46.994 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         10.7        |      131.9       |        6.06       |\n",
      "|    valid    |         9.4         |      173.1       |        6.69       |\n",
      "|   Default   |         10.9        |      176.9       |        7.72       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:41:46.994 INFO: Saving model to MACE_models/mace_benchmark_best_guess_v2_run-123_swa.model\n",
      "2025-06-03 18:41:47.220 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_best_guess_v2_swa_compiled.model\n",
      "2025-06-03 18:41:48.347 INFO: Done\n",
      "âœ… Found: RMSE_E = 9.4 meV, RMSE_F = 173.1 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_best_guess_v3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:41:51.734 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:41:51.734 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_best_guess_v3.yaml', name='mace_benchmark_best_guess_v3', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:41:51.823 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:41:51.861 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:41:51.932 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:41:51.932 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:41:51.936 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:41:51.936 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:41:52.195 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:41:52.216 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:41:52.216 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:41:52.217 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:41:52.217 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:41:52.423 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:41:52.578 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:41:52.578 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:41:52.617 INFO: Building model\n",
      "2025-06-03 18:41:52.618 INFO: Hidden irreps: 64x0e+64x1o+64x2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:41:54.063 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:41:54.182 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (2): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1-2): 2 x Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "    )\n",
      "    (2): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:41:54.184 INFO: Number of parameters: 405008\n",
      "2025-06-03 18:41:54.185 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:41:54.185 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:41:54.185 INFO: Started training\n",
      "2025-06-03 18:41:55.090 INFO: Epoch None: loss=72.5723, RMSE_E_per_atom=6251.1 meV, RMSE_F=2627.6 meV / A\n",
      "2025-06-03 18:42:04.771 INFO: Epoch 0: loss=18.8457, RMSE_E_per_atom=4792.7 meV, RMSE_F=1289.0 meV / A\n",
      "2025-06-03 18:42:14.763 INFO: Epoch 2: loss=5.8061, RMSE_E_per_atom=4103.8 meV, RMSE_F=642.4 meV / A\n",
      "2025-06-03 18:42:24.492 INFO: Epoch 4: loss=3.9537, RMSE_E_per_atom=3365.0 meV, RMSE_F=531.7 meV / A\n",
      "2025-06-03 18:42:34.285 INFO: Epoch 6: loss=2.6683, RMSE_E_per_atom=2684.6 meV, RMSE_F=442.2 meV / A\n",
      "2025-06-03 18:42:44.080 INFO: Epoch 8: loss=2.0825, RMSE_E_per_atom=1812.1 meV, RMSE_F=419.5 meV / A\n",
      "2025-06-03 18:42:53.809 INFO: Epoch 10: loss=2.2942, RMSE_E_per_atom=1322.6 meV, RMSE_F=461.9 meV / A\n",
      "2025-06-03 18:43:03.479 INFO: Epoch 12: loss=1.5860, RMSE_E_per_atom=727.5 meV, RMSE_F=393.0 meV / A\n",
      "2025-06-03 18:43:13.251 INFO: Epoch 14: loss=0.9911, RMSE_E_per_atom=423.5 meV, RMSE_F=312.4 meV / A\n",
      "2025-06-03 18:43:23.042 INFO: Epoch 16: loss=1.0217, RMSE_E_per_atom=544.8 meV, RMSE_F=316.0 meV / A\n",
      "2025-06-03 18:43:32.681 INFO: Epoch 18: loss=0.6387, RMSE_E_per_atom=217.6 meV, RMSE_F=252.5 meV / A\n",
      "2025-06-03 18:43:42.483 INFO: Epoch 20: loss=0.9250, RMSE_E_per_atom=273.1 meV, RMSE_F=303.2 meV / A\n",
      "2025-06-03 18:43:52.041 INFO: Epoch 22: loss=0.5506, RMSE_E_per_atom=205.7 meV, RMSE_F=234.6 meV / A\n",
      "2025-06-03 18:44:02.058 INFO: Epoch 24: loss=0.8732, RMSE_E_per_atom=166.1 meV, RMSE_F=295.7 meV / A\n",
      "2025-06-03 18:44:11.641 INFO: Epoch 26: loss=1.2152, RMSE_E_per_atom=290.2 meV, RMSE_F=347.0 meV / A\n",
      "2025-06-03 18:44:21.211 INFO: Epoch 28: loss=0.5992, RMSE_E_per_atom=180.0 meV, RMSE_F=244.9 meV / A\n",
      "2025-06-03 18:44:30.840 INFO: Epoch 30: loss=0.9180, RMSE_E_per_atom=286.7 meV, RMSE_F=301.9 meV / A\n",
      "2025-06-03 18:44:40.477 INFO: Epoch 32: loss=0.5219, RMSE_E_per_atom=169.0 meV, RMSE_F=228.4 meV / A\n",
      "2025-06-03 18:44:50.268 INFO: Epoch 34: loss=0.4643, RMSE_E_per_atom=221.9 meV, RMSE_F=214.8 meV / A\n",
      "2025-06-03 18:44:55.183 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:45:00.091 INFO: Epoch 36: loss=0.4395, RMSE_E_per_atom=20.4 meV, RMSE_F=199.9 meV / A\n",
      "2025-06-03 18:45:09.840 INFO: Epoch 38: loss=0.2636, RMSE_E_per_atom=13.2 meV, RMSE_F=157.1 meV / A\n",
      "2025-06-03 18:45:19.868 INFO: Epoch 40: loss=0.2600, RMSE_E_per_atom=7.0 meV, RMSE_F=160.1 meV / A\n",
      "2025-06-03 18:45:29.663 INFO: Epoch 42: loss=0.2515, RMSE_E_per_atom=7.5 meV, RMSE_F=157.3 meV / A\n",
      "2025-06-03 18:45:39.454 INFO: Epoch 44: loss=0.2699, RMSE_E_per_atom=18.4 meV, RMSE_F=154.1 meV / A\n",
      "2025-06-03 18:45:49.082 INFO: Epoch 46: loss=0.3418, RMSE_E_per_atom=27.9 meV, RMSE_F=163.0 meV / A\n",
      "2025-06-03 18:45:58.676 INFO: Epoch 48: loss=0.2599, RMSE_E_per_atom=21.7 meV, RMSE_F=146.3 meV / A\n",
      "2025-06-03 18:46:03.381 INFO: Training complete\n",
      "2025-06-03 18:46:03.381 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:46:03.881 INFO: Loading checkpoint: MACE_models/mace_benchmark_best_guess_v3_run-123_epoch-34.pt\n",
      "2025-06-03 18:46:03.926 INFO: Loaded model from epoch 34\n",
      "2025-06-03 18:46:03.927 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:46:05.640 INFO: Evaluating valid ...\n",
      "2025-06-03 18:46:05.859 INFO: Evaluating Default ...\n",
      "2025-06-03 18:46:15.612 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        188.7        |      167.9       |        7.71       |\n",
      "|    valid    |        221.9        |      214.8       |        8.30       |\n",
      "|   Default   |        199.1        |      202.0       |        8.82       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:46:15.612 INFO: Saving model to MACE_models/mace_benchmark_best_guess_v3_run-123.model\n",
      "2025-06-03 18:46:15.846 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_best_guess_v3_compiled.model\n",
      "2025-06-03 18:46:17.240 INFO: Loading checkpoint: MACE_models/mace_benchmark_best_guess_v3_run-123_epoch-42_swa.pt\n",
      "2025-06-03 18:46:17.295 INFO: Loaded model from epoch 42\n",
      "2025-06-03 18:46:17.295 INFO: Evaluating train ...\n",
      "2025-06-03 18:46:18.939 INFO: Evaluating valid ...\n",
      "2025-06-03 18:46:19.164 INFO: Evaluating Default ...\n",
      "2025-06-03 18:46:28.971 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         9.1         |      105.1       |        4.83       |\n",
      "|    valid    |         7.5         |      157.3       |        6.08       |\n",
      "|   Default   |         9.3         |      160.8       |        7.02       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:46:28.972 INFO: Saving model to MACE_models/mace_benchmark_best_guess_v3_run-123_swa.model\n",
      "2025-06-03 18:46:29.207 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_best_guess_v3_swa_compiled.model\n",
      "2025-06-03 18:46:30.421 INFO: Done\n",
      "âœ… Found: RMSE_E = 7.5 meV, RMSE_F = 157.3 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_baseline\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:46:34.704 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:46:34.704 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_baseline.yaml', name='mace_benchmark_baseline', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:46:34.795 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:46:34.830 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:46:34.901 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:46:34.901 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:46:34.905 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:46:34.905 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:46:35.161 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:46:35.181 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:46:35.181 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:46:35.182 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:46:35.182 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:46:35.382 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:46:35.536 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:46:35.536 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:46:35.573 INFO: Building model\n",
      "2025-06-03 18:46:35.574 INFO: Hidden irreps: 32x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:46:36.248 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:46:36.307 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:46:36.309 INFO: Number of parameters: 53648\n",
      "2025-06-03 18:46:36.309 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:46:36.309 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:46:36.309 INFO: Started training\n",
      "2025-06-03 18:46:37.001 INFO: Epoch None: loss=71.3685, RMSE_E_per_atom=6269.3 meV, RMSE_F=2604.0 meV / A\n",
      "2025-06-03 18:46:43.261 INFO: Epoch 0: loss=30.7212, RMSE_E_per_atom=5323.5 meV, RMSE_F=1678.3 meV / A\n",
      "2025-06-03 18:46:46.617 INFO: Epoch 2: loss=8.4117, RMSE_E_per_atom=4865.6 meV, RMSE_F=779.0 meV / A\n",
      "2025-06-03 18:46:49.366 INFO: Epoch 4: loss=4.8551, RMSE_E_per_atom=4090.0 meV, RMSE_F=564.8 meV / A\n",
      "2025-06-03 18:46:52.155 INFO: Epoch 6: loss=2.6657, RMSE_E_per_atom=3270.8 meV, RMSE_F=400.3 meV / A\n",
      "2025-06-03 18:46:55.001 INFO: Epoch 8: loss=1.8553, RMSE_E_per_atom=1755.6 meV, RMSE_F=394.8 meV / A\n",
      "2025-06-03 18:46:57.738 INFO: Epoch 10: loss=1.6979, RMSE_E_per_atom=1053.0 meV, RMSE_F=399.6 meV / A\n",
      "2025-06-03 18:47:00.425 INFO: Epoch 12: loss=1.8038, RMSE_E_per_atom=737.5 meV, RMSE_F=419.0 meV / A\n",
      "2025-06-03 18:47:02.275 INFO: Epoch 14: loss=1.2749, RMSE_E_per_atom=535.8 meV, RMSE_F=353.6 meV / A\n",
      "2025-06-03 18:47:04.402 INFO: Epoch 16: loss=1.2762, RMSE_E_per_atom=388.5 meV, RMSE_F=356.3 meV / A\n",
      "2025-06-03 18:47:06.879 INFO: Epoch 18: loss=1.0427, RMSE_E_per_atom=306.5 meV, RMSE_F=322.2 meV / A\n",
      "2025-06-03 18:47:09.868 INFO: Epoch 20: loss=0.8701, RMSE_E_per_atom=457.8 meV, RMSE_F=291.9 meV / A\n",
      "2025-06-03 18:47:12.755 INFO: Epoch 22: loss=0.8466, RMSE_E_per_atom=310.6 meV, RMSE_F=289.7 meV / A\n",
      "2025-06-03 18:47:15.667 INFO: Epoch 24: loss=0.8193, RMSE_E_per_atom=303.1 meV, RMSE_F=285.1 meV / A\n",
      "2025-06-03 18:47:18.656 INFO: Epoch 26: loss=1.0524, RMSE_E_per_atom=297.8 meV, RMSE_F=323.6 meV / A\n",
      "2025-06-03 18:47:21.492 INFO: Epoch 28: loss=1.1390, RMSE_E_per_atom=244.9 meV, RMSE_F=337.2 meV / A\n",
      "2025-06-03 18:47:24.401 INFO: Epoch 30: loss=0.8262, RMSE_E_per_atom=201.0 meV, RMSE_F=286.9 meV / A\n",
      "2025-06-03 18:47:27.340 INFO: Epoch 32: loss=0.6928, RMSE_E_per_atom=195.0 meV, RMSE_F=262.5 meV / A\n",
      "2025-06-03 18:47:30.373 INFO: Epoch 34: loss=1.0002, RMSE_E_per_atom=158.7 meV, RMSE_F=315.7 meV / A\n",
      "2025-06-03 18:47:31.744 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:47:33.199 INFO: Epoch 36: loss=0.6848, RMSE_E_per_atom=49.9 meV, RMSE_F=208.9 meV / A\n",
      "2025-06-03 18:47:36.095 INFO: Epoch 38: loss=0.5100, RMSE_E_per_atom=20.9 meV, RMSE_F=216.3 meV / A\n",
      "2025-06-03 18:47:39.018 INFO: Epoch 40: loss=0.4807, RMSE_E_per_atom=20.3 meV, RMSE_F=209.9 meV / A\n",
      "2025-06-03 18:47:42.022 INFO: Epoch 42: loss=0.4923, RMSE_E_per_atom=23.1 meV, RMSE_F=209.7 meV / A\n",
      "2025-06-03 18:47:44.924 INFO: Epoch 44: loss=0.4816, RMSE_E_per_atom=17.7 meV, RMSE_F=212.5 meV / A\n",
      "2025-06-03 18:47:47.817 INFO: Epoch 46: loss=0.4534, RMSE_E_per_atom=11.2 meV, RMSE_F=210.3 meV / A\n",
      "2025-06-03 18:47:50.745 INFO: Epoch 48: loss=0.5022, RMSE_E_per_atom=21.5 meV, RMSE_F=213.9 meV / A\n",
      "2025-06-03 18:47:52.132 INFO: Training complete\n",
      "2025-06-03 18:47:52.132 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:47:52.697 INFO: Loading checkpoint: MACE_models/mace_benchmark_baseline_run-123_epoch-32.pt\n",
      "2025-06-03 18:47:52.721 INFO: Loaded model from epoch 32\n",
      "2025-06-03 18:47:52.721 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:47:53.339 INFO: Evaluating valid ...\n",
      "2025-06-03 18:47:53.412 INFO: Evaluating Default ...\n",
      "2025-06-03 18:47:57.594 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        213.9        |      236.3       |       10.85       |\n",
      "|    valid    |        195.0        |      262.5       |       10.15       |\n",
      "|   Default   |        212.7        |      255.3       |       11.15       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:47:57.594 INFO: Saving model to MACE_models/mace_benchmark_baseline_run-123.model\n",
      "2025-06-03 18:47:57.678 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_baseline_compiled.model\n",
      "2025-06-03 18:47:58.351 INFO: Loading checkpoint: MACE_models/mace_benchmark_baseline_run-123_epoch-46_swa.pt\n",
      "2025-06-03 18:47:58.376 INFO: Loaded model from epoch 46\n",
      "2025-06-03 18:47:58.376 INFO: Evaluating train ...\n",
      "2025-06-03 18:47:59.057 INFO: Evaluating valid ...\n",
      "2025-06-03 18:47:59.132 INFO: Evaluating Default ...\n",
      "2025-06-03 18:48:03.290 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         11.2        |      181.0       |        8.31       |\n",
      "|    valid    |         11.2        |      210.3       |        8.13       |\n",
      "|   Default   |         11.2        |      211.8       |        9.25       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:48:03.290 INFO: Saving model to MACE_models/mace_benchmark_baseline_run-123_swa.model\n",
      "2025-06-03 18:48:03.399 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_baseline_swa_compiled.model\n",
      "2025-06-03 18:48:04.031 INFO: Done\n",
      "âœ… Found: RMSE_E = 11.2 meV, RMSE_F = 210.3 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_batch32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:48:08.395 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:48:08.395 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_batch32.yaml', name='mace_benchmark_batch32', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=32, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:48:08.510 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:48:08.548 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:48:08.618 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:48:08.619 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:48:08.623 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:48:08.623 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:48:08.881 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:48:08.902 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:48:08.902 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:48:08.903 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:48:08.903 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:48:09.106 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:48:09.245 INFO: Average number of neighbors: 9.842592592592593\n",
      "2025-06-03 18:48:09.245 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:48:09.268 INFO: Building model\n",
      "2025-06-03 18:48:09.268 INFO: Hidden irreps: 32x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:48:09.951 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:48:10.011 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.027244, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:48:10.012 INFO: Number of parameters: 53648\n",
      "2025-06-03 18:48:10.013 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:48:10.013 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:48:10.013 INFO: Started training\n",
      "2025-06-03 18:48:10.720 INFO: Epoch None: loss=71.2990, RMSE_E_per_atom=6268.1 meV, RMSE_F=2602.7 meV / A\n",
      "2025-06-03 18:48:16.153 INFO: Epoch 0: loss=56.8414, RMSE_E_per_atom=6095.5 meV, RMSE_F=2312.7 meV / A\n",
      "2025-06-03 18:48:17.638 INFO: Epoch 2: loss=32.6968, RMSE_E_per_atom=5320.9 meV, RMSE_F=1736.6 meV / A\n",
      "2025-06-03 18:48:18.507 INFO: Epoch 4: loss=20.7923, RMSE_E_per_atom=5495.8 meV, RMSE_F=1338.7 meV / A\n",
      "2025-06-03 18:48:19.448 INFO: Epoch 6: loss=12.4232, RMSE_E_per_atom=5082.4 meV, RMSE_F=992.6 meV / A\n",
      "2025-06-03 18:48:20.389 INFO: Epoch 8: loss=8.4416, RMSE_E_per_atom=4955.5 meV, RMSE_F=775.3 meV / A\n",
      "2025-06-03 18:48:21.284 INFO: Epoch 10: loss=6.1862, RMSE_E_per_atom=4742.0 meV, RMSE_F=628.5 meV / A\n",
      "2025-06-03 18:48:22.243 INFO: Epoch 12: loss=5.0937, RMSE_E_per_atom=4507.6 meV, RMSE_F=554.7 meV / A\n",
      "2025-06-03 18:48:23.196 INFO: Epoch 14: loss=4.1925, RMSE_E_per_atom=4302.1 meV, RMSE_F=484.8 meV / A\n",
      "2025-06-03 18:48:24.140 INFO: Epoch 16: loss=3.6794, RMSE_E_per_atom=4009.9 meV, RMSE_F=455.8 meV / A\n",
      "2025-06-03 18:48:25.077 INFO: Epoch 18: loss=3.7215, RMSE_E_per_atom=3809.4 meV, RMSE_F=477.5 meV / A\n",
      "2025-06-03 18:48:25.989 INFO: Epoch 20: loss=3.3791, RMSE_E_per_atom=3461.6 meV, RMSE_F=467.4 meV / A\n",
      "2025-06-03 18:48:26.929 INFO: Epoch 22: loss=3.0201, RMSE_E_per_atom=3251.6 meV, RMSE_F=444.4 meV / A\n",
      "2025-06-03 18:48:27.874 INFO: Epoch 24: loss=2.7116, RMSE_E_per_atom=2748.2 meV, RMSE_F=442.9 meV / A\n",
      "2025-06-03 18:48:28.856 INFO: Epoch 26: loss=2.0742, RMSE_E_per_atom=2397.8 meV, RMSE_F=388.2 meV / A\n",
      "2025-06-03 18:48:29.811 INFO: Epoch 28: loss=1.8529, RMSE_E_per_atom=1958.1 meV, RMSE_F=384.5 meV / A\n",
      "2025-06-03 18:48:30.779 INFO: Epoch 30: loss=1.3955, RMSE_E_per_atom=1582.0 meV, RMSE_F=339.4 meV / A\n",
      "2025-06-03 18:48:31.737 INFO: Epoch 32: loss=1.3955, RMSE_E_per_atom=1219.2 meV, RMSE_F=353.7 meV / A\n",
      "2025-06-03 18:48:32.697 INFO: Epoch 34: loss=1.3291, RMSE_E_per_atom=916.0 meV, RMSE_F=353.9 meV / A\n",
      "2025-06-03 18:48:33.148 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:48:33.653 INFO: Epoch 36: loss=40.6581, RMSE_E_per_atom=625.0 meV, RMSE_F=400.2 meV / A\n",
      "2025-06-03 18:48:34.671 INFO: Epoch 38: loss=16.1641, RMSE_E_per_atom=349.4 meV, RMSE_F=629.5 meV / A\n",
      "2025-06-03 18:48:35.675 INFO: Epoch 40: loss=3.4389, RMSE_E_per_atom=156.0 meV, RMSE_F=318.0 meV / A\n",
      "2025-06-03 18:48:36.681 INFO: Epoch 42: loss=1.1969, RMSE_E_per_atom=43.6 meV, RMSE_F=317.8 meV / A\n",
      "2025-06-03 18:48:37.753 INFO: Epoch 44: loss=1.1282, RMSE_E_per_atom=50.8 meV, RMSE_F=295.6 meV / A\n",
      "2025-06-03 18:48:38.764 INFO: Epoch 46: loss=1.0852, RMSE_E_per_atom=50.2 meV, RMSE_F=289.5 meV / A\n",
      "2025-06-03 18:48:39.770 INFO: Epoch 48: loss=0.9065, RMSE_E_per_atom=24.7 meV, RMSE_F=291.3 meV / A\n",
      "2025-06-03 18:48:40.276 INFO: Training complete\n",
      "2025-06-03 18:48:40.277 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:48:40.838 INFO: Loading checkpoint: MACE_models/mace_benchmark_batch32_run-123_epoch-34.pt\n",
      "2025-06-03 18:48:40.862 INFO: Loaded model from epoch 34\n",
      "2025-06-03 18:48:40.862 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:48:41.042 INFO: Evaluating valid ...\n",
      "2025-06-03 18:48:41.100 INFO: Evaluating Default ...\n",
      "2025-06-03 18:48:45.960 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        886.1        |      330.1       |       15.14       |\n",
      "|    valid    |        916.0        |      353.9       |       13.68       |\n",
      "|   Default   |        902.1        |      353.7       |       15.45       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:48:45.960 INFO: Saving model to MACE_models/mace_benchmark_batch32_run-123.model\n",
      "2025-06-03 18:48:46.041 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_batch32_compiled.model\n",
      "2025-06-03 18:48:46.713 INFO: Loading checkpoint: MACE_models/mace_benchmark_batch32_run-123_epoch-48_swa.pt\n",
      "2025-06-03 18:48:46.738 INFO: Loaded model from epoch 48\n",
      "2025-06-03 18:48:46.738 INFO: Evaluating train ...\n",
      "2025-06-03 18:48:46.975 INFO: Evaluating valid ...\n",
      "2025-06-03 18:48:47.039 INFO: Evaluating Default ...\n",
      "2025-06-03 18:48:51.513 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         35.0        |      278.1       |       12.46       |\n",
      "|    valid    |         24.7        |      291.3       |       11.26       |\n",
      "|   Default   |         36.6        |      305.0       |       13.32       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:48:51.513 INFO: Saving model to MACE_models/mace_benchmark_batch32_run-123_swa.model\n",
      "2025-06-03 18:48:51.594 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_batch32_swa_compiled.model\n",
      "2025-06-03 18:48:52.236 INFO: Done\n",
      "âœ… Found: RMSE_E = 24.7 meV, RMSE_F = 291.3 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_batch64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:48:55.728 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:48:55.728 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_batch64.yaml', name='mace_benchmark_batch64', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=64, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:48:55.819 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:48:55.854 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:48:55.925 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:48:55.926 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:48:55.930 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:48:55.930 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:48:56.187 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:48:56.208 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:48:56.208 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:48:56.209 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:48:56.209 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:48:56.455 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:48:56.590 INFO: Average number of neighbors: 9.750211924272394\n",
      "2025-06-03 18:48:56.590 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:48:56.606 INFO: Building model\n",
      "2025-06-03 18:48:56.607 INFO: Hidden irreps: 32x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:48:57.285 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:48:57.346 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.035435, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:48:57.348 INFO: Number of parameters: 53648\n",
      "2025-06-03 18:48:57.348 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:48:57.348 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:48:57.348 INFO: Started training\n",
      "2025-06-03 18:48:58.054 INFO: Epoch None: loss=71.3133, RMSE_E_per_atom=6268.3 meV, RMSE_F=2603.0 meV / A\n",
      "2025-06-03 18:49:00.232 INFO: Epoch 0: loss=66.7197, RMSE_E_per_atom=6217.4 meV, RMSE_F=2514.3 meV / A\n",
      "2025-06-03 18:49:04.249 INFO: Epoch 2: loss=50.7422, RMSE_E_per_atom=6028.4 meV, RMSE_F=2178.2 meV / A\n",
      "2025-06-03 18:49:04.750 INFO: Epoch 4: loss=38.7279, RMSE_E_per_atom=5402.9 meV, RMSE_F=1900.7 meV / A\n",
      "2025-06-03 18:49:05.271 INFO: Epoch 6: loss=34.6697, RMSE_E_per_atom=5292.2 meV, RMSE_F=1792.8 meV / A\n",
      "2025-06-03 18:49:05.796 INFO: Epoch 8: loss=25.7258, RMSE_E_per_atom=5585.8 meV, RMSE_F=1510.3 meV / A\n",
      "2025-06-03 18:49:06.325 INFO: Epoch 10: loss=20.4469, RMSE_E_per_atom=5578.6 meV, RMSE_F=1320.7 meV / A\n",
      "2025-06-03 18:49:06.842 INFO: Epoch 12: loss=16.4651, RMSE_E_per_atom=5361.9 meV, RMSE_F=1166.8 meV / A\n",
      "2025-06-03 18:49:07.369 INFO: Epoch 14: loss=14.1738, RMSE_E_per_atom=5321.1 meV, RMSE_F=1066.4 meV / A\n",
      "2025-06-03 18:49:07.897 INFO: Epoch 16: loss=11.7105, RMSE_E_per_atom=5302.2 meV, RMSE_F=943.5 meV / A\n",
      "2025-06-03 18:49:08.415 INFO: Epoch 18: loss=8.8481, RMSE_E_per_atom=5047.0 meV, RMSE_F=794.4 meV / A\n",
      "2025-06-03 18:49:08.930 INFO: Epoch 20: loss=7.5937, RMSE_E_per_atom=4896.2 meV, RMSE_F=721.7 meV / A\n",
      "2025-06-03 18:49:09.545 INFO: Epoch 22: loss=8.3074, RMSE_E_per_atom=4960.3 meV, RMSE_F=766.1 meV / A\n",
      "2025-06-03 18:49:10.042 INFO: Epoch 24: loss=6.9678, RMSE_E_per_atom=4811.4 meV, RMSE_F=683.5 meV / A\n",
      "2025-06-03 18:49:10.565 INFO: Epoch 26: loss=6.3775, RMSE_E_per_atom=4692.8 meV, RMSE_F=647.1 meV / A\n",
      "2025-06-03 18:49:11.094 INFO: Epoch 28: loss=6.0383, RMSE_E_per_atom=4714.8 meV, RMSE_F=618.8 meV / A\n",
      "2025-06-03 18:49:11.615 INFO: Epoch 30: loss=5.4743, RMSE_E_per_atom=4629.9 meV, RMSE_F=578.4 meV / A\n",
      "2025-06-03 18:49:12.141 INFO: Epoch 32: loss=4.9507, RMSE_E_per_atom=4541.2 meV, RMSE_F=538.8 meV / A\n",
      "2025-06-03 18:49:12.664 INFO: Epoch 34: loss=4.9651, RMSE_E_per_atom=4482.9 meV, RMSE_F=544.5 meV / A\n",
      "2025-06-03 18:49:12.878 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:49:13.153 INFO: Epoch 36: loss=1991.9498, RMSE_E_per_atom=4460.1 meV, RMSE_F=520.2 meV / A\n",
      "2025-06-03 18:49:13.683 INFO: Epoch 38: loss=1866.7834, RMSE_E_per_atom=4317.6 meV, RMSE_F=514.6 meV / A\n",
      "2025-06-03 18:49:14.211 INFO: Epoch 40: loss=1673.7764, RMSE_E_per_atom=4087.3 meV, RMSE_F=565.7 meV / A\n",
      "2025-06-03 18:49:14.743 INFO: Epoch 42: loss=1437.5045, RMSE_E_per_atom=3784.6 meV, RMSE_F=722.7 meV / A\n",
      "2025-06-03 18:49:15.254 INFO: Epoch 44: loss=1175.6040, RMSE_E_per_atom=3414.4 meV, RMSE_F=990.4 meV / A\n",
      "2025-06-03 18:49:15.791 INFO: Epoch 46: loss=903.5542, RMSE_E_per_atom=2975.2 meV, RMSE_F=1357.7 meV / A\n",
      "2025-06-03 18:49:16.323 INFO: Epoch 48: loss=638.5849, RMSE_E_per_atom=2460.6 meV, RMSE_F=1822.2 meV / A\n",
      "2025-06-03 18:49:16.571 INFO: Training complete\n",
      "2025-06-03 18:49:16.571 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:49:17.112 INFO: Loading checkpoint: MACE_models/mace_benchmark_batch64_run-123_epoch-32.pt\n",
      "2025-06-03 18:49:17.136 INFO: Loaded model from epoch 32\n",
      "2025-06-03 18:49:17.136 INFO: Evaluating train ...\n",
      "2025-06-03 18:49:17.250 INFO: Evaluating valid ...\n",
      "2025-06-03 18:49:17.311 INFO: Evaluating Default ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:49:21.208 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        4397.5       |      549.3       |       24.33       |\n",
      "|    valid    |        4541.2       |      538.8       |       20.83       |\n",
      "|   Default   |        4460.6       |      545.7       |       23.83       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:49:21.209 INFO: Saving model to MACE_models/mace_benchmark_batch64_run-123.model\n",
      "2025-06-03 18:49:21.289 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_batch64_compiled.model\n",
      "2025-06-03 18:49:21.883 INFO: Loading checkpoint: MACE_models/mace_benchmark_batch64_run-123_epoch-48_swa.pt\n",
      "2025-06-03 18:49:21.901 INFO: Loaded model from epoch 48\n",
      "2025-06-03 18:49:21.902 INFO: Evaluating train ...\n",
      "2025-06-03 18:49:21.992 INFO: Evaluating valid ...\n",
      "2025-06-03 18:49:22.048 INFO: Evaluating Default ...\n",
      "2025-06-03 18:49:26.319 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        2380.6       |      1818.9      |       75.97       |\n",
      "|    valid    |        2460.6       |      1822.2      |       70.45       |\n",
      "|   Default   |        2410.7       |      1792.9      |       78.29       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:49:26.319 INFO: Saving model to MACE_models/mace_benchmark_batch64_run-123_swa.model\n",
      "2025-06-03 18:49:26.399 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_batch64_swa_compiled.model\n",
      "2025-06-03 18:49:27.028 INFO: Done\n",
      "âœ… Found: RMSE_E = 2460.6 meV, RMSE_F = 1822.2 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_channels64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:49:31.303 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:49:31.303 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_channels64.yaml', name='mace_benchmark_channels64', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:49:31.397 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:49:31.432 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:49:31.504 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:49:31.504 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:49:31.508 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:49:31.508 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:49:31.768 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:49:31.788 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:49:31.788 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:49:31.789 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:49:31.789 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:49:31.996 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:49:32.151 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:49:32.151 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:49:32.191 INFO: Building model\n",
      "2025-06-03 18:49:32.191 INFO: Hidden irreps: 64x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:49:32.874 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:49:32.934 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:49:32.935 INFO: Number of parameters: 134928\n",
      "2025-06-03 18:49:32.936 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:49:32.936 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:49:32.936 INFO: Started training\n",
      "2025-06-03 18:49:33.647 INFO: Epoch None: loss=69.0268, RMSE_E_per_atom=6238.6 meV, RMSE_F=2558.9 meV / A\n",
      "2025-06-03 18:49:39.941 INFO: Epoch 0: loss=25.2036, RMSE_E_per_atom=4891.4 meV, RMSE_F=1513.5 meV / A\n",
      "2025-06-03 18:49:43.401 INFO: Epoch 2: loss=6.3951, RMSE_E_per_atom=4052.1 meV, RMSE_F=689.1 meV / A\n",
      "2025-06-03 18:49:46.197 INFO: Epoch 4: loss=3.3535, RMSE_E_per_atom=3041.8 meV, RMSE_F=492.9 meV / A\n",
      "2025-06-03 18:49:49.010 INFO: Epoch 6: loss=2.1942, RMSE_E_per_atom=1867.0 meV, RMSE_F=430.0 meV / A\n",
      "2025-06-03 18:49:51.876 INFO: Epoch 8: loss=2.3119, RMSE_E_per_atom=1212.5 meV, RMSE_F=466.0 meV / A\n",
      "2025-06-03 18:49:54.643 INFO: Epoch 10: loss=1.6583, RMSE_E_per_atom=874.8 meV, RMSE_F=398.4 meV / A\n",
      "2025-06-03 18:49:57.484 INFO: Epoch 12: loss=1.7964, RMSE_E_per_atom=598.0 meV, RMSE_F=419.3 meV / A\n",
      "2025-06-03 18:50:00.317 INFO: Epoch 14: loss=1.8545, RMSE_E_per_atom=417.6 meV, RMSE_F=428.8 meV / A\n",
      "2025-06-03 18:50:03.255 INFO: Epoch 16: loss=1.3223, RMSE_E_per_atom=392.7 meV, RMSE_F=362.4 meV / A\n",
      "2025-06-03 18:50:06.287 INFO: Epoch 18: loss=0.8177, RMSE_E_per_atom=323.5 meV, RMSE_F=284.3 meV / A\n",
      "2025-06-03 18:50:09.275 INFO: Epoch 20: loss=0.8230, RMSE_E_per_atom=291.9 meV, RMSE_F=286.4 meV / A\n",
      "2025-06-03 18:50:12.161 INFO: Epoch 22: loss=0.7683, RMSE_E_per_atom=287.4 meV, RMSE_F=276.0 meV / A\n",
      "2025-06-03 18:50:15.145 INFO: Epoch 24: loss=1.2456, RMSE_E_per_atom=82.8 meV, RMSE_F=353.5 meV / A\n",
      "2025-06-03 18:50:18.115 INFO: Epoch 26: loss=0.7892, RMSE_E_per_atom=72.9 meV, RMSE_F=280.7 meV / A\n",
      "2025-06-03 18:50:20.931 INFO: Epoch 28: loss=0.6072, RMSE_E_per_atom=169.7 meV, RMSE_F=246.1 meV / A\n",
      "2025-06-03 18:50:23.910 INFO: Epoch 30: loss=0.6987, RMSE_E_per_atom=209.9 meV, RMSE_F=263.7 meV / A\n",
      "2025-06-03 18:50:26.855 INFO: Epoch 32: loss=0.5596, RMSE_E_per_atom=172.1 meV, RMSE_F=236.4 meV / A\n",
      "2025-06-03 18:50:29.838 INFO: Epoch 34: loss=0.9698, RMSE_E_per_atom=240.3 meV, RMSE_F=310.0 meV / A\n",
      "2025-06-03 18:50:31.222 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:50:32.669 INFO: Epoch 36: loss=1.3343, RMSE_E_per_atom=80.1 meV, RMSE_F=263.6 meV / A\n",
      "2025-06-03 18:50:35.604 INFO: Epoch 38: loss=0.4396, RMSE_E_per_atom=16.4 meV, RMSE_F=203.6 meV / A\n",
      "2025-06-03 18:50:38.589 INFO: Epoch 40: loss=0.3891, RMSE_E_per_atom=8.4 meV, RMSE_F=195.9 meV / A\n",
      "2025-06-03 18:50:41.627 INFO: Epoch 42: loss=0.4355, RMSE_E_per_atom=22.4 meV, RMSE_F=196.5 meV / A\n",
      "2025-06-03 18:50:44.543 INFO: Epoch 44: loss=0.4077, RMSE_E_per_atom=18.3 meV, RMSE_F=193.8 meV / A\n",
      "2025-06-03 18:50:47.441 INFO: Epoch 46: loss=0.4339, RMSE_E_per_atom=20.5 meV, RMSE_F=198.3 meV / A\n",
      "2025-06-03 18:50:50.343 INFO: Epoch 48: loss=0.3911, RMSE_E_per_atom=12.2 meV, RMSE_F=194.2 meV / A\n",
      "2025-06-03 18:50:51.758 INFO: Training complete\n",
      "2025-06-03 18:50:51.759 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:50:52.325 INFO: Loading checkpoint: MACE_models/mace_benchmark_channels64_run-123_epoch-32.pt\n",
      "2025-06-03 18:50:52.352 INFO: Loaded model from epoch 32\n",
      "2025-06-03 18:50:52.352 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:50:52.997 INFO: Evaluating valid ...\n",
      "2025-06-03 18:50:53.071 INFO: Evaluating Default ...\n",
      "2025-06-03 18:50:57.334 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        158.0        |      231.5       |       10.63       |\n",
      "|    valid    |        172.1        |      236.4       |        9.14       |\n",
      "|   Default   |        160.7        |      257.0       |       11.22       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:50:57.334 INFO: Saving model to MACE_models/mace_benchmark_channels64_run-123.model\n",
      "2025-06-03 18:50:57.431 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_channels64_compiled.model\n",
      "2025-06-03 18:50:58.109 INFO: Loading checkpoint: MACE_models/mace_benchmark_channels64_run-123_epoch-40_swa.pt\n",
      "2025-06-03 18:50:58.136 INFO: Loaded model from epoch 40\n",
      "2025-06-03 18:50:58.137 INFO: Evaluating train ...\n",
      "2025-06-03 18:50:58.818 INFO: Evaluating valid ...\n",
      "2025-06-03 18:50:58.895 INFO: Evaluating Default ...\n",
      "2025-06-03 18:51:03.091 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         9.5         |      171.8       |        7.89       |\n",
      "|    valid    |         8.4         |      195.9       |        7.57       |\n",
      "|   Default   |         9.7         |      204.8       |        8.94       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:51:03.091 INFO: Saving model to MACE_models/mace_benchmark_channels64_run-123_swa.model\n",
      "2025-06-03 18:51:03.185 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_channels64_swa_compiled.model\n",
      "2025-06-03 18:51:03.822 INFO: Done\n",
      "âœ… Found: RMSE_E = 8.4 meV, RMSE_F = 195.9 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_channels128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:51:08.237 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:51:08.237 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_channels128.yaml', name='mace_benchmark_channels128', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=128, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:51:08.333 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:51:08.371 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:51:08.441 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:51:08.442 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:51:08.446 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:51:08.446 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:51:08.702 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:51:08.723 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:51:08.723 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:51:08.724 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:51:08.724 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:51:08.931 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:51:09.086 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:51:09.086 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:51:09.124 INFO: Building model\n",
      "2025-06-03 18:51:09.125 INFO: Hidden irreps: 128x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:51:09.819 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:51:09.879 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 128x0e | 384 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "      (conv_tp): TensorProduct(128x0e x 1x0e+1x1o+1x2e -> 128x0e+128x1o+128x2e | 384 paths | 384 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 384]\n",
      "      (linear): Linear(128x0e+128x1o+128x2e -> 128x0e+128x1o+128x2e | 49152 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(128x0e+128x1o+128x2e x 3x0e -> 128x0e+128x1o+128x2e | 147456 paths | 147456 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "      (conv_tp): TensorProduct(128x0e x 1x0e+1x1o+1x2e -> 128x0e+128x1o+128x2e | 384 paths | 384 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 384]\n",
      "      (linear): Linear(128x0e+128x1o+128x2e -> 128x0e+128x1o+128x2e | 49152 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(128x0e x 3x0e -> 128x0e | 49152 paths | 49152 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x128 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(128x0e -> 128x0e | 16384 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(128x0e -> 1x0e | 128 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(128x0e -> 16x0e | 2048 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:51:09.881 INFO: Number of parameters: 432656\n",
      "2025-06-03 18:51:09.881 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:51:09.881 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:51:09.881 INFO: Started training\n",
      "2025-06-03 18:51:10.597 INFO: Epoch None: loss=70.9322, RMSE_E_per_atom=6262.8 meV, RMSE_F=2595.8 meV / A\n",
      "2025-06-03 18:51:17.165 INFO: Epoch 0: loss=25.0471, RMSE_E_per_atom=4920.1 meV, RMSE_F=1506.7 meV / A\n",
      "2025-06-03 18:51:21.119 INFO: Epoch 2: loss=10.5401, RMSE_E_per_atom=3883.5 meV, RMSE_F=951.0 meV / A\n",
      "2025-06-03 18:51:24.399 INFO: Epoch 4: loss=4.2860, RMSE_E_per_atom=2678.2 meV, RMSE_F=598.0 meV / A\n",
      "2025-06-03 18:51:27.765 INFO: Epoch 6: loss=3.3319, RMSE_E_per_atom=2122.5 meV, RMSE_F=537.3 meV / A\n",
      "2025-06-03 18:51:31.121 INFO: Epoch 8: loss=2.2299, RMSE_E_per_atom=1906.1 meV, RMSE_F=433.6 meV / A\n",
      "2025-06-03 18:51:34.432 INFO: Epoch 10: loss=2.1384, RMSE_E_per_atom=1083.6 meV, RMSE_F=450.6 meV / A\n",
      "2025-06-03 18:51:37.791 INFO: Epoch 12: loss=1.7551, RMSE_E_per_atom=695.4 meV, RMSE_F=413.6 meV / A\n",
      "2025-06-03 18:51:41.212 INFO: Epoch 14: loss=1.9294, RMSE_E_per_atom=620.1 meV, RMSE_F=435.9 meV / A\n",
      "2025-06-03 18:51:44.519 INFO: Epoch 16: loss=2.1528, RMSE_E_per_atom=653.9 meV, RMSE_F=459.5 meV / A\n",
      "2025-06-03 18:51:47.813 INFO: Epoch 18: loss=1.4294, RMSE_E_per_atom=511.4 meV, RMSE_F=374.8 meV / A\n",
      "2025-06-03 18:51:51.319 INFO: Epoch 20: loss=0.8798, RMSE_E_per_atom=412.6 meV, RMSE_F=294.6 meV / A\n",
      "2025-06-03 18:51:54.805 INFO: Epoch 22: loss=0.9480, RMSE_E_per_atom=252.5 meV, RMSE_F=307.7 meV / A\n",
      "2025-06-03 18:51:58.092 INFO: Epoch 24: loss=1.5125, RMSE_E_per_atom=283.0 meV, RMSE_F=388.7 meV / A\n",
      "2025-06-03 18:52:01.527 INFO: Epoch 26: loss=0.8921, RMSE_E_per_atom=157.4 meV, RMSE_F=298.6 meV / A\n",
      "2025-06-03 18:52:04.777 INFO: Epoch 28: loss=0.8083, RMSE_E_per_atom=114.8 meV, RMSE_F=284.5 meV / A\n",
      "2025-06-03 18:52:08.255 INFO: Epoch 30: loss=0.6927, RMSE_E_per_atom=130.1 meV, RMSE_F=263.4 meV / A\n",
      "2025-06-03 18:52:11.800 INFO: Epoch 32: loss=0.8639, RMSE_E_per_atom=128.5 meV, RMSE_F=293.8 meV / A\n",
      "2025-06-03 18:52:15.123 INFO: Epoch 34: loss=0.8807, RMSE_E_per_atom=126.8 meV, RMSE_F=297.0 meV / A\n",
      "2025-06-03 18:52:16.745 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:52:18.427 INFO: Epoch 36: loss=0.7224, RMSE_E_per_atom=53.2 meV, RMSE_F=210.2 meV / A\n",
      "2025-06-03 18:52:21.861 INFO: Epoch 38: loss=0.7797, RMSE_E_per_atom=60.8 meV, RMSE_F=203.2 meV / A\n",
      "2025-06-03 18:52:25.180 INFO: Epoch 40: loss=0.5262, RMSE_E_per_atom=38.1 meV, RMSE_F=195.9 meV / A\n",
      "2025-06-03 18:52:28.713 INFO: Epoch 42: loss=0.4069, RMSE_E_per_atom=16.0 meV, RMSE_F=195.8 meV / A\n",
      "2025-06-03 18:52:32.172 INFO: Epoch 44: loss=0.3926, RMSE_E_per_atom=17.8 meV, RMSE_F=190.6 meV / A\n",
      "2025-06-03 18:52:35.670 INFO: Epoch 46: loss=0.4303, RMSE_E_per_atom=25.4 meV, RMSE_F=191.9 meV / A\n",
      "2025-06-03 18:52:38.960 INFO: Epoch 48: loss=0.3672, RMSE_E_per_atom=11.7 meV, RMSE_F=188.6 meV / A\n",
      "2025-06-03 18:52:40.732 INFO: Training complete\n",
      "2025-06-03 18:52:40.732 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:52:41.294 INFO: Loading checkpoint: MACE_models/mace_benchmark_channels128_run-123_epoch-30.pt\n",
      "2025-06-03 18:52:41.324 INFO: Loaded model from epoch 30\n",
      "2025-06-03 18:52:41.325 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:52:42.001 INFO: Evaluating valid ...\n",
      "2025-06-03 18:52:42.075 INFO: Evaluating Default ...\n",
      "2025-06-03 18:52:46.325 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        117.2        |      247.5       |       11.37       |\n",
      "|    valid    |        130.1        |      263.4       |       10.18       |\n",
      "|   Default   |        116.2        |      266.4       |       11.63       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:52:46.325 INFO: Saving model to MACE_models/mace_benchmark_channels128_run-123.model\n",
      "2025-06-03 18:52:46.463 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_channels128_compiled.model\n",
      "2025-06-03 18:52:47.184 INFO: Loading checkpoint: MACE_models/mace_benchmark_channels128_run-123_epoch-48_swa.pt\n",
      "2025-06-03 18:52:47.224 INFO: Loaded model from epoch 48\n",
      "2025-06-03 18:52:47.224 INFO: Evaluating train ...\n",
      "2025-06-03 18:52:47.930 INFO: Evaluating valid ...\n",
      "2025-06-03 18:52:48.021 INFO: Evaluating Default ...\n",
      "2025-06-03 18:52:52.197 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         14.4        |      164.8       |        7.57       |\n",
      "|    valid    |         11.7        |      188.6       |        7.29       |\n",
      "|   Default   |         13.6        |      194.1       |        8.48       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:52:52.197 INFO: Saving model to MACE_models/mace_benchmark_channels128_run-123_swa.model\n",
      "2025-06-03 18:52:52.350 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_channels128_swa_compiled.model\n",
      "2025-06-03 18:52:53.014 INFO: Done\n",
      "âœ… Found: RMSE_E = 11.7 meV, RMSE_F = 188.6 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_interactions3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:52:57.052 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:52:57.052 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_interactions3.yaml', name='mace_benchmark_interactions3', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:52:57.145 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:52:57.183 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:52:57.253 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:52:57.253 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:52:57.258 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:52:57.258 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:52:57.512 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:52:57.532 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:52:57.532 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:52:57.533 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:52:57.533 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:52:57.736 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:52:57.889 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:52:57.889 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:52:57.928 INFO: Building model\n",
      "2025-06-03 18:52:57.928 INFO: Hidden irreps: 32x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:52:58.878 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:52:58.963 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1-2): 2 x RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-2): 3 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:52:58.965 INFO: Number of parameters: 77104\n",
      "2025-06-03 18:52:58.965 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:52:58.965 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:52:58.965 INFO: Started training\n",
      "2025-06-03 18:52:59.713 INFO: Epoch None: loss=71.4399, RMSE_E_per_atom=6290.1 meV, RMSE_F=2604.8 meV / A\n",
      "2025-06-03 18:53:06.240 INFO: Epoch 0: loss=31.2661, RMSE_E_per_atom=5671.7 meV, RMSE_F=1681.0 meV / A\n",
      "2025-06-03 18:53:10.219 INFO: Epoch 2: loss=8.5055, RMSE_E_per_atom=5430.1 meV, RMSE_F=746.1 meV / A\n",
      "2025-06-03 18:53:13.432 INFO: Epoch 4: loss=5.0786, RMSE_E_per_atom=4680.0 meV, RMSE_F=538.1 meV / A\n",
      "2025-06-03 18:53:16.697 INFO: Epoch 6: loss=3.5806, RMSE_E_per_atom=4138.3 meV, RMSE_F=433.3 meV / A\n",
      "2025-06-03 18:53:19.936 INFO: Epoch 8: loss=2.6942, RMSE_E_per_atom=3407.7 meV, RMSE_F=392.3 meV / A\n",
      "2025-06-03 18:53:23.214 INFO: Epoch 10: loss=2.1843, RMSE_E_per_atom=2422.0 meV, RMSE_F=400.7 meV / A\n",
      "2025-06-03 18:53:26.469 INFO: Epoch 12: loss=1.5327, RMSE_E_per_atom=1624.9 meV, RMSE_F=356.9 meV / A\n",
      "2025-06-03 18:53:29.791 INFO: Epoch 14: loss=2.0653, RMSE_E_per_atom=797.4 meV, RMSE_F=448.2 meV / A\n",
      "2025-06-03 18:53:33.123 INFO: Epoch 16: loss=1.7421, RMSE_E_per_atom=947.0 meV, RMSE_F=407.0 meV / A\n",
      "2025-06-03 18:53:36.431 INFO: Epoch 18: loss=1.2341, RMSE_E_per_atom=774.8 meV, RMSE_F=343.4 meV / A\n",
      "2025-06-03 18:53:39.830 INFO: Epoch 20: loss=2.0239, RMSE_E_per_atom=774.5 meV, RMSE_F=443.8 meV / A\n",
      "2025-06-03 18:53:43.167 INFO: Epoch 22: loss=0.7889, RMSE_E_per_atom=447.3 meV, RMSE_F=277.8 meV / A\n",
      "2025-06-03 18:53:46.544 INFO: Epoch 24: loss=0.8536, RMSE_E_per_atom=397.5 meV, RMSE_F=289.7 meV / A\n",
      "2025-06-03 18:53:49.922 INFO: Epoch 26: loss=2.2082, RMSE_E_per_atom=350.3 meV, RMSE_F=468.9 meV / A\n",
      "2025-06-03 18:53:53.226 INFO: Epoch 28: loss=1.0951, RMSE_E_per_atom=323.7 meV, RMSE_F=329.5 meV / A\n",
      "2025-06-03 18:53:56.611 INFO: Epoch 30: loss=0.6958, RMSE_E_per_atom=334.6 meV, RMSE_F=261.7 meV / A\n",
      "2025-06-03 18:54:00.058 INFO: Epoch 32: loss=0.8047, RMSE_E_per_atom=255.9 meV, RMSE_F=282.7 meV / A\n",
      "2025-06-03 18:54:03.443 INFO: Epoch 34: loss=0.7831, RMSE_E_per_atom=301.4 meV, RMSE_F=278.2 meV / A\n",
      "2025-06-03 18:54:04.999 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:54:06.674 INFO: Epoch 36: loss=0.7436, RMSE_E_per_atom=43.4 meV, RMSE_F=236.2 meV / A\n",
      "2025-06-03 18:54:10.021 INFO: Epoch 38: loss=0.6289, RMSE_E_per_atom=39.9 meV, RMSE_F=216.8 meV / A\n",
      "2025-06-03 18:54:13.388 INFO: Epoch 40: loss=0.5566, RMSE_E_per_atom=35.0 meV, RMSE_F=208.6 meV / A\n",
      "2025-06-03 18:54:16.833 INFO: Epoch 42: loss=0.5171, RMSE_E_per_atom=24.6 meV, RMSE_F=213.9 meV / A\n",
      "2025-06-03 18:54:20.316 INFO: Epoch 44: loss=0.4671, RMSE_E_per_atom=15.5 meV, RMSE_F=210.7 meV / A\n",
      "2025-06-03 18:54:23.709 INFO: Epoch 46: loss=0.4430, RMSE_E_per_atom=15.7 meV, RMSE_F=204.7 meV / A\n",
      "2025-06-03 18:54:27.086 INFO: Epoch 48: loss=0.4697, RMSE_E_per_atom=20.0 meV, RMSE_F=207.4 meV / A\n",
      "2025-06-03 18:54:28.680 INFO: Training complete\n",
      "2025-06-03 18:54:28.681 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:54:29.236 INFO: Loading checkpoint: MACE_models/mace_benchmark_interactions3_run-123_epoch-30.pt\n",
      "2025-06-03 18:54:29.269 INFO: Loaded model from epoch 30\n",
      "2025-06-03 18:54:29.269 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:29.979 INFO: Evaluating valid ...\n",
      "2025-06-03 18:54:30.065 INFO: Evaluating Default ...\n",
      "2025-06-03 18:54:35.187 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        349.3        |      241.3       |       11.08       |\n",
      "|    valid    |        334.6        |      261.7       |       10.12       |\n",
      "|   Default   |        348.5        |      271.3       |       11.85       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:54:35.187 INFO: Saving model to MACE_models/mace_benchmark_interactions3_run-123.model\n",
      "2025-06-03 18:54:35.300 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_interactions3_compiled.model\n",
      "2025-06-03 18:54:36.212 INFO: Loading checkpoint: MACE_models/mace_benchmark_interactions3_run-123_epoch-46_swa.pt\n",
      "2025-06-03 18:54:36.238 INFO: Loaded model from epoch 46\n",
      "2025-06-03 18:54:36.238 INFO: Evaluating train ...\n",
      "2025-06-03 18:54:36.995 INFO: Evaluating valid ...\n",
      "2025-06-03 18:54:37.086 INFO: Evaluating Default ...\n",
      "2025-06-03 18:54:42.209 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         14.8        |      189.3       |        8.69       |\n",
      "|    valid    |         15.7        |      204.7       |        7.92       |\n",
      "|   Default   |         14.1        |      227.8       |        9.95       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:54:42.209 INFO: Saving model to MACE_models/mace_benchmark_interactions3_run-123_swa.model\n",
      "2025-06-03 18:54:42.321 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_interactions3_swa_compiled.model\n",
      "2025-06-03 18:54:43.183 INFO: Done\n",
      "âœ… Found: RMSE_E = 15.7 meV, RMSE_F = 204.7 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_interactions4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:47.021 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:54:47.022 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_interactions4.yaml', name='mace_benchmark_interactions4', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=4, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:54:47.117 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:54:47.156 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:54:47.226 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:54:47.227 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:54:47.231 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:54:47.231 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:54:47.485 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:54:47.506 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:54:47.506 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:54:47.507 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:54:47.507 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:54:47.710 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:54:47.865 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:54:47.865 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:54:47.903 INFO: Building model\n",
      "2025-06-03 18:54:47.904 INFO: Hidden irreps: 32x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:54:49.132 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:54:49.241 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1-3): 3 x RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-3): 4 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-2): 3 x LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (3): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:54:49.243 INFO: Number of parameters: 100560\n",
      "2025-06-03 18:54:49.243 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:54:49.243 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:54:49.243 INFO: Started training\n",
      "2025-06-03 18:54:50.029 INFO: Epoch None: loss=71.3611, RMSE_E_per_atom=6298.1 meV, RMSE_F=2603.2 meV / A\n",
      "2025-06-03 18:54:56.783 INFO: Epoch 0: loss=26.2961, RMSE_E_per_atom=5467.5 meV, RMSE_F=1533.4 meV / A\n",
      "2025-06-03 18:55:01.057 INFO: Epoch 2: loss=9.1886, RMSE_E_per_atom=5473.2 meV, RMSE_F=788.8 meV / A\n",
      "2025-06-03 18:55:04.719 INFO: Epoch 4: loss=5.3587, RMSE_E_per_atom=4819.6 meV, RMSE_F=552.7 meV / A\n",
      "2025-06-03 18:55:08.436 INFO: Epoch 6: loss=4.3816, RMSE_E_per_atom=4329.4 meV, RMSE_F=501.4 meV / A\n",
      "2025-06-03 18:55:12.173 INFO: Epoch 8: loss=3.8269, RMSE_E_per_atom=3871.9 meV, RMSE_F=482.7 meV / A\n",
      "2025-06-03 18:55:15.918 INFO: Epoch 10: loss=2.4901, RMSE_E_per_atom=2941.4 meV, RMSE_F=404.1 meV / A\n",
      "2025-06-03 18:55:19.642 INFO: Epoch 12: loss=2.2405, RMSE_E_per_atom=2353.6 meV, RMSE_F=412.4 meV / A\n",
      "2025-06-03 18:55:23.357 INFO: Epoch 14: loss=1.1494, RMSE_E_per_atom=1047.7 meV, RMSE_F=322.9 meV / A\n",
      "2025-06-03 18:55:27.059 INFO: Epoch 16: loss=1.8866, RMSE_E_per_atom=876.1 meV, RMSE_F=426.1 meV / A\n",
      "2025-06-03 18:55:30.723 INFO: Epoch 18: loss=1.7610, RMSE_E_per_atom=931.6 meV, RMSE_F=410.1 meV / A\n",
      "2025-06-03 18:55:34.358 INFO: Epoch 20: loss=1.1467, RMSE_E_per_atom=779.4 meV, RMSE_F=329.8 meV / A\n",
      "2025-06-03 18:55:38.116 INFO: Epoch 22: loss=0.9616, RMSE_E_per_atom=429.4 meV, RMSE_F=307.8 meV / A\n",
      "2025-06-03 18:55:41.990 INFO: Epoch 24: loss=0.8157, RMSE_E_per_atom=290.2 meV, RMSE_F=284.3 meV / A\n",
      "2025-06-03 18:55:45.769 INFO: Epoch 26: loss=1.1629, RMSE_E_per_atom=435.8 meV, RMSE_F=338.4 meV / A\n",
      "2025-06-03 18:55:49.362 INFO: Epoch 28: loss=0.6976, RMSE_E_per_atom=303.4 meV, RMSE_F=262.7 meV / A\n",
      "2025-06-03 18:55:53.175 INFO: Epoch 30: loss=1.2718, RMSE_E_per_atom=244.2 meV, RMSE_F=356.1 meV / A\n",
      "2025-06-03 18:55:56.881 INFO: Epoch 32: loss=0.9233, RMSE_E_per_atom=313.5 meV, RMSE_F=302.4 meV / A\n",
      "2025-06-03 18:56:00.560 INFO: Epoch 34: loss=0.8059, RMSE_E_per_atom=247.4 meV, RMSE_F=283.2 meV / A\n",
      "2025-06-03 18:56:02.322 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:56:04.211 INFO: Epoch 36: loss=2.8298, RMSE_E_per_atom=150.9 meV, RMSE_F=235.2 meV / A\n",
      "2025-06-03 18:56:07.924 INFO: Epoch 38: loss=1.3949, RMSE_E_per_atom=91.2 meV, RMSE_F=236.7 meV / A\n",
      "2025-06-03 18:56:11.819 INFO: Epoch 40: loss=0.7202, RMSE_E_per_atom=44.5 meV, RMSE_F=228.4 meV / A\n",
      "2025-06-03 18:56:15.528 INFO: Epoch 42: loss=0.6200, RMSE_E_per_atom=33.5 meV, RMSE_F=225.3 meV / A\n",
      "2025-06-03 18:56:19.405 INFO: Epoch 44: loss=0.6052, RMSE_E_per_atom=25.0 meV, RMSE_F=232.9 meV / A\n",
      "2025-06-03 18:56:23.229 INFO: Epoch 46: loss=0.5501, RMSE_E_per_atom=27.5 meV, RMSE_F=217.8 meV / A\n",
      "2025-06-03 18:56:27.121 INFO: Epoch 48: loss=0.7870, RMSE_E_per_atom=52.5 meV, RMSE_F=226.2 meV / A\n",
      "2025-06-03 18:56:28.945 INFO: Training complete\n",
      "2025-06-03 18:56:28.945 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:56:29.507 INFO: Loading checkpoint: MACE_models/mace_benchmark_interactions4_run-123_epoch-28.pt\n",
      "2025-06-03 18:56:29.550 INFO: Loaded model from epoch 28\n",
      "2025-06-03 18:56:29.550 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:56:30.351 INFO: Evaluating valid ...\n",
      "2025-06-03 18:56:30.443 INFO: Evaluating Default ...\n",
      "2025-06-03 18:56:35.923 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        316.6        |      251.3       |       11.54       |\n",
      "|    valid    |        303.4        |      262.7       |       10.16       |\n",
      "|   Default   |        317.8        |      274.3       |       11.98       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:56:35.923 INFO: Saving model to MACE_models/mace_benchmark_interactions4_run-123.model\n",
      "2025-06-03 18:56:36.062 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_interactions4_compiled.model\n",
      "2025-06-03 18:56:37.141 INFO: Loading checkpoint: MACE_models/mace_benchmark_interactions4_run-123_epoch-46_swa.pt\n",
      "2025-06-03 18:56:37.175 INFO: Loaded model from epoch 46\n",
      "2025-06-03 18:56:37.176 INFO: Evaluating train ...\n",
      "2025-06-03 18:56:38.017 INFO: Evaluating valid ...\n",
      "2025-06-03 18:56:38.120 INFO: Evaluating Default ...\n",
      "2025-06-03 18:56:43.688 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         19.6        |      199.9       |        9.18       |\n",
      "|    valid    |         27.5        |      217.8       |        8.42       |\n",
      "|   Default   |         20.7        |      226.8       |        9.91       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:56:43.688 INFO: Saving model to MACE_models/mace_benchmark_interactions4_run-123_swa.model\n",
      "2025-06-03 18:56:43.826 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_interactions4_swa_compiled.model\n",
      "2025-06-03 18:56:44.988 INFO: Done\n",
      "âœ… Found: RMSE_E = 27.5 meV, RMSE_F = 217.8 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_maxL1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:56:48.402 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:56:48.402 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_maxL1.yaml', name='mace_benchmark_maxL1', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=1, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:56:48.490 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:56:48.528 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:56:48.598 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:56:48.599 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:56:48.603 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:56:48.603 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:56:48.857 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:56:48.878 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:56:48.878 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:56:48.879 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:56:48.879 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:56:49.083 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:56:49.238 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:56:49.238 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:56:49.278 INFO: Building model\n",
      "2025-06-03 18:56:49.278 INFO: Hidden irreps: 32x0e+32x1o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:56:50.011 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:56:50.079 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e+32x1o -> 32x0e+32x1o | 2048 weights)\n",
      "      (conv_tp): TensorProduct(32x0e+32x1o x 1x0e+1x1o+1x2e -> 64x0e+96x1o+64x2e | 224 paths | 224 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 224]\n",
      "      (linear): Linear(64x0e+96x1o+64x2e -> 32x0e+32x1o+32x2e | 7168 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0-1): 2 x Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e+32x1o -> 32x0e+32x1o | 2048 weights)\n",
      "    )\n",
      "    (1): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e+32x1o -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:56:50.080 INFO: Number of parameters: 68464\n",
      "2025-06-03 18:56:50.081 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:56:50.081 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:56:50.081 INFO: Started training\n",
      "2025-06-03 18:56:50.824 INFO: Epoch None: loss=70.0901, RMSE_E_per_atom=6189.6 meV, RMSE_F=2581.3 meV / A\n",
      "2025-06-03 18:56:57.318 INFO: Epoch 0: loss=31.2198, RMSE_E_per_atom=5094.9 meV, RMSE_F=1700.0 meV / A\n",
      "2025-06-03 18:57:01.012 INFO: Epoch 2: loss=9.0581, RMSE_E_per_atom=4274.1 meV, RMSE_F=851.7 meV / A\n",
      "2025-06-03 18:57:04.044 INFO: Epoch 4: loss=5.6601, RMSE_E_per_atom=3832.4 meV, RMSE_F=649.5 meV / A\n",
      "2025-06-03 18:57:07.445 INFO: Epoch 6: loss=3.8395, RMSE_E_per_atom=3670.0 meV, RMSE_F=500.2 meV / A\n",
      "2025-06-03 18:57:11.140 INFO: Epoch 8: loss=3.3756, RMSE_E_per_atom=2774.2 meV, RMSE_F=511.4 meV / A\n",
      "2025-06-03 18:57:14.713 INFO: Epoch 10: loss=2.6328, RMSE_E_per_atom=1808.8 meV, RMSE_F=481.4 meV / A\n",
      "2025-06-03 18:57:18.315 INFO: Epoch 12: loss=2.8217, RMSE_E_per_atom=1369.3 meV, RMSE_F=514.6 meV / A\n",
      "2025-06-03 18:57:21.851 INFO: Epoch 14: loss=1.7536, RMSE_E_per_atom=904.9 meV, RMSE_F=410.4 meV / A\n",
      "2025-06-03 18:57:25.513 INFO: Epoch 16: loss=1.1444, RMSE_E_per_atom=777.7 meV, RMSE_F=330.3 meV / A\n",
      "2025-06-03 18:57:29.090 INFO: Epoch 18: loss=1.0882, RMSE_E_per_atom=453.2 meV, RMSE_F=327.3 meV / A\n",
      "2025-06-03 18:57:32.692 INFO: Epoch 20: loss=1.6214, RMSE_E_per_atom=553.0 meV, RMSE_F=400.1 meV / A\n",
      "2025-06-03 18:57:36.251 INFO: Epoch 22: loss=1.2854, RMSE_E_per_atom=380.5 meV, RMSE_F=357.5 meV / A\n",
      "2025-06-03 18:57:39.837 INFO: Epoch 24: loss=1.2097, RMSE_E_per_atom=383.3 meV, RMSE_F=347.7 meV / A\n",
      "2025-06-03 18:57:43.356 INFO: Epoch 26: loss=1.3292, RMSE_E_per_atom=285.5 meV, RMSE_F=364.2 meV / A\n",
      "2025-06-03 18:57:46.905 INFO: Epoch 28: loss=0.7158, RMSE_E_per_atom=300.8 meV, RMSE_F=267.2 meV / A\n",
      "2025-06-03 18:57:50.440 INFO: Epoch 30: loss=0.9298, RMSE_E_per_atom=270.1 meV, RMSE_F=304.1 meV / A\n",
      "2025-06-03 18:57:54.056 INFO: Epoch 32: loss=1.0567, RMSE_E_per_atom=365.3 meV, RMSE_F=323.8 meV / A\n",
      "2025-06-03 18:57:57.673 INFO: Epoch 34: loss=1.0793, RMSE_E_per_atom=260.2 meV, RMSE_F=328.2 meV / A\n",
      "2025-06-03 18:57:59.361 INFO: Changing loss based on SWA\n",
      "2025-06-03 18:58:01.132 INFO: Epoch 36: loss=4.5698, RMSE_E_per_atom=195.9 meV, RMSE_F=271.8 meV / A\n",
      "2025-06-03 18:58:04.648 INFO: Epoch 38: loss=0.9141, RMSE_E_per_atom=46.1 meV, RMSE_F=266.0 meV / A\n",
      "2025-06-03 18:58:08.169 INFO: Epoch 40: loss=0.6460, RMSE_E_per_atom=14.0 meV, RMSE_F=251.3 meV / A\n",
      "2025-06-03 18:58:11.792 INFO: Epoch 42: loss=0.6747, RMSE_E_per_atom=12.9 meV, RMSE_F=258.0 meV / A\n",
      "2025-06-03 18:58:15.361 INFO: Epoch 44: loss=0.7435, RMSE_E_per_atom=35.0 meV, RMSE_F=250.6 meV / A\n",
      "2025-06-03 18:58:18.913 INFO: Epoch 46: loss=0.6293, RMSE_E_per_atom=15.9 meV, RMSE_F=247.2 meV / A\n",
      "2025-06-03 18:58:22.552 INFO: Epoch 48: loss=0.5614, RMSE_E_per_atom=9.1 meV, RMSE_F=236.3 meV / A\n",
      "2025-06-03 18:58:24.265 INFO: Training complete\n",
      "2025-06-03 18:58:24.266 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 18:58:24.833 INFO: Loading checkpoint: MACE_models/mace_benchmark_maxL1_run-123_epoch-28.pt\n",
      "2025-06-03 18:58:24.859 INFO: Loaded model from epoch 28\n",
      "2025-06-03 18:58:24.859 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:58:25.654 INFO: Evaluating valid ...\n",
      "2025-06-03 18:58:25.733 INFO: Evaluating Default ...\n",
      "2025-06-03 18:58:31.112 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        204.7        |      212.0       |        9.73       |\n",
      "|    valid    |        300.8        |      267.2       |       10.33       |\n",
      "|   Default   |        223.3        |      255.7       |       11.16       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:58:31.112 INFO: Saving model to MACE_models/mace_benchmark_maxL1_run-123.model\n",
      "2025-06-03 18:58:31.203 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_maxL1_compiled.model\n",
      "2025-06-03 18:58:31.962 INFO: Loading checkpoint: MACE_models/mace_benchmark_maxL1_run-123_epoch-48_swa.pt\n",
      "2025-06-03 18:58:31.989 INFO: Loaded model from epoch 48\n",
      "2025-06-03 18:58:31.989 INFO: Evaluating train ...\n",
      "2025-06-03 18:58:32.844 INFO: Evaluating valid ...\n",
      "2025-06-03 18:58:32.924 INFO: Evaluating Default ...\n",
      "2025-06-03 18:58:38.224 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         8.0         |      165.2       |        7.58       |\n",
      "|    valid    |         9.1         |      236.3       |        9.13       |\n",
      "|   Default   |         8.7         |      213.8       |        9.33       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 18:58:38.224 INFO: Saving model to MACE_models/mace_benchmark_maxL1_run-123_swa.model\n",
      "2025-06-03 18:58:38.313 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_maxL1_swa_compiled.model\n",
      "2025-06-03 18:58:39.031 INFO: Done\n",
      "âœ… Found: RMSE_E = 9.1 meV, RMSE_F = 236.3 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_maxL2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:58:42.831 INFO: MACE version: 0.3.6\n",
      "2025-06-03 18:58:42.831 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_maxL2.yaml', name='mace_benchmark_maxL2', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 18:58:42.922 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 18:58:42.968 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 18:58:43.048 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:58:43.048 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 18:58:43.052 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 18:58:43.052 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 18:58:43.308 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 18:58:43.328 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 18:58:43.328 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 18:58:43.329 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 18:58:43.329 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 18:58:43.531 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 18:58:43.685 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 18:58:43.685 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 18:58:43.724 INFO: Building model\n",
      "2025-06-03 18:58:43.724 INFO: Hidden irreps: 32x0e+32x1o+32x2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 18:58:44.514 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 18:58:44.589 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (conv_tp): TensorProduct(32x0e+32x1o+32x2e x 1x0e+1x1o+1x2e -> 96x0e+128x1o+128x2e | 352 paths | 352 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 352]\n",
      "      (linear): Linear(96x0e+128x1o+128x2e -> 32x0e+32x1o+32x2e | 11264 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0-2): 3 x Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "    )\n",
      "    (1): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 18:58:44.591 INFO: Number of parameters: 83280\n",
      "2025-06-03 18:58:44.591 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 18:58:44.591 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 18:58:44.591 INFO: Started training\n",
      "2025-06-03 18:58:45.333 INFO: Epoch None: loss=70.1735, RMSE_E_per_atom=6265.2 meV, RMSE_F=2581.0 meV / A\n",
      "2025-06-03 18:58:52.056 INFO: Epoch 0: loss=28.8420, RMSE_E_per_atom=5393.2 meV, RMSE_F=1618.8 meV / A\n",
      "2025-06-03 18:58:56.175 INFO: Epoch 2: loss=6.5984, RMSE_E_per_atom=4681.1 meV, RMSE_F=664.7 meV / A\n",
      "2025-06-03 18:58:59.877 INFO: Epoch 4: loss=4.2767, RMSE_E_per_atom=3926.8 meV, RMSE_F=524.5 meV / A\n",
      "2025-06-03 18:59:03.744 INFO: Epoch 6: loss=2.9493, RMSE_E_per_atom=2937.2 meV, RMSE_F=457.4 meV / A\n",
      "2025-06-03 18:59:07.611 INFO: Epoch 8: loss=2.1387, RMSE_E_per_atom=1957.3 meV, RMSE_F=420.4 meV / A\n",
      "2025-06-03 18:59:11.546 INFO: Epoch 10: loss=1.7596, RMSE_E_per_atom=1486.6 meV, RMSE_F=393.7 meV / A\n",
      "2025-06-03 18:59:15.420 INFO: Epoch 12: loss=1.4409, RMSE_E_per_atom=920.7 meV, RMSE_F=369.5 meV / A\n",
      "2025-06-03 18:59:19.247 INFO: Epoch 14: loss=1.7067, RMSE_E_per_atom=724.2 meV, RMSE_F=407.6 meV / A\n",
      "2025-06-03 18:59:22.998 INFO: Epoch 16: loss=1.0728, RMSE_E_per_atom=453.1 meV, RMSE_F=325.0 meV / A\n",
      "2025-06-03 18:59:26.876 INFO: Epoch 18: loss=1.2900, RMSE_E_per_atom=388.8 meV, RMSE_F=358.6 meV / A\n",
      "2025-06-03 18:59:30.731 INFO: Epoch 20: loss=1.1990, RMSE_E_per_atom=426.8 meV, RMSE_F=344.2 meV / A\n",
      "2025-06-03 18:59:34.555 INFO: Epoch 22: loss=0.6059, RMSE_E_per_atom=274.9 meV, RMSE_F=245.3 meV / A\n",
      "2025-06-03 18:59:38.420 INFO: Epoch 24: loss=1.0442, RMSE_E_per_atom=259.9 meV, RMSE_F=323.0 meV / A\n",
      "2025-06-03 18:59:42.241 INFO: Epoch 26: loss=0.8235, RMSE_E_per_atom=458.7 meV, RMSE_F=284.2 meV / A\n",
      "2025-06-03 18:59:46.017 INFO: Epoch 28: loss=0.8519, RMSE_E_per_atom=251.7 meV, RMSE_F=292.3 meV / A\n",
      "2025-06-03 18:59:49.815 INFO: Epoch 30: loss=0.7742, RMSE_E_per_atom=259.0 meV, RMSE_F=278.7 meV / A\n",
      "2025-06-03 18:59:53.717 INFO: Epoch 32: loss=0.8865, RMSE_E_per_atom=328.1 meV, RMSE_F=296.4 meV / A\n",
      "2025-06-03 18:59:57.530 INFO: Epoch 34: loss=1.2715, RMSE_E_per_atom=297.0 meV, RMSE_F=356.9 meV / A\n",
      "2025-06-03 18:59:59.356 INFO: Changing loss based on SWA\n",
      "2025-06-03 19:00:01.321 INFO: Epoch 36: loss=2.3602, RMSE_E_per_atom=136.8 meV, RMSE_F=221.2 meV / A\n",
      "2025-06-03 19:00:05.182 INFO: Epoch 38: loss=0.4462, RMSE_E_per_atom=28.1 meV, RMSE_F=192.2 meV / A\n",
      "2025-06-03 19:00:08.994 INFO: Epoch 40: loss=0.3820, RMSE_E_per_atom=14.3 meV, RMSE_F=191.1 meV / A\n",
      "2025-06-03 19:00:12.868 INFO: Epoch 42: loss=0.3776, RMSE_E_per_atom=8.9 meV, RMSE_F=193.2 meV / A\n",
      "2025-06-03 19:00:16.831 INFO: Epoch 44: loss=0.3637, RMSE_E_per_atom=8.9 meV, RMSE_F=189.5 meV / A\n",
      "2025-06-03 19:00:20.726 INFO: Epoch 46: loss=0.3773, RMSE_E_per_atom=8.5 meV, RMSE_F=193.3 meV / A\n",
      "2025-06-03 19:00:24.594 INFO: Epoch 48: loss=0.3540, RMSE_E_per_atom=9.8 meV, RMSE_F=186.4 meV / A\n",
      "2025-06-03 19:00:26.506 INFO: Training complete\n",
      "2025-06-03 19:00:26.506 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 19:00:27.055 INFO: Loading checkpoint: MACE_models/mace_benchmark_maxL2_run-123_epoch-22.pt\n",
      "2025-06-03 19:00:27.079 INFO: Loaded model from epoch 22\n",
      "2025-06-03 19:00:27.079 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:00:27.935 INFO: Evaluating valid ...\n",
      "2025-06-03 19:00:28.016 INFO: Evaluating Default ...\n",
      "2025-06-03 19:00:33.851 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        251.1        |      213.5       |        9.80       |\n",
      "|    valid    |        274.9        |      245.3       |        9.48       |\n",
      "|   Default   |        261.1        |      251.7       |       10.99       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:00:33.851 INFO: Saving model to MACE_models/mace_benchmark_maxL2_run-123.model\n",
      "2025-06-03 19:00:33.948 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_maxL2_compiled.model\n",
      "2025-06-03 19:00:34.726 INFO: Loading checkpoint: MACE_models/mace_benchmark_maxL2_run-123_epoch-48_swa.pt\n",
      "2025-06-03 19:00:34.748 INFO: Loaded model from epoch 48\n",
      "2025-06-03 19:00:34.748 INFO: Evaluating train ...\n",
      "2025-06-03 19:00:35.620 INFO: Evaluating valid ...\n",
      "2025-06-03 19:00:35.713 INFO: Evaluating Default ...\n",
      "2025-06-03 19:00:41.364 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         7.4         |      140.8       |        6.47       |\n",
      "|    valid    |         9.8         |      186.4       |        7.21       |\n",
      "|   Default   |         7.5         |      186.8       |        8.16       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:00:41.364 INFO: Saving model to MACE_models/mace_benchmark_maxL2_run-123_swa.model\n",
      "2025-06-03 19:00:41.462 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_maxL2_swa_compiled.model\n",
      "2025-06-03 19:00:42.214 INFO: Done\n",
      "âœ… Found: RMSE_E = 9.8 meV, RMSE_F = 186.4 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_correlation3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:00:45.723 INFO: MACE version: 0.3.6\n",
      "2025-06-03 19:00:45.723 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_correlation3.yaml', name='mace_benchmark_correlation3', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 19:00:45.811 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 19:00:45.849 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 19:00:45.919 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:00:45.920 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 19:00:45.924 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 19:00:45.924 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 19:00:46.179 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:00:46.200 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 19:00:46.200 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 19:00:46.201 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 19:00:46.201 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 19:00:46.406 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 19:00:46.561 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 19:00:46.561 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 19:00:46.600 INFO: Building model\n",
      "2025-06-03 19:00:46.601 INFO: Hidden irreps: 32x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:00:47.346 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 19:00:47.411 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x32 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 19:00:47.413 INFO: Number of parameters: 55760\n",
      "2025-06-03 19:00:47.413 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 19:00:47.413 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 19:00:47.413 INFO: Started training\n",
      "2025-06-03 19:00:48.120 INFO: Epoch None: loss=70.1168, RMSE_E_per_atom=6242.6 meV, RMSE_F=2580.3 meV / A\n",
      "2025-06-03 19:00:54.467 INFO: Epoch 0: loss=33.5538, RMSE_E_per_atom=5546.5 meV, RMSE_F=1754.9 meV / A\n",
      "2025-06-03 19:00:57.976 INFO: Epoch 2: loss=10.4096, RMSE_E_per_atom=5161.9 meV, RMSE_F=882.7 meV / A\n",
      "2025-06-03 19:01:00.850 INFO: Epoch 4: loss=4.8708, RMSE_E_per_atom=4148.5 meV, RMSE_F=562.9 meV / A\n",
      "2025-06-03 19:01:03.759 INFO: Epoch 6: loss=3.0602, RMSE_E_per_atom=2681.0 meV, RMSE_F=484.4 meV / A\n",
      "2025-06-03 19:01:06.668 INFO: Epoch 8: loss=2.2652, RMSE_E_per_atom=1375.6 meV, RMSE_F=456.0 meV / A\n",
      "2025-06-03 19:01:09.593 INFO: Epoch 10: loss=1.3572, RMSE_E_per_atom=694.4 meV, RMSE_F=362.9 meV / A\n",
      "2025-06-03 19:01:12.701 INFO: Epoch 12: loss=1.0258, RMSE_E_per_atom=457.5 meV, RMSE_F=317.9 meV / A\n",
      "2025-06-03 19:01:15.768 INFO: Epoch 14: loss=1.1072, RMSE_E_per_atom=568.7 meV, RMSE_F=328.4 meV / A\n",
      "2025-06-03 19:01:18.896 INFO: Epoch 16: loss=1.2298, RMSE_E_per_atom=403.6 meV, RMSE_F=349.0 meV / A\n",
      "2025-06-03 19:01:21.981 INFO: Epoch 18: loss=1.0196, RMSE_E_per_atom=304.6 meV, RMSE_F=318.5 meV / A\n",
      "2025-06-03 19:01:25.068 INFO: Epoch 20: loss=1.0197, RMSE_E_per_atom=294.4 meV, RMSE_F=318.6 meV / A\n",
      "2025-06-03 19:01:28.140 INFO: Epoch 22: loss=0.7767, RMSE_E_per_atom=172.5 meV, RMSE_F=278.3 meV / A\n",
      "2025-06-03 19:01:31.237 INFO: Epoch 24: loss=0.9349, RMSE_E_per_atom=160.3 meV, RMSE_F=305.0 meV / A\n",
      "2025-06-03 19:01:34.362 INFO: Epoch 26: loss=0.7325, RMSE_E_per_atom=94.0 meV, RMSE_F=270.9 meV / A\n",
      "2025-06-03 19:01:37.426 INFO: Epoch 28: loss=0.5906, RMSE_E_per_atom=42.9 meV, RMSE_F=242.8 meV / A\n",
      "2025-06-03 19:01:40.532 INFO: Epoch 30: loss=0.7035, RMSE_E_per_atom=108.9 meV, RMSE_F=265.0 meV / A\n",
      "2025-06-03 19:01:43.637 INFO: Epoch 32: loss=0.7460, RMSE_E_per_atom=188.8 meV, RMSE_F=272.9 meV / A\n",
      "2025-06-03 19:01:46.993 INFO: Epoch 34: loss=0.5071, RMSE_E_per_atom=108.2 meV, RMSE_F=225.3 meV / A\n",
      "2025-06-03 19:01:48.520 INFO: Changing loss based on SWA\n",
      "2025-06-03 19:01:50.037 INFO: Epoch 36: loss=0.4375, RMSE_E_per_atom=22.4 meV, RMSE_F=196.9 meV / A\n",
      "2025-06-03 19:01:53.088 INFO: Epoch 38: loss=0.3577, RMSE_E_per_atom=13.9 meV, RMSE_F=184.1 meV / A\n",
      "2025-06-03 19:01:56.143 INFO: Epoch 40: loss=0.3814, RMSE_E_per_atom=17.4 meV, RMSE_F=187.5 meV / A\n",
      "2025-06-03 19:01:59.259 INFO: Epoch 42: loss=0.3284, RMSE_E_per_atom=11.0 meV, RMSE_F=178.1 meV / A\n",
      "2025-06-03 19:02:02.355 INFO: Epoch 44: loss=0.3236, RMSE_E_per_atom=8.6 meV, RMSE_F=178.1 meV / A\n",
      "2025-06-03 19:02:05.398 INFO: Epoch 46: loss=0.3280, RMSE_E_per_atom=13.6 meV, RMSE_F=176.3 meV / A\n",
      "2025-06-03 19:02:08.428 INFO: Epoch 48: loss=0.3069, RMSE_E_per_atom=7.6 meV, RMSE_F=173.8 meV / A\n",
      "2025-06-03 19:02:09.865 INFO: Training complete\n",
      "2025-06-03 19:02:09.866 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 19:02:10.422 INFO: Loading checkpoint: MACE_models/mace_benchmark_correlation3_run-123_epoch-34.pt\n",
      "2025-06-03 19:02:10.448 INFO: Loaded model from epoch 34\n",
      "2025-06-03 19:02:10.448 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:02:11.099 INFO: Evaluating valid ...\n",
      "2025-06-03 19:02:11.176 INFO: Evaluating Default ...\n",
      "2025-06-03 19:02:15.701 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        125.1        |      195.5       |        8.98       |\n",
      "|    valid    |        108.2        |      225.3       |        8.71       |\n",
      "|   Default   |        124.4        |      232.6       |       10.16       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:02:15.701 INFO: Saving model to MACE_models/mace_benchmark_correlation3_run-123.model\n",
      "2025-06-03 19:02:15.789 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_correlation3_compiled.model\n",
      "2025-06-03 19:02:16.544 INFO: Loading checkpoint: MACE_models/mace_benchmark_correlation3_run-123_epoch-48_swa.pt\n",
      "2025-06-03 19:02:16.570 INFO: Loaded model from epoch 48\n",
      "2025-06-03 19:02:16.571 INFO: Evaluating train ...\n",
      "2025-06-03 19:02:17.270 INFO: Evaluating valid ...\n",
      "2025-06-03 19:02:17.350 INFO: Evaluating Default ...\n",
      "2025-06-03 19:02:21.793 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         7.4         |      156.5       |        7.19       |\n",
      "|    valid    |         7.6         |      173.8       |        6.72       |\n",
      "|   Default   |         7.5         |      197.7       |        8.63       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:02:21.793 INFO: Saving model to MACE_models/mace_benchmark_correlation3_run-123_swa.model\n",
      "2025-06-03 19:02:21.881 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_correlation3_swa_compiled.model\n",
      "2025-06-03 19:02:22.604 INFO: Done\n",
      "âœ… Found: RMSE_E = 7.6 meV, RMSE_F = 173.8 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_correlation4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:02:26.439 INFO: MACE version: 0.3.6\n",
      "2025-06-03 19:02:26.439 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_correlation4.yaml', name='mace_benchmark_correlation4', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=4, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 19:02:26.554 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 19:02:26.591 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 19:02:26.662 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:02:26.662 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 19:02:26.666 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 19:02:26.666 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 19:02:26.922 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:02:26.942 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 19:02:26.942 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 19:02:26.943 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 19:02:26.943 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 19:02:27.145 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 19:02:27.298 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 19:02:27.298 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 19:02:27.337 INFO: Building model\n",
      "2025-06-03 19:02:27.337 INFO: Hidden irreps: 32x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:02:28.377 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 19:02:28.442 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-2): 3 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-2): 3 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x11x32 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x3x32 (cuda:0)]\n",
      "                (2): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 19:02:28.444 INFO: Number of parameters: 64592\n",
      "2025-06-03 19:02:28.444 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 19:02:28.444 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 19:02:28.444 INFO: Started training\n",
      "2025-06-03 19:02:29.128 INFO: Epoch None: loss=69.5671, RMSE_E_per_atom=6237.6 meV, RMSE_F=2569.9 meV / A\n",
      "2025-06-03 19:02:35.797 INFO: Epoch 0: loss=28.2930, RMSE_E_per_atom=5280.0 meV, RMSE_F=1606.5 meV / A\n",
      "2025-06-03 19:02:39.766 INFO: Epoch 2: loss=6.7681, RMSE_E_per_atom=4464.4 meV, RMSE_F=693.6 meV / A\n",
      "2025-06-03 19:02:43.147 INFO: Epoch 4: loss=3.8649, RMSE_E_per_atom=3598.8 meV, RMSE_F=508.0 meV / A\n",
      "2025-06-03 19:02:46.501 INFO: Epoch 6: loss=2.2338, RMSE_E_per_atom=2066.1 meV, RMSE_F=425.7 meV / A\n",
      "2025-06-03 19:02:49.863 INFO: Epoch 8: loss=1.6382, RMSE_E_per_atom=1656.7 meV, RMSE_F=370.2 meV / A\n",
      "2025-06-03 19:02:53.318 INFO: Epoch 10: loss=1.6544, RMSE_E_per_atom=1014.1 meV, RMSE_F=394.1 meV / A\n",
      "2025-06-03 19:02:56.753 INFO: Epoch 12: loss=1.1690, RMSE_E_per_atom=755.6 meV, RMSE_F=333.3 meV / A\n",
      "2025-06-03 19:03:00.234 INFO: Epoch 14: loss=1.1923, RMSE_E_per_atom=563.7 meV, RMSE_F=340.9 meV / A\n",
      "2025-06-03 19:03:03.770 INFO: Epoch 16: loss=1.1076, RMSE_E_per_atom=476.6 meV, RMSE_F=329.3 meV / A\n",
      "2025-06-03 19:03:07.264 INFO: Epoch 18: loss=1.2738, RMSE_E_per_atom=355.5 meV, RMSE_F=355.7 meV / A\n",
      "2025-06-03 19:03:10.680 INFO: Epoch 20: loss=0.9195, RMSE_E_per_atom=382.5 meV, RMSE_F=302.1 meV / A\n",
      "2025-06-03 19:03:14.181 INFO: Epoch 22: loss=0.7886, RMSE_E_per_atom=192.4 meV, RMSE_F=280.7 meV / A\n",
      "2025-06-03 19:03:17.681 INFO: Epoch 24: loss=0.9188, RMSE_E_per_atom=262.1 meV, RMSE_F=302.5 meV / A\n",
      "2025-06-03 19:03:21.125 INFO: Epoch 26: loss=0.7868, RMSE_E_per_atom=178.9 meV, RMSE_F=280.2 meV / A\n",
      "2025-06-03 19:03:24.595 INFO: Epoch 28: loss=0.7254, RMSE_E_per_atom=160.0 meV, RMSE_F=269.5 meV / A\n",
      "2025-06-03 19:03:28.105 INFO: Epoch 30: loss=0.6591, RMSE_E_per_atom=172.6 meV, RMSE_F=256.3 meV / A\n",
      "2025-06-03 19:03:31.597 INFO: Epoch 32: loss=0.5819, RMSE_E_per_atom=130.4 meV, RMSE_F=241.1 meV / A\n",
      "2025-06-03 19:03:35.322 INFO: Epoch 34: loss=0.5604, RMSE_E_per_atom=72.9 meV, RMSE_F=237.5 meV / A\n",
      "2025-06-03 19:03:37.063 INFO: Changing loss based on SWA\n",
      "2025-06-03 19:03:38.774 INFO: Epoch 36: loss=0.9193, RMSE_E_per_atom=33.4 meV, RMSE_F=285.2 meV / A\n",
      "2025-06-03 19:03:42.247 INFO: Epoch 38: loss=0.4157, RMSE_E_per_atom=4.9 meV, RMSE_F=204.3 meV / A\n",
      "2025-06-03 19:03:45.678 INFO: Epoch 40: loss=0.3751, RMSE_E_per_atom=9.5 meV, RMSE_F=192.2 meV / A\n",
      "2025-06-03 19:03:49.258 INFO: Epoch 42: loss=0.3754, RMSE_E_per_atom=15.2 meV, RMSE_F=188.4 meV / A\n",
      "2025-06-03 19:03:52.661 INFO: Epoch 44: loss=0.3090, RMSE_E_per_atom=7.3 meV, RMSE_F=174.8 meV / A\n",
      "2025-06-03 19:03:56.141 INFO: Epoch 46: loss=0.3309, RMSE_E_per_atom=7.1 meV, RMSE_F=181.2 meV / A\n",
      "2025-06-03 19:03:59.572 INFO: Epoch 48: loss=0.3188, RMSE_E_per_atom=12.3 meV, RMSE_F=174.8 meV / A\n",
      "2025-06-03 19:04:01.212 INFO: Training complete\n",
      "2025-06-03 19:04:01.213 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 19:04:01.776 INFO: Loading checkpoint: MACE_models/mace_benchmark_correlation4_run-123_epoch-34.pt\n",
      "2025-06-03 19:04:01.806 INFO: Loaded model from epoch 34\n",
      "2025-06-03 19:04:01.806 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:04:02.543 INFO: Evaluating valid ...\n",
      "2025-06-03 19:04:02.625 INFO: Evaluating Default ...\n",
      "2025-06-03 19:04:07.248 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         52.3        |      196.4       |        9.02       |\n",
      "|    valid    |         72.9        |      237.5       |        9.18       |\n",
      "|   Default   |         58.3        |      235.2       |       10.27       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:04:07.248 INFO: Saving model to MACE_models/mace_benchmark_correlation4_run-123.model\n",
      "2025-06-03 19:04:07.438 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_correlation4_compiled.model\n",
      "2025-06-03 19:04:08.325 INFO: Loading checkpoint: MACE_models/mace_benchmark_correlation4_run-123_epoch-44_swa.pt\n",
      "2025-06-03 19:04:08.357 INFO: Loaded model from epoch 44\n",
      "2025-06-03 19:04:08.357 INFO: Evaluating train ...\n",
      "2025-06-03 19:04:09.128 INFO: Evaluating valid ...\n",
      "2025-06-03 19:04:09.216 INFO: Evaluating Default ...\n",
      "2025-06-03 19:04:13.844 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         9.3         |      147.2       |        6.76       |\n",
      "|    valid    |         7.3         |      174.8       |        6.76       |\n",
      "|   Default   |         9.8         |      193.1       |        8.43       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:04:13.844 INFO: Saving model to MACE_models/mace_benchmark_correlation4_run-123_swa.model\n",
      "2025-06-03 19:04:14.047 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_correlation4_swa_compiled.model\n",
      "2025-06-03 19:04:14.923 INFO: Done\n",
      "âœ… Found: RMSE_E = 7.3 meV, RMSE_F = 174.8 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_chan64_corr3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:04:18.415 INFO: MACE version: 0.3.6\n",
      "2025-06-03 19:04:18.416 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_chan64_corr3.yaml', name='mace_benchmark_chan64_corr3', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 19:04:18.506 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 19:04:18.543 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 19:04:18.616 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:04:18.616 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 19:04:18.620 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 19:04:18.620 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 19:04:18.887 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:04:18.909 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 19:04:18.909 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 19:04:18.911 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 19:04:18.911 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 19:04:19.138 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 19:04:19.294 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 19:04:19.294 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 19:04:19.333 INFO: Building model\n",
      "2025-06-03 19:04:19.333 INFO: Hidden irreps: 64x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:04:20.075 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 19:04:20.139 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 19:04:20.141 INFO: Number of parameters: 139152\n",
      "2025-06-03 19:04:20.141 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 19:04:20.141 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 19:04:20.141 INFO: Started training\n",
      "2025-06-03 19:04:20.851 INFO: Epoch None: loss=69.6884, RMSE_E_per_atom=6219.2 meV, RMSE_F=2572.6 meV / A\n",
      "2025-06-03 19:04:27.247 INFO: Epoch 0: loss=23.1447, RMSE_E_per_atom=4823.2 meV, RMSE_F=1446.6 meV / A\n",
      "2025-06-03 19:04:30.835 INFO: Epoch 2: loss=6.9383, RMSE_E_per_atom=3578.6 meV, RMSE_F=753.0 meV / A\n",
      "2025-06-03 19:04:33.770 INFO: Epoch 4: loss=3.3438, RMSE_E_per_atom=2693.0 meV, RMSE_F=512.5 meV / A\n",
      "2025-06-03 19:04:36.741 INFO: Epoch 6: loss=3.2190, RMSE_E_per_atom=1655.1 meV, RMSE_F=544.5 meV / A\n",
      "2025-06-03 19:04:39.729 INFO: Epoch 8: loss=2.0426, RMSE_E_per_atom=1269.0 meV, RMSE_F=434.3 meV / A\n",
      "2025-06-03 19:04:42.695 INFO: Epoch 10: loss=1.2700, RMSE_E_per_atom=848.8 meV, RMSE_F=346.5 meV / A\n",
      "2025-06-03 19:04:45.845 INFO: Epoch 12: loss=1.7311, RMSE_E_per_atom=686.8 meV, RMSE_F=410.8 meV / A\n",
      "2025-06-03 19:04:48.857 INFO: Epoch 14: loss=1.2560, RMSE_E_per_atom=394.5 meV, RMSE_F=352.5 meV / A\n",
      "2025-06-03 19:04:52.059 INFO: Epoch 16: loss=0.8882, RMSE_E_per_atom=311.1 meV, RMSE_F=296.7 meV / A\n",
      "2025-06-03 19:04:55.267 INFO: Epoch 18: loss=0.8972, RMSE_E_per_atom=294.6 meV, RMSE_F=300.0 meV / A\n",
      "2025-06-03 19:04:58.336 INFO: Epoch 20: loss=0.9639, RMSE_E_per_atom=269.0 meV, RMSE_F=310.1 meV / A\n",
      "2025-06-03 19:05:01.389 INFO: Epoch 22: loss=0.6705, RMSE_E_per_atom=226.7 meV, RMSE_F=258.5 meV / A\n",
      "2025-06-03 19:05:04.573 INFO: Epoch 24: loss=0.5844, RMSE_E_per_atom=201.7 meV, RMSE_F=241.5 meV / A\n",
      "2025-06-03 19:05:07.795 INFO: Epoch 26: loss=0.6911, RMSE_E_per_atom=194.3 meV, RMSE_F=262.8 meV / A\n",
      "2025-06-03 19:05:10.831 INFO: Epoch 28: loss=0.5844, RMSE_E_per_atom=164.4 meV, RMSE_F=241.5 meV / A\n",
      "2025-06-03 19:05:13.899 INFO: Epoch 30: loss=0.4639, RMSE_E_per_atom=142.3 meV, RMSE_F=215.2 meV / A\n",
      "2025-06-03 19:05:17.095 INFO: Epoch 32: loss=0.5105, RMSE_E_per_atom=178.9 meV, RMSE_F=225.0 meV / A\n",
      "2025-06-03 19:05:20.231 INFO: Epoch 34: loss=0.6239, RMSE_E_per_atom=191.7 meV, RMSE_F=250.3 meV / A\n",
      "2025-06-03 19:05:21.699 INFO: Changing loss based on SWA\n",
      "2025-06-03 19:05:23.227 INFO: Epoch 36: loss=0.9928, RMSE_E_per_atom=67.6 meV, RMSE_F=232.4 meV / A\n",
      "2025-06-03 19:05:26.331 INFO: Epoch 38: loss=0.4423, RMSE_E_per_atom=21.2 meV, RMSE_F=200.1 meV / A\n",
      "2025-06-03 19:05:29.482 INFO: Epoch 40: loss=0.4289, RMSE_E_per_atom=26.7 meV, RMSE_F=189.8 meV / A\n",
      "2025-06-03 19:05:32.678 INFO: Epoch 42: loss=0.5024, RMSE_E_per_atom=36.9 meV, RMSE_F=192.2 meV / A\n",
      "2025-06-03 19:05:35.779 INFO: Epoch 44: loss=0.3295, RMSE_E_per_atom=13.7 meV, RMSE_F=176.7 meV / A\n",
      "2025-06-03 19:05:38.919 INFO: Epoch 46: loss=0.4093, RMSE_E_per_atom=14.5 meV, RMSE_F=197.9 meV / A\n",
      "2025-06-03 19:05:42.010 INFO: Epoch 48: loss=0.3689, RMSE_E_per_atom=16.4 meV, RMSE_F=185.6 meV / A\n",
      "2025-06-03 19:05:43.476 INFO: Training complete\n",
      "2025-06-03 19:05:43.477 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 19:05:44.046 INFO: Loading checkpoint: MACE_models/mace_benchmark_chan64_corr3_run-123_epoch-30.pt\n",
      "2025-06-03 19:05:44.073 INFO: Loaded model from epoch 30\n",
      "2025-06-03 19:05:44.074 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:05:44.701 INFO: Evaluating valid ...\n",
      "2025-06-03 19:05:44.776 INFO: Evaluating Default ...\n",
      "2025-06-03 19:05:49.189 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         97.0        |      192.4       |        8.84       |\n",
      "|    valid    |        142.3        |      215.2       |        8.32       |\n",
      "|   Default   |        107.3        |      227.3       |        9.93       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:05:49.189 INFO: Saving model to MACE_models/mace_benchmark_chan64_corr3_run-123.model\n",
      "2025-06-03 19:05:49.290 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_chan64_corr3_compiled.model\n",
      "2025-06-03 19:05:50.058 INFO: Loading checkpoint: MACE_models/mace_benchmark_chan64_corr3_run-123_epoch-44_swa.pt\n",
      "2025-06-03 19:05:50.087 INFO: Loaded model from epoch 44\n",
      "2025-06-03 19:05:50.087 INFO: Evaluating train ...\n",
      "2025-06-03 19:05:50.799 INFO: Evaluating valid ...\n",
      "2025-06-03 19:05:50.878 INFO: Evaluating Default ...\n",
      "2025-06-03 19:05:55.239 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         15.7        |      147.8       |        6.79       |\n",
      "|    valid    |         13.7        |      176.7       |        6.83       |\n",
      "|   Default   |         15.7        |      184.8       |        8.07       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:05:55.240 INFO: Saving model to MACE_models/mace_benchmark_chan64_corr3_run-123_swa.model\n",
      "2025-06-03 19:05:55.342 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_chan64_corr3_swa_compiled.model\n",
      "2025-06-03 19:05:56.064 INFO: Done\n",
      "âœ… Found: RMSE_E = 13.7 meV, RMSE_F = 176.7 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_chan64_inter3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:05:59.633 INFO: MACE version: 0.3.6\n",
      "2025-06-03 19:05:59.634 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_chan64_inter3.yaml', name='mace_benchmark_chan64_inter3', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=2, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 19:05:59.723 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 19:05:59.760 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 19:05:59.830 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:05:59.831 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 19:05:59.835 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 19:05:59.835 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 19:06:00.089 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:06:00.110 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 19:06:00.110 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 19:06:00.111 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 19:06:00.111 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 19:06:00.339 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 19:06:00.494 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 19:06:00.494 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 19:06:00.532 INFO: Building model\n",
      "2025-06-03 19:06:00.533 INFO: Hidden irreps: 64x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:06:01.494 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 19:06:01.580 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1-2): 2 x RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-2): 3 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0): GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(  (0): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)])\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 19:06:01.582 INFO: Number of parameters: 189520\n",
      "2025-06-03 19:06:01.582 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 19:06:01.582 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 19:06:01.582 INFO: Started training\n",
      "2025-06-03 19:06:02.373 INFO: Epoch None: loss=70.2915, RMSE_E_per_atom=6263.6 meV, RMSE_F=2582.9 meV / A\n",
      "2025-06-03 19:06:08.902 INFO: Epoch 0: loss=25.4138, RMSE_E_per_atom=4822.4 meV, RMSE_F=1523.5 meV / A\n",
      "2025-06-03 19:06:12.764 INFO: Epoch 2: loss=7.2389, RMSE_E_per_atom=3983.5 meV, RMSE_F=753.0 meV / A\n",
      "2025-06-03 19:06:16.027 INFO: Epoch 4: loss=5.1086, RMSE_E_per_atom=3247.5 meV, RMSE_F=638.3 meV / A\n",
      "2025-06-03 19:06:19.309 INFO: Epoch 6: loss=2.5512, RMSE_E_per_atom=2023.4 meV, RMSE_F=463.4 meV / A\n",
      "2025-06-03 19:06:22.575 INFO: Epoch 8: loss=1.9945, RMSE_E_per_atom=1690.5 meV, RMSE_F=414.1 meV / A\n",
      "2025-06-03 19:06:25.863 INFO: Epoch 10: loss=2.3490, RMSE_E_per_atom=1378.2 meV, RMSE_F=465.0 meV / A\n",
      "2025-06-03 19:06:29.050 INFO: Epoch 12: loss=1.3152, RMSE_E_per_atom=904.0 meV, RMSE_F=351.7 meV / A\n",
      "2025-06-03 19:06:32.404 INFO: Epoch 14: loss=1.3888, RMSE_E_per_atom=530.0 meV, RMSE_F=370.1 meV / A\n",
      "2025-06-03 19:06:35.701 INFO: Epoch 16: loss=1.5589, RMSE_E_per_atom=373.0 meV, RMSE_F=394.3 meV / A\n",
      "2025-06-03 19:06:38.985 INFO: Epoch 18: loss=1.0120, RMSE_E_per_atom=345.9 meV, RMSE_F=316.9 meV / A\n",
      "2025-06-03 19:06:42.413 INFO: Epoch 20: loss=1.4463, RMSE_E_per_atom=418.4 meV, RMSE_F=378.9 meV / A\n",
      "2025-06-03 19:06:45.704 INFO: Epoch 22: loss=0.7502, RMSE_E_per_atom=402.4 meV, RMSE_F=271.4 meV / A\n",
      "2025-06-03 19:06:49.130 INFO: Epoch 24: loss=1.1994, RMSE_E_per_atom=141.6 meV, RMSE_F=346.6 meV / A\n",
      "2025-06-03 19:06:52.504 INFO: Epoch 26: loss=0.8237, RMSE_E_per_atom=221.3 meV, RMSE_F=286.6 meV / A\n",
      "2025-06-03 19:06:55.755 INFO: Epoch 28: loss=0.5580, RMSE_E_per_atom=205.2 meV, RMSE_F=236.0 meV / A\n",
      "2025-06-03 19:06:59.203 INFO: Epoch 30: loss=0.9531, RMSE_E_per_atom=215.4 meV, RMSE_F=308.3 meV / A\n",
      "2025-06-03 19:07:02.566 INFO: Epoch 32: loss=0.8451, RMSE_E_per_atom=260.5 meV, RMSE_F=289.7 meV / A\n",
      "2025-06-03 19:07:06.112 INFO: Epoch 34: loss=0.5069, RMSE_E_per_atom=216.2 meV, RMSE_F=224.4 meV / A\n",
      "2025-06-03 19:07:07.772 INFO: Changing loss based on SWA\n",
      "2025-06-03 19:07:09.449 INFO: Epoch 36: loss=0.8618, RMSE_E_per_atom=52.8 meV, RMSE_F=242.1 meV / A\n",
      "2025-06-03 19:07:12.846 INFO: Epoch 38: loss=0.4058, RMSE_E_per_atom=9.5 meV, RMSE_F=199.7 meV / A\n",
      "2025-06-03 19:07:16.262 INFO: Epoch 40: loss=0.3786, RMSE_E_per_atom=6.6 meV, RMSE_F=193.8 meV / A\n",
      "2025-06-03 19:07:19.754 INFO: Epoch 42: loss=0.3537, RMSE_E_per_atom=6.7 meV, RMSE_F=187.2 meV / A\n",
      "2025-06-03 19:07:23.215 INFO: Epoch 44: loss=0.3937, RMSE_E_per_atom=21.9 meV, RMSE_F=186.2 meV / A\n",
      "2025-06-03 19:07:26.600 INFO: Epoch 46: loss=0.3550, RMSE_E_per_atom=10.0 meV, RMSE_F=186.1 meV / A\n",
      "2025-06-03 19:07:29.957 INFO: Epoch 48: loss=0.3529, RMSE_E_per_atom=10.0 meV, RMSE_F=185.5 meV / A\n",
      "2025-06-03 19:07:31.653 INFO: Training complete\n",
      "2025-06-03 19:07:31.653 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 19:07:32.213 INFO: Loading checkpoint: MACE_models/mace_benchmark_chan64_inter3_run-123_epoch-34.pt\n",
      "2025-06-03 19:07:32.248 INFO: Loaded model from epoch 34\n",
      "2025-06-03 19:07:32.249 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:07:32.975 INFO: Evaluating valid ...\n",
      "2025-06-03 19:07:33.062 INFO: Evaluating Default ...\n",
      "2025-06-03 19:07:38.124 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        207.7        |      196.9       |        9.04       |\n",
      "|    valid    |        216.2        |      224.4       |        8.68       |\n",
      "|   Default   |        201.4        |      226.5       |        9.89       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:07:38.125 INFO: Saving model to MACE_models/mace_benchmark_chan64_inter3_run-123.model\n",
      "2025-06-03 19:07:38.256 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_chan64_inter3_compiled.model\n",
      "2025-06-03 19:07:39.009 INFO: Loading checkpoint: MACE_models/mace_benchmark_chan64_inter3_run-123_epoch-48_swa.pt\n",
      "2025-06-03 19:07:39.037 INFO: Loaded model from epoch 48\n",
      "2025-06-03 19:07:39.038 INFO: Evaluating train ...\n",
      "2025-06-03 19:07:39.801 INFO: Evaluating valid ...\n",
      "2025-06-03 19:07:39.896 INFO: Evaluating Default ...\n",
      "2025-06-03 19:07:44.929 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         12.0        |      159.1       |        7.30       |\n",
      "|    valid    |         10.0        |      185.5       |        7.17       |\n",
      "|   Default   |         10.6        |      193.1       |        8.43       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:07:44.930 INFO: Saving model to MACE_models/mace_benchmark_chan64_inter3_run-123_swa.model\n",
      "2025-06-03 19:07:45.058 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_chan64_inter3_swa_compiled.model\n",
      "2025-06-03 19:07:45.929 INFO: Done\n",
      "âœ… Found: RMSE_E = 10.0 meV, RMSE_F = 185.5 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_inter3_corr3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:07:49.691 INFO: MACE version: 0.3.6\n",
      "2025-06-03 19:07:49.691 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_inter3_corr3.yaml', name='mace_benchmark_inter3_corr3', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=0, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 19:07:49.779 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 19:07:49.815 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 19:07:49.885 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:07:49.885 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 19:07:49.889 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 19:07:49.889 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 19:07:50.143 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:07:50.164 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 19:07:50.164 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 19:07:50.165 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 19:07:50.165 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 19:07:50.369 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 19:07:50.523 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 19:07:50.523 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 19:07:50.561 INFO: Building model\n",
      "2025-06-03 19:07:50.561 INFO: Hidden irreps: 32x0e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:07:51.608 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 19:07:51.695 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1-2): 2 x RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-2): 3 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x32 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 19:07:51.697 INFO: Number of parameters: 80272\n",
      "2025-06-03 19:07:51.697 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 19:07:51.697 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 19:07:51.697 INFO: Started training\n",
      "2025-06-03 19:07:52.448 INFO: Epoch None: loss=70.7632, RMSE_E_per_atom=6242.6 meV, RMSE_F=2592.8 meV / A\n",
      "2025-06-03 19:07:59.058 INFO: Epoch 0: loss=31.9432, RMSE_E_per_atom=5211.8 meV, RMSE_F=1718.3 meV / A\n",
      "2025-06-03 19:08:03.066 INFO: Epoch 2: loss=8.7191, RMSE_E_per_atom=4561.9 meV, RMSE_F=817.4 meV / A\n",
      "2025-06-03 19:08:06.439 INFO: Epoch 4: loss=4.3788, RMSE_E_per_atom=4004.1 meV, RMSE_F=528.2 meV / A\n",
      "2025-06-03 19:08:09.803 INFO: Epoch 6: loss=2.3657, RMSE_E_per_atom=2759.7 meV, RMSE_F=401.3 meV / A\n",
      "2025-06-03 19:08:13.194 INFO: Epoch 8: loss=2.8757, RMSE_E_per_atom=1745.3 meV, RMSE_F=507.4 meV / A\n",
      "2025-06-03 19:08:16.581 INFO: Epoch 10: loss=1.8874, RMSE_E_per_atom=1368.1 meV, RMSE_F=412.9 meV / A\n",
      "2025-06-03 19:08:20.149 INFO: Epoch 12: loss=1.6383, RMSE_E_per_atom=776.7 meV, RMSE_F=397.4 meV / A\n",
      "2025-06-03 19:08:23.668 INFO: Epoch 14: loss=2.0482, RMSE_E_per_atom=606.8 meV, RMSE_F=450.7 meV / A\n",
      "2025-06-03 19:08:27.200 INFO: Epoch 16: loss=1.1868, RMSE_E_per_atom=312.2 meV, RMSE_F=342.7 meV / A\n",
      "2025-06-03 19:08:30.739 INFO: Epoch 18: loss=1.2556, RMSE_E_per_atom=289.7 meV, RMSE_F=353.9 meV / A\n",
      "2025-06-03 19:08:34.240 INFO: Epoch 20: loss=1.3033, RMSE_E_per_atom=392.3 meV, RMSE_F=360.0 meV / A\n",
      "2025-06-03 19:08:37.709 INFO: Epoch 22: loss=0.9595, RMSE_E_per_atom=437.7 meV, RMSE_F=307.9 meV / A\n",
      "2025-06-03 19:08:41.227 INFO: Epoch 24: loss=0.8827, RMSE_E_per_atom=308.4 meV, RMSE_F=296.3 meV / A\n",
      "2025-06-03 19:08:44.854 INFO: Epoch 26: loss=0.8380, RMSE_E_per_atom=96.4 meV, RMSE_F=289.8 meV / A\n",
      "2025-06-03 19:08:48.399 INFO: Epoch 28: loss=0.7808, RMSE_E_per_atom=48.1 meV, RMSE_F=279.6 meV / A\n",
      "2025-06-03 19:08:51.905 INFO: Epoch 30: loss=0.7992, RMSE_E_per_atom=250.1 meV, RMSE_F=282.5 meV / A\n",
      "2025-06-03 19:08:55.420 INFO: Epoch 32: loss=0.7436, RMSE_E_per_atom=168.4 meV, RMSE_F=272.2 meV / A\n",
      "2025-06-03 19:08:59.008 INFO: Epoch 34: loss=0.6052, RMSE_E_per_atom=102.5 meV, RMSE_F=246.0 meV / A\n",
      "2025-06-03 19:09:00.740 INFO: Changing loss based on SWA\n",
      "2025-06-03 19:09:02.490 INFO: Epoch 36: loss=1.4788, RMSE_E_per_atom=83.2 meV, RMSE_F=282.1 meV / A\n",
      "2025-06-03 19:09:06.027 INFO: Epoch 38: loss=0.4537, RMSE_E_per_atom=18.3 meV, RMSE_F=205.4 meV / A\n",
      "2025-06-03 19:09:09.553 INFO: Epoch 40: loss=0.4032, RMSE_E_per_atom=12.2 meV, RMSE_F=197.4 meV / A\n",
      "2025-06-03 19:09:13.155 INFO: Epoch 42: loss=0.4500, RMSE_E_per_atom=17.5 meV, RMSE_F=205.3 meV / A\n",
      "2025-06-03 19:09:16.677 INFO: Epoch 44: loss=0.4206, RMSE_E_per_atom=11.7 meV, RMSE_F=202.2 meV / A\n",
      "2025-06-03 19:09:20.155 INFO: Epoch 46: loss=0.4091, RMSE_E_per_atom=9.2 meV, RMSE_F=200.6 meV / A\n",
      "2025-06-03 19:09:23.645 INFO: Epoch 48: loss=0.5279, RMSE_E_per_atom=34.3 meV, RMSE_F=203.2 meV / A\n",
      "2025-06-03 19:09:25.292 INFO: Training complete\n",
      "2025-06-03 19:09:25.292 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 19:09:25.850 INFO: Loading checkpoint: MACE_models/mace_benchmark_inter3_corr3_run-123_epoch-34.pt\n",
      "2025-06-03 19:09:25.885 INFO: Loaded model from epoch 34\n",
      "2025-06-03 19:09:25.885 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:09:26.633 INFO: Evaluating valid ...\n",
      "2025-06-03 19:09:26.720 INFO: Evaluating Default ...\n",
      "2025-06-03 19:09:31.946 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        123.2        |      227.5       |       10.45       |\n",
      "|    valid    |        102.5        |      246.0       |        9.51       |\n",
      "|   Default   |        122.1        |      253.3       |       11.06       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:09:31.946 INFO: Saving model to MACE_models/mace_benchmark_inter3_corr3_run-123.model\n",
      "2025-06-03 19:09:32.067 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_inter3_corr3_compiled.model\n",
      "2025-06-03 19:09:33.043 INFO: Loading checkpoint: MACE_models/mace_benchmark_inter3_corr3_run-123_epoch-40_swa.pt\n",
      "2025-06-03 19:09:33.071 INFO: Loaded model from epoch 40\n",
      "2025-06-03 19:09:33.072 INFO: Evaluating train ...\n",
      "2025-06-03 19:09:33.871 INFO: Evaluating valid ...\n",
      "2025-06-03 19:09:33.967 INFO: Evaluating Default ...\n",
      "2025-06-03 19:09:39.187 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         12.5        |      176.8       |        8.12       |\n",
      "|    valid    |         12.2        |      197.4       |        7.63       |\n",
      "|   Default   |         13.7        |      207.6       |        9.07       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:09:39.187 INFO: Saving model to MACE_models/mace_benchmark_inter3_corr3_run-123_swa.model\n",
      "2025-06-03 19:09:39.310 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_inter3_corr3_swa_compiled.model\n",
      "2025-06-03 19:09:40.284 INFO: Done\n",
      "âœ… Found: RMSE_E = 12.2 meV, RMSE_F = 197.4 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_benchmark_maxL1_corr3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:09:43.956 INFO: MACE version: 0.3.6\n",
      "2025-06-03 19:09:43.956 INFO: Configuration: Namespace(config='benchmark_results/mace_benchmark_maxL1_corr3.yaml', name='mace_benchmark_maxL1_corr3', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=2, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=32, max_L=1, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 19:09:44.064 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 19:09:44.101 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 19:09:44.172 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:09:44.172 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 19:09:44.177 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 19:09:44.177 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 19:09:44.434 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 19:09:44.455 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 19:09:44.455 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 19:09:44.456 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 19:09:44.456 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 19:09:44.662 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 19:09:44.818 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 19:09:44.818 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 19:09:44.856 INFO: Building model\n",
      "2025-06-03 19:09:44.857 INFO: Hidden irreps: 32x0e+32x1o\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:09:45.692 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 19:09:45.764 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 32x0e | 96 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "      (conv_tp): TensorProduct(32x0e x 1x0e+1x1o+1x2e -> 32x0e+32x1o+32x2e | 96 paths | 96 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 96]\n",
      "      (linear): Linear(32x0e+32x1o+32x2e -> 32x0e+32x1o+32x2e | 3072 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o+32x2e x 3x0e -> 32x0e+32x1o+32x2e | 9216 paths | 9216 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(32x0e+32x1o -> 32x0e+32x1o | 2048 weights)\n",
      "      (conv_tp): TensorProduct(32x0e+32x1o x 1x0e+1x1o+1x2e -> 64x0e+96x1o+64x2e | 224 paths | 224 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 224]\n",
      "      (linear): Linear(64x0e+96x1o+64x2e -> 32x0e+32x1o+32x2e | 7168 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(32x0e+32x1o x 3x0e -> 32x0e | 3072 paths | 3072 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x32 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x32 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e+32x1o -> 32x0e+32x1o | 2048 weights)\n",
      "    )\n",
      "    (1): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x32 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x32 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(32x0e -> 32x0e | 1024 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0): LinearReadoutBlock(\n",
      "      (linear): Linear(32x0e+32x1o -> 1x0e | 32 weights)\n",
      "    )\n",
      "    (1): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(32x0e -> 16x0e | 512 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 19:09:45.765 INFO: Number of parameters: 72592\n",
      "2025-06-03 19:09:45.766 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 19:09:45.766 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 19:09:45.766 INFO: Started training\n",
      "2025-06-03 19:09:46.501 INFO: Epoch None: loss=70.7215, RMSE_E_per_atom=6269.4 meV, RMSE_F=2591.4 meV / A\n",
      "2025-06-03 19:09:53.084 INFO: Epoch 0: loss=30.8022, RMSE_E_per_atom=5354.9 meV, RMSE_F=1680.5 meV / A\n",
      "2025-06-03 19:09:57.002 INFO: Epoch 2: loss=8.2413, RMSE_E_per_atom=4606.4 meV, RMSE_F=783.2 meV / A\n",
      "2025-06-03 19:10:00.335 INFO: Epoch 4: loss=4.2758, RMSE_E_per_atom=4145.6 meV, RMSE_F=507.1 meV / A\n",
      "2025-06-03 19:10:04.143 INFO: Epoch 6: loss=2.8518, RMSE_E_per_atom=2950.2 meV, RMSE_F=446.3 meV / A\n",
      "2025-06-03 19:10:08.095 INFO: Epoch 8: loss=2.0104, RMSE_E_per_atom=1752.4 meV, RMSE_F=413.5 meV / A\n",
      "2025-06-03 19:10:11.980 INFO: Epoch 10: loss=2.0542, RMSE_E_per_atom=1195.3 meV, RMSE_F=437.6 meV / A\n",
      "2025-06-03 19:10:15.742 INFO: Epoch 12: loss=1.1922, RMSE_E_per_atom=921.8 meV, RMSE_F=333.0 meV / A\n",
      "2025-06-03 19:10:19.577 INFO: Epoch 14: loss=0.9322, RMSE_E_per_atom=639.8 meV, RMSE_F=299.5 meV / A\n",
      "2025-06-03 19:10:23.555 INFO: Epoch 16: loss=1.3263, RMSE_E_per_atom=631.9 meV, RMSE_F=359.3 meV / A\n",
      "2025-06-03 19:10:27.400 INFO: Epoch 18: loss=0.6715, RMSE_E_per_atom=442.1 meV, RMSE_F=256.0 meV / A\n",
      "2025-06-03 19:10:31.226 INFO: Epoch 20: loss=0.7611, RMSE_E_per_atom=423.1 meV, RMSE_F=273.2 meV / A\n",
      "2025-06-03 19:10:34.967 INFO: Epoch 22: loss=0.8073, RMSE_E_per_atom=256.4 meV, RMSE_F=283.1 meV / A\n",
      "2025-06-03 19:10:38.825 INFO: Epoch 24: loss=1.0469, RMSE_E_per_atom=387.1 meV, RMSE_F=321.2 meV / A\n",
      "2025-06-03 19:10:42.584 INFO: Epoch 26: loss=0.8785, RMSE_E_per_atom=404.1 meV, RMSE_F=293.5 meV / A\n",
      "2025-06-03 19:10:46.328 INFO: Epoch 28: loss=0.5657, RMSE_E_per_atom=221.9 meV, RMSE_F=236.5 meV / A\n",
      "2025-06-03 19:10:50.148 INFO: Epoch 30: loss=0.4514, RMSE_E_per_atom=296.4 meV, RMSE_F=211.0 meV / A\n",
      "2025-06-03 19:10:54.079 INFO: Epoch 32: loss=0.7848, RMSE_E_per_atom=359.3 meV, RMSE_F=277.9 meV / A\n",
      "2025-06-03 19:10:58.133 INFO: Epoch 34: loss=0.5211, RMSE_E_per_atom=235.2 meV, RMSE_F=226.9 meV / A\n",
      "2025-06-03 19:10:59.953 INFO: Changing loss based on SWA\n",
      "2025-06-03 19:11:01.921 INFO: Epoch 36: loss=2.8045, RMSE_E_per_atom=150.6 meV, RMSE_F=232.0 meV / A\n",
      "2025-06-03 19:11:05.764 INFO: Epoch 38: loss=0.5548, RMSE_E_per_atom=29.6 meV, RMSE_F=216.4 meV / A\n",
      "2025-06-03 19:11:09.583 INFO: Epoch 40: loss=0.4235, RMSE_E_per_atom=22.1 meV, RMSE_F=193.9 meV / A\n",
      "2025-06-03 19:11:13.493 INFO: Epoch 42: loss=0.3840, RMSE_E_per_atom=16.5 meV, RMSE_F=189.4 meV / A\n",
      "2025-06-03 19:11:17.268 INFO: Epoch 44: loss=0.3272, RMSE_E_per_atom=7.2 meV, RMSE_F=179.8 meV / A\n",
      "2025-06-03 19:11:21.181 INFO: Epoch 46: loss=0.3097, RMSE_E_per_atom=6.5 meV, RMSE_F=175.1 meV / A\n",
      "2025-06-03 19:11:24.789 INFO: Epoch 48: loss=0.3452, RMSE_E_per_atom=15.1 meV, RMSE_F=179.9 meV / A\n",
      "2025-06-03 19:11:26.679 INFO: Training complete\n",
      "2025-06-03 19:11:26.680 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 19:11:27.239 INFO: Loading checkpoint: MACE_models/mace_benchmark_maxL1_corr3_run-123_epoch-30.pt\n",
      "2025-06-03 19:11:27.268 INFO: Loaded model from epoch 30\n",
      "2025-06-03 19:11:27.269 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 19:11:28.104 INFO: Evaluating valid ...\n",
      "2025-06-03 19:11:28.183 INFO: Evaluating Default ...\n",
      "2025-06-03 19:11:33.675 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        224.8        |      194.1       |        8.91       |\n",
      "|    valid    |        296.4        |      211.0       |        8.16       |\n",
      "|   Default   |        241.7        |      227.1       |        9.92       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:11:33.675 INFO: Saving model to MACE_models/mace_benchmark_maxL1_corr3_run-123.model\n",
      "2025-06-03 19:11:33.784 INFO: Compiling model, saving metadata to MACE_models/mace_benchmark_maxL1_corr3_compiled.model\n",
      "2025-06-03 19:11:34.641 INFO: Loading checkpoint: MACE_models/mace_benchmark_maxL1_corr3_run-123_epoch-46_swa.pt\n",
      "2025-06-03 19:11:34.663 INFO: Loaded model from epoch 46\n",
      "2025-06-03 19:11:34.664 INFO: Evaluating train ...\n",
      "2025-06-03 19:11:35.535 INFO: Evaluating valid ...\n",
      "2025-06-03 19:11:35.618 INFO: Evaluating Default ...\n",
      "2025-06-03 19:11:40.956 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         6.7         |      140.2       |        6.44       |\n",
      "|    valid    |         6.5         |      175.1       |        6.77       |\n",
      "|   Default   |         7.0         |      183.6       |        8.02       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 19:11:40.956 INFO: Saving model to MACE_models/mace_benchmark_maxL1_corr3_run-123_swa.model\n",
      "2025-06-03 19:11:41.064 INFO: Compiling model, saving metadata MACE_models/mace_benchmark_maxL1_corr3_swa_compiled.model\n",
      "2025-06-03 19:11:41.904 INFO: Done\n",
      "âœ… Found: RMSE_E = 6.5 meV, RMSE_F = 175.1 meV/Ã…\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for conf in configs_to_test:\n",
    "    cfg = base_config.copy()\n",
    "    cfg.update(conf)\n",
    "    cfg['name'] = f\"mace_benchmark_{conf['name']}\"\n",
    "    cfg['batch_size'] = conf['batch_size']\n",
    "    cleanup_old_files(cfg['name'])\n",
    "\n",
    "    cfg_file = f\"benchmark_results/{cfg['name']}.yaml\"\n",
    "    with open(cfg_file, 'w') as f:\n",
    "        yaml.dump(cfg, f)\n",
    "\n",
    "    print(f\"\\nðŸš€ Training started: {cfg['name']}\")\n",
    "    \n",
    "    monitor_data = []\n",
    "    def monitor_gpu(interval=1.0):\n",
    "        while monitoring:\n",
    "            util, mem = get_gpu_stats()\n",
    "            monitor_data.append((time.time(), util, mem))\n",
    "            time.sleep(interval)\n",
    "\n",
    "\n",
    "    monitoring = True\n",
    "    import threading\n",
    "    monitor_thread = threading.Thread(target=monitor_gpu)\n",
    "    monitor_thread.start()\n",
    "\n",
    "    start_time = time.time()\n",
    "    subprocess.run(['mace_run_train', '--config', cfg_file])\n",
    "    duration = time.time() - start_time\n",
    "    monitoring = False\n",
    "    monitor_thread.join()\n",
    "\n",
    "    if monitor_data:\n",
    "        gpu_util_avg = sum(x[1] for x in monitor_data) / len(monitor_data)\n",
    "        gpu_mem_avg = sum(x[2] for x in monitor_data) / len(monitor_data)\n",
    "    else:\n",
    "        gpu_util_avg = gpu_mem_avg = None\n",
    "\n",
    "    log_file = Path(f\"MACE_models/{cfg['name']}_run-123.log\")\n",
    "    rmse_e, rmse_f = parse_log_file(log_file)\n",
    "\n",
    "    results.append({\n",
    "        'config': cfg['name'],\n",
    "        'batch_size': cfg['batch_size'],\n",
    "        'num_channels': cfg.get('num_channels', 32),\n",
    "        'num_interactions': cfg.get('num_interactions', 2),\n",
    "        'max_L': cfg.get('max_L', 0),\n",
    "        'correlation': cfg.get('correlation', 2),\n",
    "        'train_time_s': round(duration, 2),\n",
    "        'gpu_util_avg': round(gpu_util_avg, 1) if gpu_util_avg else None,\n",
    "        'gpu_mem_avg_MB': round(gpu_mem_avg, 1) if gpu_mem_avg else None,\n",
    "        'rmse_e_meV': rmse_e,\n",
    "        'rmse_f_meV': rmse_f\n",
    "    })\n",
    "\n",
    "df_results = pd.DataFrame(results)\n",
    "df_results.sort_values(\"rmse_f_meV\")\n",
    "df_results.to_csv(\"benchmark_results/parameters_results.csv\", index=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
