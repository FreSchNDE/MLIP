{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1b671c-7191-41cc-8e69-a2a8f516304e",
   "metadata": {},
   "source": [
    "# Training Dataset Size Variation\n",
    "\n",
    "This script trains models for training sets of different sizes and measures energy and force error.\n",
    "Model parameters are the best results from 1-MACE-parameters.ipynb."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4890ccc5-d16c-42eb-9e4a-cf7d1c0c17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# db = read('data/solvent_xtb.xyz', ':')\n",
    "# write('data/solvent_xtb_train_200.xyz', db[:203]) #first 200 configs plus the 3 E0s\n",
    "# write('data/solvent_xtb_train_400.xyz', db[:403])\n",
    "# write('data/solvent_xtb_train_1000.xyz', db[:1003])\n",
    "# write('data/solvent_xtb_train_2000.xyz', db[:2003])\n",
    "# write('data/solvent_xtb_train_4000.xyz', db[:4003])\n",
    "\n",
    "# write('data/solvent_xtb_test.xyz', db[-1000:]) #last 1000 configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af0a2d0b-37a1-4e7f-8261-dfe2f0b97f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import subprocess\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c587058f-a9c9-4c28-9db5-d4bcc5127ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_log_file(log_path):\n",
    "    with open(log_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    rmse_table_lines = [i for i, l in enumerate(lines) if \"+-------------+---------------------+------------------+-------------------+\" in l]\n",
    "    if not rmse_table_lines:\n",
    "        print(\"âŒ No RMSE table found.\")\n",
    "        return None, None\n",
    "\n",
    "    for idx in reversed(rmse_table_lines):\n",
    "        for j in range(idx, min(idx + 10, len(lines))):\n",
    "            if '|    valid    |' in lines[j]:\n",
    "                parts = lines[j].strip().split('|')\n",
    "                try:\n",
    "                    rmse_e = float(parts[2].strip())\n",
    "                    rmse_f = float(parts[3].strip())\n",
    "                    print(f\"âœ… Found: RMSE_E = {rmse_e} meV, RMSE_F = {rmse_f} meV/Ã…\")\n",
    "                    return rmse_e, rmse_f\n",
    "                except Exception as e:\n",
    "                    print(f\"âŒ Error during parsing: {e}\")\n",
    "                    return None, None\n",
    "    print(\"âŒ Found no 'valid' line.\")\n",
    "    return None, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7210b234-0bf4-4fd7-a550-f8c6732c3174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base config from T01-MACE-Practice-I.ipynb\n",
    "base_config = {\n",
    "    'model': 'MACE',\n",
    "    'num_interactions': 3,\n",
    "    'num_channels': 64,\n",
    "    'max_L': 2,\n",
    "    'correlation': 3,\n",
    "    'r_max': 4.0,\n",
    "    'max_ell': 2,\n",
    "    'model_dir': 'MACE_models',\n",
    "    'log_dir': 'MACE_models',\n",
    "    'checkpoints_dir': 'MACE_models',\n",
    "    'results_dir': 'MACE_models',\n",
    "    'test_file': 'data/solvent_xtb_test.xyz',\n",
    "    'E0s': 'average',\n",
    "    'energy_key': 'energy_xtb',\n",
    "    'forces_key': 'forces_xtb',\n",
    "    'device': 'cuda',\n",
    "    'max_num_epochs': 50,\n",
    "    'swa': True,\n",
    "    'seed': 123,\n",
    "    'batch_size': 10,\n",
    "    'valid_fraction': 0.10\n",
    "}\n",
    "\n",
    "# Size of training data to be tested:\n",
    "train_sizes = [200, 400, 1000, 2000, 4000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cffa3186-98eb-4390-800b-7808319eecf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸš€ Training started: mace_learncurve_train200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:49:52.334 INFO: MACE version: 0.3.6\n",
      "2025-06-03 20:49:52.335 INFO: Configuration: Namespace(config='benchmark_results/mace_learncurve_train200.yaml', name='mace_learncurve_train200', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_200.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 20:49:52.450 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 20:49:52.487 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 20:49:52.558 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 20:49:52.558 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 20:49:52.562 INFO: Loaded 200 training configurations from 'data/solvent_xtb_train_200.xyz'\n",
      "2025-06-03 20:49:52.562 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 20:49:52.820 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 20:49:52.841 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 20:49:52.841 INFO: Total number of configurations: train=180, valid=20, tests=[Default: 1000]\n",
      "2025-06-03 20:49:52.842 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 20:49:52.842 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 20:49:53.044 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 20:49:53.200 INFO: Average number of neighbors: 9.86205556634933\n",
      "2025-06-03 20:49:53.200 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 20:49:53.238 INFO: Building model\n",
      "2025-06-03 20:49:53.239 INFO: Hidden irreps: 64x0e+64x1o+64x2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:49:54.685 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 20:49:54.805 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (2): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1-2): 2 x Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "    )\n",
      "    (2): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.177545, shift=0.000000)\n",
      ")\n",
      "2025-06-03 20:49:54.808 INFO: Number of parameters: 405008\n",
      "2025-06-03 20:49:54.808 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 20:49:54.808 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 20:49:54.808 INFO: Started training\n",
      "2025-06-03 20:49:55.753 INFO: Epoch None: loss=72.5723, RMSE_E_per_atom=6251.1 meV, RMSE_F=2627.6 meV / A\n",
      "2025-06-03 20:50:05.408 INFO: Epoch 0: loss=18.8457, RMSE_E_per_atom=4792.7 meV, RMSE_F=1289.0 meV / A\n",
      "2025-06-03 20:50:15.316 INFO: Epoch 2: loss=5.8061, RMSE_E_per_atom=4103.8 meV, RMSE_F=642.4 meV / A\n",
      "2025-06-03 20:50:24.992 INFO: Epoch 4: loss=3.9537, RMSE_E_per_atom=3365.0 meV, RMSE_F=531.7 meV / A\n",
      "2025-06-03 20:50:34.738 INFO: Epoch 6: loss=2.6683, RMSE_E_per_atom=2684.6 meV, RMSE_F=442.2 meV / A\n",
      "2025-06-03 20:50:44.503 INFO: Epoch 8: loss=2.0825, RMSE_E_per_atom=1812.1 meV, RMSE_F=419.5 meV / A\n",
      "2025-06-03 20:50:54.267 INFO: Epoch 10: loss=2.2941, RMSE_E_per_atom=1322.6 meV, RMSE_F=461.8 meV / A\n",
      "2025-06-03 20:51:03.826 INFO: Epoch 12: loss=1.5855, RMSE_E_per_atom=727.8 meV, RMSE_F=392.9 meV / A\n",
      "2025-06-03 20:51:13.499 INFO: Epoch 14: loss=1.7496, RMSE_E_per_atom=302.4 meV, RMSE_F=417.8 meV / A\n",
      "2025-06-03 20:51:22.958 INFO: Epoch 16: loss=0.6523, RMSE_E_per_atom=274.7 meV, RMSE_F=254.8 meV / A\n",
      "2025-06-03 20:51:32.585 INFO: Epoch 18: loss=0.6520, RMSE_E_per_atom=207.3 meV, RMSE_F=255.3 meV / A\n",
      "2025-06-03 20:51:42.314 INFO: Epoch 20: loss=0.9386, RMSE_E_per_atom=311.1 meV, RMSE_F=305.8 meV / A\n",
      "2025-06-03 20:51:51.807 INFO: Epoch 22: loss=0.5526, RMSE_E_per_atom=195.6 meV, RMSE_F=234.5 meV / A\n",
      "2025-06-03 20:52:01.499 INFO: Epoch 24: loss=0.8605, RMSE_E_per_atom=139.7 meV, RMSE_F=293.3 meV / A\n",
      "2025-06-03 20:52:11.011 INFO: Epoch 26: loss=0.7527, RMSE_E_per_atom=202.6 meV, RMSE_F=274.7 meV / A\n",
      "2025-06-03 20:52:20.520 INFO: Epoch 28: loss=0.4753, RMSE_E_per_atom=219.7 meV, RMSE_F=217.3 meV / A\n",
      "2025-06-03 20:52:30.252 INFO: Epoch 30: loss=0.8868, RMSE_E_per_atom=211.4 meV, RMSE_F=298.0 meV / A\n",
      "2025-06-03 20:52:39.779 INFO: Epoch 32: loss=0.5715, RMSE_E_per_atom=125.7 meV, RMSE_F=239.6 meV / A\n",
      "2025-06-03 20:52:49.301 INFO: Epoch 34: loss=0.6190, RMSE_E_per_atom=116.7 meV, RMSE_F=249.6 meV / A\n",
      "2025-06-03 20:52:54.003 INFO: Changing loss based on SWA\n",
      "2025-06-03 20:52:58.896 INFO: Epoch 36: loss=0.5321, RMSE_E_per_atom=36.2 meV, RMSE_F=200.6 meV / A\n",
      "2025-06-03 20:53:08.637 INFO: Epoch 38: loss=0.2832, RMSE_E_per_atom=10.3 meV, RMSE_F=165.6 meV / A\n",
      "2025-06-03 20:53:18.344 INFO: Epoch 40: loss=0.3458, RMSE_E_per_atom=29.2 meV, RMSE_F=161.9 meV / A\n",
      "2025-06-03 20:53:27.874 INFO: Epoch 42: loss=0.2481, RMSE_E_per_atom=10.2 meV, RMSE_F=154.8 meV / A\n",
      "2025-06-03 20:53:37.675 INFO: Epoch 44: loss=0.2817, RMSE_E_per_atom=18.6 meV, RMSE_F=157.8 meV / A\n",
      "2025-06-03 20:53:47.226 INFO: Epoch 46: loss=0.2463, RMSE_E_per_atom=6.3 meV, RMSE_F=156.3 meV / A\n",
      "2025-06-03 20:53:57.004 INFO: Epoch 48: loss=0.2642, RMSE_E_per_atom=15.8 meV, RMSE_F=155.4 meV / A\n",
      "2025-06-03 20:54:01.678 INFO: Training complete\n",
      "2025-06-03 20:54:01.678 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 20:54:02.142 INFO: Loading checkpoint: MACE_models/mace_learncurve_train200_run-123_epoch-28.pt\n",
      "2025-06-03 20:54:02.189 INFO: Loaded model from epoch 28\n",
      "2025-06-03 20:54:02.190 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:54:03.895 INFO: Evaluating valid ...\n",
      "2025-06-03 20:54:04.115 INFO: Evaluating Default ...\n",
      "2025-06-03 20:54:13.819 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |        203.4        |      187.7       |        8.62       |\n",
      "|    valid    |        219.7        |      217.3       |        8.40       |\n",
      "|   Default   |        209.4        |      224.5       |        9.80       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 20:54:13.819 INFO: Saving model to MACE_models/mace_learncurve_train200_run-123.model\n",
      "2025-06-03 20:54:14.058 INFO: Compiling model, saving metadata to MACE_models/mace_learncurve_train200_compiled.model\n",
      "2025-06-03 20:54:15.443 INFO: Loading checkpoint: MACE_models/mace_learncurve_train200_run-123_epoch-48_swa.pt\n",
      "2025-06-03 20:54:15.822 INFO: Loaded model from epoch 48\n",
      "2025-06-03 20:54:15.823 INFO: Evaluating train ...\n",
      "2025-06-03 20:54:17.480 INFO: Evaluating valid ...\n",
      "2025-06-03 20:54:17.705 INFO: Evaluating Default ...\n",
      "2025-06-03 20:54:27.450 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         10.6        |      105.0       |        4.82       |\n",
      "|    valid    |         9.0         |      156.2       |        6.04       |\n",
      "|   Default   |         10.9        |      161.5       |        7.05       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 20:54:27.450 INFO: Saving model to MACE_models/mace_learncurve_train200_run-123_swa.model\n",
      "2025-06-03 20:54:27.692 INFO: Compiling model, saving metadata MACE_models/mace_learncurve_train200_swa_compiled.model\n",
      "2025-06-03 20:54:28.936 INFO: Done\n",
      "âœ… Found: RMSE_E = 9.0 meV, RMSE_F = 156.2 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_learncurve_train400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:54:32.311 INFO: MACE version: 0.3.6\n",
      "2025-06-03 20:54:32.311 INFO: Configuration: Namespace(config='benchmark_results/mace_learncurve_train400.yaml', name='mace_learncurve_train400', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_400.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 20:54:32.402 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 20:54:32.440 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 20:54:32.560 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 20:54:32.560 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 20:54:32.568 INFO: Loaded 400 training configurations from 'data/solvent_xtb_train_400.xyz'\n",
      "2025-06-03 20:54:32.568 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 20:54:32.824 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 20:54:32.845 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 20:54:32.845 INFO: Total number of configurations: train=360, valid=40, tests=[Default: 1000]\n",
      "2025-06-03 20:54:32.847 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 20:54:32.847 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 20:54:33.043 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 20:54:33.236 INFO: Average number of neighbors: 9.929327941459658\n",
      "2025-06-03 20:54:33.236 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 20:54:33.312 INFO: Building model\n",
      "2025-06-03 20:54:33.402 INFO: Hidden irreps: 64x0e+64x1o+64x2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 20:54:34.861 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 20:54:34.980 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (2): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1-2): 2 x Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "    )\n",
      "    (2): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.471057, shift=0.000000)\n",
      ")\n",
      "2025-06-03 20:54:34.983 INFO: Number of parameters: 405008\n",
      "2025-06-03 20:54:34.983 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 20:54:34.983 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 20:54:34.983 INFO: Started training\n",
      "2025-06-03 20:54:37.333 INFO: Epoch None: loss=65.3744, RMSE_E_per_atom=6200.4 meV, RMSE_F=2517.3 meV / A\n",
      "2025-06-03 20:54:51.197 INFO: Epoch 0: loss=9.8561, RMSE_E_per_atom=4351.3 meV, RMSE_F=896.5 meV / A\n",
      "2025-06-03 20:55:10.601 INFO: Epoch 2: loss=4.4978, RMSE_E_per_atom=3055.9 meV, RMSE_F=595.8 meV / A\n",
      "2025-06-03 20:55:30.143 INFO: Epoch 4: loss=2.2275, RMSE_E_per_atom=1289.8 meV, RMSE_F=455.2 meV / A\n",
      "2025-06-03 20:55:49.784 INFO: Epoch 6: loss=1.8776, RMSE_E_per_atom=785.0 meV, RMSE_F=425.2 meV / A\n",
      "2025-06-03 20:56:09.290 INFO: Epoch 8: loss=0.9828, RMSE_E_per_atom=303.1 meV, RMSE_F=313.9 meV / A\n",
      "2025-06-03 20:56:28.768 INFO: Epoch 10: loss=1.0528, RMSE_E_per_atom=242.0 meV, RMSE_F=321.4 meV / A\n",
      "2025-06-03 20:56:48.131 INFO: Epoch 12: loss=0.6707, RMSE_E_per_atom=257.0 meV, RMSE_F=255.3 meV / A\n",
      "2025-06-03 20:57:07.659 INFO: Epoch 14: loss=0.6499, RMSE_E_per_atom=245.4 meV, RMSE_F=252.9 meV / A\n",
      "2025-06-03 20:57:27.169 INFO: Epoch 16: loss=0.6969, RMSE_E_per_atom=86.5 meV, RMSE_F=262.9 meV / A\n",
      "2025-06-03 20:57:46.816 INFO: Epoch 18: loss=0.5928, RMSE_E_per_atom=111.7 meV, RMSE_F=241.1 meV / A\n",
      "2025-06-03 20:58:06.322 INFO: Epoch 20: loss=0.7320, RMSE_E_per_atom=62.7 meV, RMSE_F=270.6 meV / A\n",
      "2025-06-03 20:58:25.686 INFO: Epoch 22: loss=0.3470, RMSE_E_per_atom=52.1 meV, RMSE_F=184.2 meV / A\n",
      "2025-06-03 20:58:45.163 INFO: Epoch 24: loss=0.8827, RMSE_E_per_atom=89.9 meV, RMSE_F=294.7 meV / A\n",
      "2025-06-03 20:59:04.448 INFO: Epoch 26: loss=0.5955, RMSE_E_per_atom=87.2 meV, RMSE_F=241.9 meV / A\n",
      "2025-06-03 20:59:23.812 INFO: Epoch 28: loss=0.4840, RMSE_E_per_atom=101.0 meV, RMSE_F=218.3 meV / A\n",
      "2025-06-03 20:59:43.118 INFO: Epoch 30: loss=0.5023, RMSE_E_per_atom=20.4 meV, RMSE_F=222.9 meV / A\n",
      "2025-06-03 21:00:02.398 INFO: Epoch 32: loss=0.5469, RMSE_E_per_atom=89.0 meV, RMSE_F=229.7 meV / A\n",
      "2025-06-03 21:00:21.762 INFO: Epoch 34: loss=0.6297, RMSE_E_per_atom=62.1 meV, RMSE_F=249.4 meV / A\n",
      "2025-06-03 21:00:31.245 INFO: Changing loss based on SWA\n",
      "2025-06-03 21:00:41.263 INFO: Epoch 36: loss=0.5222, RMSE_E_per_atom=44.2 meV, RMSE_F=177.6 meV / A\n",
      "2025-06-03 21:01:00.794 INFO: Epoch 38: loss=0.2916, RMSE_E_per_atom=7.6 meV, RMSE_F=164.9 meV / A\n",
      "2025-06-03 21:01:20.357 INFO: Epoch 40: loss=0.2839, RMSE_E_per_atom=6.1 meV, RMSE_F=163.3 meV / A\n",
      "2025-06-03 21:01:39.912 INFO: Epoch 42: loss=0.2775, RMSE_E_per_atom=8.8 meV, RMSE_F=159.8 meV / A\n",
      "2025-06-03 21:01:59.442 INFO: Epoch 44: loss=0.2780, RMSE_E_per_atom=12.3 meV, RMSE_F=157.8 meV / A\n",
      "2025-06-03 21:02:18.879 INFO: Epoch 46: loss=0.2609, RMSE_E_per_atom=5.0 meV, RMSE_F=156.1 meV / A\n",
      "2025-06-03 21:02:38.354 INFO: Epoch 48: loss=0.2591, RMSE_E_per_atom=6.7 meV, RMSE_F=155.0 meV / A\n",
      "2025-06-03 21:02:48.007 INFO: Training complete\n",
      "2025-06-03 21:02:48.008 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 21:02:48.512 INFO: Loading checkpoint: MACE_models/mace_learncurve_train400_run-123_epoch-26.pt\n",
      "2025-06-03 21:02:48.560 INFO: Loaded model from epoch 26\n",
      "2025-06-03 21:02:48.560 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:02:51.934 INFO: Evaluating valid ...\n",
      "2025-06-03 21:02:52.352 INFO: Evaluating Default ...\n",
      "2025-06-03 21:03:02.059 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         35.0        |      150.5       |        6.09       |\n",
      "|    valid    |         35.0        |      195.7       |        7.89       |\n",
      "|   Default   |         36.2        |      167.6       |        7.32       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 21:03:02.059 INFO: Saving model to MACE_models/mace_learncurve_train400_run-123.model\n",
      "2025-06-03 21:03:02.290 INFO: Compiling model, saving metadata to MACE_models/mace_learncurve_train400_compiled.model\n",
      "2025-06-03 21:03:03.662 INFO: Loading checkpoint: MACE_models/mace_learncurve_train400_run-123_epoch-48_swa.pt\n",
      "2025-06-03 21:03:03.837 INFO: Loaded model from epoch 48\n",
      "2025-06-03 21:03:03.837 INFO: Evaluating train ...\n",
      "2025-06-03 21:03:07.196 INFO: Evaluating valid ...\n",
      "2025-06-03 21:03:07.624 INFO: Evaluating Default ...\n",
      "2025-06-03 21:03:17.366 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         7.2         |       85.8       |        3.47       |\n",
      "|    valid    |         6.7         |      155.0       |        6.25       |\n",
      "|   Default   |         7.8         |      122.0       |        5.33       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 21:03:17.366 INFO: Saving model to MACE_models/mace_learncurve_train400_run-123_swa.model\n",
      "2025-06-03 21:03:17.597 INFO: Compiling model, saving metadata MACE_models/mace_learncurve_train400_swa_compiled.model\n",
      "2025-06-03 21:03:18.839 INFO: Done\n",
      "âœ… Found: RMSE_E = 6.7 meV, RMSE_F = 155.0 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_learncurve_train1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:03:22.174 INFO: MACE version: 0.3.6\n",
      "2025-06-03 21:03:22.174 INFO: Configuration: Namespace(config='benchmark_results/mace_learncurve_train1000.yaml', name='mace_learncurve_train1000', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_1000.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 21:03:22.267 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 21:03:22.304 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 21:03:22.567 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 21:03:22.568 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 21:03:22.588 INFO: Loaded 1000 training configurations from 'data/solvent_xtb_train_1000.xyz'\n",
      "2025-06-03 21:03:22.589 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 21:03:22.931 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 21:03:22.953 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 21:03:22.953 INFO: Total number of configurations: train=900, valid=100, tests=[Default: 1000]\n",
      "2025-06-03 21:03:22.958 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 21:03:22.958 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 21:03:23.436 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 21:03:23.754 INFO: Average number of neighbors: 10.016136329362087\n",
      "2025-06-03 21:03:23.754 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 21:03:23.947 INFO: Building model\n",
      "2025-06-03 21:03:23.948 INFO: Hidden irreps: 64x0e+64x1o+64x2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:03:25.396 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 21:03:25.517 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (2): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1-2): 2 x Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "    )\n",
      "    (2): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.244857, shift=0.000000)\n",
      ")\n",
      "2025-06-03 21:03:25.520 INFO: Number of parameters: 405008\n",
      "2025-06-03 21:03:25.520 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 21:03:25.520 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 21:03:25.520 INFO: Started training\n",
      "2025-06-03 21:03:29.225 INFO: Epoch None: loss=51.3840, RMSE_E_per_atom=6134.4 meV, RMSE_F=2200.4 meV / A\n",
      "2025-06-03 21:03:57.335 INFO: Epoch 0: loss=3.3188, RMSE_E_per_atom=3142.3 meV, RMSE_F=488.4 meV / A\n",
      "2025-06-03 21:04:46.779 INFO: Epoch 2: loss=0.9617, RMSE_E_per_atom=504.8 meV, RMSE_F=307.9 meV / A\n",
      "2025-06-03 21:05:36.299 INFO: Epoch 4: loss=1.1512, RMSE_E_per_atom=236.9 meV, RMSE_F=339.9 meV / A\n",
      "2025-06-03 21:06:25.917 INFO: Epoch 6: loss=0.2969, RMSE_E_per_atom=157.2 meV, RMSE_F=172.8 meV / A\n",
      "2025-06-03 21:07:15.419 INFO: Epoch 8: loss=0.5709, RMSE_E_per_atom=170.9 meV, RMSE_F=239.7 meV / A\n",
      "2025-06-03 21:08:04.710 INFO: Epoch 10: loss=0.2755, RMSE_E_per_atom=90.3 meV, RMSE_F=166.8 meV / A\n",
      "2025-06-03 21:08:54.335 INFO: Epoch 12: loss=0.5476, RMSE_E_per_atom=133.1 meV, RMSE_F=236.4 meV / A\n",
      "2025-06-03 21:09:43.961 INFO: Epoch 14: loss=0.2035, RMSE_E_per_atom=20.8 meV, RMSE_F=143.8 meV / A\n",
      "2025-06-03 21:10:33.694 INFO: Epoch 16: loss=0.2650, RMSE_E_per_atom=93.7 meV, RMSE_F=163.8 meV / A\n",
      "2025-06-03 21:11:23.277 INFO: Epoch 18: loss=0.2646, RMSE_E_per_atom=54.1 meV, RMSE_F=164.0 meV / A\n",
      "2025-06-03 21:12:12.731 INFO: Epoch 20: loss=0.2015, RMSE_E_per_atom=73.2 meV, RMSE_F=142.8 meV / A\n",
      "2025-06-03 21:13:02.527 INFO: Epoch 22: loss=0.2235, RMSE_E_per_atom=78.7 meV, RMSE_F=149.8 meV / A\n",
      "2025-06-03 21:13:52.019 INFO: Epoch 24: loss=0.2384, RMSE_E_per_atom=77.1 meV, RMSE_F=155.1 meV / A\n",
      "2025-06-03 21:14:41.408 INFO: Epoch 26: loss=0.1276, RMSE_E_per_atom=83.0 meV, RMSE_F=113.9 meV / A\n",
      "2025-06-03 21:15:31.064 INFO: Epoch 28: loss=0.1101, RMSE_E_per_atom=70.4 meV, RMSE_F=106.1 meV / A\n",
      "2025-06-03 21:16:20.657 INFO: Epoch 30: loss=0.1948, RMSE_E_per_atom=87.1 meV, RMSE_F=140.6 meV / A\n",
      "2025-06-03 21:17:10.075 INFO: Epoch 32: loss=0.1373, RMSE_E_per_atom=72.6 meV, RMSE_F=118.2 meV / A\n",
      "2025-06-03 21:17:59.826 INFO: Epoch 34: loss=0.2021, RMSE_E_per_atom=70.0 meV, RMSE_F=142.9 meV / A\n",
      "2025-06-03 21:18:24.134 INFO: Changing loss based on SWA\n",
      "2025-06-03 21:18:49.302 INFO: Epoch 36: loss=0.0606, RMSE_E_per_atom=5.9 meV, RMSE_F=76.6 meV / A\n",
      "2025-06-03 21:19:38.905 INFO: Epoch 38: loss=0.0613, RMSE_E_per_atom=9.2 meV, RMSE_F=73.6 meV / A\n",
      "2025-06-03 21:20:28.472 INFO: Epoch 40: loss=0.0531, RMSE_E_per_atom=4.8 meV, RMSE_F=72.2 meV / A\n",
      "2025-06-03 21:21:18.201 INFO: Epoch 42: loss=0.0522, RMSE_E_per_atom=4.3 meV, RMSE_F=71.8 meV / A\n",
      "2025-06-03 21:22:07.868 INFO: Epoch 44: loss=0.0583, RMSE_E_per_atom=10.0 meV, RMSE_F=70.5 meV / A\n",
      "2025-06-03 21:22:57.357 INFO: Epoch 46: loss=0.0520, RMSE_E_per_atom=6.1 meV, RMSE_F=70.3 meV / A\n",
      "2025-06-03 21:23:47.010 INFO: Epoch 48: loss=0.0482, RMSE_E_per_atom=3.8 meV, RMSE_F=69.2 meV / A\n",
      "2025-06-03 21:24:11.469 INFO: Training complete\n",
      "2025-06-03 21:24:11.469 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 21:24:12.086 INFO: Loading checkpoint: MACE_models/mace_learncurve_train1000_run-123_epoch-34.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:24:12.334 INFO: Loaded model from epoch 34\n",
      "2025-06-03 21:24:12.335 INFO: Evaluating train ...\n",
      "2025-06-03 21:24:21.044 INFO: Evaluating valid ...\n",
      "2025-06-03 21:24:21.990 INFO: Evaluating Default ...\n",
      "2025-06-03 21:24:31.744 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         79.6        |      109.0       |        4.86       |\n",
      "|    valid    |         81.0        |      105.5       |        4.89       |\n",
      "|   Default   |         78.1        |      116.1       |        5.07       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 21:24:31.745 INFO: Saving model to MACE_models/mace_learncurve_train1000_run-123.model\n",
      "2025-06-03 21:24:31.964 INFO: Compiling model, saving metadata to MACE_models/mace_learncurve_train1000_compiled.model\n",
      "2025-06-03 21:24:33.237 INFO: Loading checkpoint: MACE_models/mace_learncurve_train1000_run-123_epoch-48_swa.pt\n",
      "2025-06-03 21:24:33.410 INFO: Loaded model from epoch 48\n",
      "2025-06-03 21:24:33.410 INFO: Evaluating train ...\n",
      "2025-06-03 21:24:42.014 INFO: Evaluating valid ...\n",
      "2025-06-03 21:24:42.931 INFO: Evaluating Default ...\n",
      "2025-06-03 21:24:52.759 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         4.1         |       64.2       |        2.86       |\n",
      "|    valid    |         3.8         |       69.2       |        3.21       |\n",
      "|   Default   |         4.0         |       83.0       |        3.62       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 21:24:52.759 INFO: Saving model to MACE_models/mace_learncurve_train1000_run-123_swa.model\n",
      "2025-06-03 21:24:53.000 INFO: Compiling model, saving metadata MACE_models/mace_learncurve_train1000_swa_compiled.model\n",
      "2025-06-03 21:24:54.264 INFO: Done\n",
      "âœ… Found: RMSE_E = 3.8 meV, RMSE_F = 69.2 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_learncurve_train2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:24:57.711 INFO: MACE version: 0.3.6\n",
      "2025-06-03 21:24:57.711 INFO: Configuration: Namespace(config='benchmark_results/mace_learncurve_train2000.yaml', name='mace_learncurve_train2000', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_2000.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 21:24:57.802 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 21:24:57.839 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 21:24:58.433 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 21:24:58.435 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 21:24:58.476 INFO: Loaded 2000 training configurations from 'data/solvent_xtb_train_2000.xyz'\n",
      "2025-06-03 21:24:58.477 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 21:24:58.733 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 21:24:58.754 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 21:24:58.754 INFO: Total number of configurations: train=1800, valid=200, tests=[Default: 1000]\n",
      "2025-06-03 21:24:58.763 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 21:24:58.763 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 21:24:59.712 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 21:25:00.228 INFO: Average number of neighbors: 10.068712162390119\n",
      "2025-06-03 21:25:00.228 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 21:25:00.623 INFO: Building model\n",
      "2025-06-03 21:25:00.624 INFO: Hidden irreps: 64x0e+64x1o+64x2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 21:25:02.214 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 21:25:02.339 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (2): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1-2): 2 x Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "    )\n",
      "    (2): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.254663, shift=0.000000)\n",
      ")\n",
      "2025-06-03 21:25:02.342 INFO: Number of parameters: 405008\n",
      "2025-06-03 21:25:02.342 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 21:25:02.342 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 21:25:02.342 INFO: Started training\n",
      "2025-06-03 21:25:07.028 INFO: Epoch None: loss=50.1083, RMSE_E_per_atom=6122.2 meV, RMSE_F=2152.7 meV / A\n",
      "2025-06-03 21:26:00.797 INFO: Epoch 0: loss=2.6603, RMSE_E_per_atom=1615.9 meV, RMSE_F=489.5 meV / A\n",
      "2025-06-03 21:27:40.221 INFO: Epoch 2: loss=0.5388, RMSE_E_per_atom=324.6 meV, RMSE_F=229.5 meV / A\n",
      "2025-06-03 21:29:20.227 INFO: Epoch 4: loss=0.5340, RMSE_E_per_atom=153.2 meV, RMSE_F=230.5 meV / A\n",
      "2025-06-03 21:30:59.606 INFO: Epoch 6: loss=0.2907, RMSE_E_per_atom=108.2 meV, RMSE_F=170.0 meV / A\n",
      "2025-06-03 21:32:39.325 INFO: Epoch 8: loss=0.2465, RMSE_E_per_atom=74.7 meV, RMSE_F=156.8 meV / A\n",
      "2025-06-03 21:34:18.955 INFO: Epoch 10: loss=0.2891, RMSE_E_per_atom=69.0 meV, RMSE_F=169.8 meV / A\n",
      "2025-06-03 21:35:58.364 INFO: Epoch 12: loss=0.2568, RMSE_E_per_atom=76.6 meV, RMSE_F=158.7 meV / A\n",
      "2025-06-03 21:37:38.004 INFO: Epoch 14: loss=0.2355, RMSE_E_per_atom=72.7 meV, RMSE_F=152.9 meV / A\n",
      "2025-06-03 21:39:17.808 INFO: Epoch 16: loss=0.2655, RMSE_E_per_atom=82.9 meV, RMSE_F=162.2 meV / A\n",
      "2025-06-03 21:40:57.658 INFO: Epoch 18: loss=0.1458, RMSE_E_per_atom=105.5 meV, RMSE_F=120.0 meV / A\n",
      "2025-06-03 21:42:37.788 INFO: Epoch 20: loss=0.1253, RMSE_E_per_atom=65.8 meV, RMSE_F=111.8 meV / A\n",
      "2025-06-03 21:44:17.733 INFO: Epoch 22: loss=0.1417, RMSE_E_per_atom=71.5 meV, RMSE_F=118.8 meV / A\n",
      "2025-06-03 21:45:57.524 INFO: Epoch 24: loss=0.2066, RMSE_E_per_atom=85.7 meV, RMSE_F=143.2 meV / A\n",
      "2025-06-03 21:47:37.226 INFO: Epoch 26: loss=0.1376, RMSE_E_per_atom=76.5 meV, RMSE_F=116.8 meV / A\n",
      "2025-06-03 21:49:17.371 INFO: Epoch 28: loss=0.2033, RMSE_E_per_atom=122.5 meV, RMSE_F=142.2 meV / A\n",
      "2025-06-03 21:50:57.084 INFO: Epoch 30: loss=0.0932, RMSE_E_per_atom=60.3 meV, RMSE_F=96.4 meV / A\n",
      "2025-06-03 21:52:37.076 INFO: Epoch 32: loss=0.0872, RMSE_E_per_atom=56.4 meV, RMSE_F=93.4 meV / A\n",
      "2025-06-03 21:54:17.092 INFO: Epoch 34: loss=0.1355, RMSE_E_per_atom=62.3 meV, RMSE_F=116.2 meV / A\n",
      "2025-06-03 21:55:06.036 INFO: Changing loss based on SWA\n",
      "2025-06-03 21:55:56.899 INFO: Epoch 36: loss=0.0570, RMSE_E_per_atom=7.0 meV, RMSE_F=72.1 meV / A\n",
      "2025-06-03 21:57:36.800 INFO: Epoch 38: loss=0.0486, RMSE_E_per_atom=4.6 meV, RMSE_F=68.1 meV / A\n",
      "2025-06-03 21:59:16.616 INFO: Epoch 40: loss=0.0473, RMSE_E_per_atom=5.4 meV, RMSE_F=66.5 meV / A\n",
      "2025-06-03 22:00:56.511 INFO: Epoch 42: loss=0.0465, RMSE_E_per_atom=3.1 meV, RMSE_F=67.4 meV / A\n",
      "2025-06-03 22:02:36.743 INFO: Epoch 44: loss=0.0442, RMSE_E_per_atom=3.0 meV, RMSE_F=65.7 meV / A\n",
      "2025-06-03 22:04:16.771 INFO: Epoch 46: loss=0.0430, RMSE_E_per_atom=3.3 meV, RMSE_F=64.6 meV / A\n",
      "2025-06-03 22:05:56.561 INFO: Epoch 48: loss=0.0431, RMSE_E_per_atom=3.8 meV, RMSE_F=64.5 meV / A\n",
      "2025-06-03 22:06:45.631 INFO: Training complete\n",
      "2025-06-03 22:06:45.632 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 22:06:46.097 INFO: Loading checkpoint: MACE_models/mace_learncurve_train2000_run-123_epoch-32.pt\n",
      "2025-06-03 22:06:46.280 INFO: Loaded model from epoch 32\n",
      "2025-06-03 22:06:46.281 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:07:03.992 INFO: Evaluating valid ...\n",
      "2025-06-03 22:07:05.890 INFO: Evaluating Default ...\n",
      "2025-06-03 22:07:15.676 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         56.2        |       85.7       |        3.80       |\n",
      "|    valid    |         56.4        |       93.4       |        4.44       |\n",
      "|   Default   |         54.9        |       93.6       |        4.09       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 22:07:15.677 INFO: Saving model to MACE_models/mace_learncurve_train2000_run-123.model\n",
      "2025-06-03 22:07:15.910 INFO: Compiling model, saving metadata to MACE_models/mace_learncurve_train2000_compiled.model\n",
      "2025-06-03 22:07:17.170 INFO: Loading checkpoint: MACE_models/mace_learncurve_train2000_run-123_epoch-48_swa.pt\n",
      "2025-06-03 22:07:17.575 INFO: Loaded model from epoch 48\n",
      "2025-06-03 22:07:17.576 INFO: Evaluating train ...\n",
      "2025-06-03 22:07:35.346 INFO: Evaluating valid ...\n",
      "2025-06-03 22:07:37.247 INFO: Evaluating Default ...\n",
      "2025-06-03 22:07:47.078 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         4.4         |       49.7       |        2.20       |\n",
      "|    valid    |         3.8         |       64.5       |        3.06       |\n",
      "|   Default   |         4.3         |       64.0       |        2.80       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 22:07:47.078 INFO: Saving model to MACE_models/mace_learncurve_train2000_run-123_swa.model\n",
      "2025-06-03 22:07:47.316 INFO: Compiling model, saving metadata MACE_models/mace_learncurve_train2000_swa_compiled.model\n",
      "2025-06-03 22:07:48.645 INFO: Done\n",
      "âœ… Found: RMSE_E = 3.8 meV, RMSE_F = 64.5 meV/Ã…\n",
      "\n",
      "ðŸš€ Training started: mace_learncurve_train4000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/e3nn/o3/_wigner.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  _Jd, _W3j_flat, _W3j_indices = torch.load(os.path.join(os.path.dirname(__file__), 'constants.pt'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:07:52.086 INFO: MACE version: 0.3.6\n",
      "2025-06-03 22:07:52.086 INFO: Configuration: Namespace(config='benchmark_results/mace_learncurve_train4000.yaml', name='mace_learncurve_train4000', seed=123, log_dir='MACE_models', model_dir='MACE_models', checkpoints_dir='MACE_models', results_dir='MACE_models', downloads_dir='downloads', device='cuda', default_dtype='float64', distributed=False, log_level='INFO', error_table='PerAtomRMSE', model='MACE', r_max=4.0, radial_type='bessel', num_radial_basis=8, num_cutoff_basis=5, pair_repulsion=False, distance_transform='None', interaction='RealAgnosticResidualInteractionBlock', interaction_first='RealAgnosticResidualInteractionBlock', max_ell=2, correlation=3, num_interactions=3, MLP_irreps='16x0e', radial_MLP='[64, 64, 64]', hidden_irreps='128x0e + 128x1o', num_channels=64, max_L=2, gate='silu', scaling='rms_forces_scaling', avg_num_neighbors=1, compute_avg_num_neighbors=True, compute_stress=False, compute_forces=True, train_file='data/solvent_xtb_train_4000.xyz', valid_file=None, valid_fraction=0.1, test_file='data/solvent_xtb_test.xyz', test_dir=None, multi_processed_test=False, num_workers=0, pin_memory=True, atomic_numbers=None, mean=None, std=None, statistics_file=None, E0s='average', keep_isolated_atoms=False, energy_key='energy_xtb', forces_key='forces_xtb', virials_key='virials', stress_key='stress', dipole_key='dipole', charges_key='charges', loss='weighted', forces_weight=100.0, swa_forces_weight=100.0, energy_weight=1.0, swa_energy_weight=1000.0, virials_weight=1.0, swa_virials_weight=10.0, stress_weight=1.0, swa_stress_weight=10.0, dipole_weight=1.0, swa_dipole_weight=1.0, config_type_weights='{\"Default\":1.0}', huber_delta=0.01, optimizer='adam', beta=0.9, batch_size=10, valid_batch_size=10, lr=0.01, swa_lr=0.001, weight_decay=5e-07, amsgrad=True, scheduler='ReduceLROnPlateau', lr_factor=0.8, scheduler_patience=50, lr_scheduler_gamma=0.9993, swa=True, start_swa=None, ema=False, ema_decay=0.99, max_num_epochs=50, patience=2048, foundation_model=None, foundation_model_readout=True, eval_interval=2, keep_checkpoints=False, save_all_checkpoints=False, restart_latest=False, save_cpu=False, clip_grad=10.0, wandb=False, wandb_dir=None, wandb_project='', wandb_entity='', wandb_name='', wandb_log_hypers=['num_channels', 'max_L', 'correlation', 'lr', 'swa_lr', 'weight_decay', 'batch_size', 'max_num_epochs', 'start_swa', 'energy_weight', 'forces_weight'])\n",
      "2025-06-03 22:07:52.196 INFO: CUDA version: 12.1, CUDA device: 0\n",
      "2025-06-03 22:07:52.233 INFO: Current Git commit: 3b7b691f60afdffc0cd66948e333883ae1689cd8\n",
      "2025-06-03 22:07:53.293 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 22:07:53.297 INFO: Using isolated atom energies from training file\n",
      "2025-06-03 22:07:53.375 INFO: Loaded 4000 training configurations from 'data/solvent_xtb_train_4000.xyz'\n",
      "2025-06-03 22:07:53.377 INFO: Using random 10.0% of training set for validation\n",
      "2025-06-03 22:07:53.629 INFO: Since ASE version 3.23.0b1, using stress_key 'stress' is no longer safe when communicating between MACE and ASE. We recommend using a different key, rewriting energies to 'REF_stress'. You need to use --stress_key='REF_stress', to tell the key name chosen.\n",
      "2025-06-03 22:07:53.649 INFO: Loaded 1000 test configurations from 'data/solvent_xtb_test.xyz'\n",
      "2025-06-03 22:07:53.649 INFO: Total number of configurations: train=3600, valid=400, tests=[Default: 1000]\n",
      "2025-06-03 22:07:53.668 INFO: AtomicNumberTable: (1, 6, 8)\n",
      "2025-06-03 22:07:53.668 INFO: Atomic energies: [-10.707211383396714, -48.847445262804705, -102.57117256025786]\n",
      "2025-06-03 22:07:55.613 INFO: WeightedEnergyForcesLoss(energy_weight=1.000, forces_weight=100.000)\n",
      "2025-06-03 22:07:56.514 INFO: Average number of neighbors: 10.085180744169618\n",
      "2025-06-03 22:07:56.515 INFO: Selected the following outputs: {'energy': True, 'forces': True, 'virials': False, 'stress': False, 'dipoles': False}\n",
      "2025-06-03 22:07:57.298 INFO: Building model\n",
      "2025-06-03 22:07:57.300 INFO: Hidden irreps: 64x0e+64x1o+64x2e\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/dist-packages/torch/jit/_check.py:178: UserWarning: The TorchScript type system doesn't support instance-level annotations on empty non-base types in `__init__`. Instead, either 1) use a type annotation in the class body, or 2) wrap the type in `torch.jit.Attribute`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 22:07:58.747 INFO: Using stochastic weight averaging (after 36 epochs) with energy weight : 1000.0, forces weight : 100.0 and learning rate : 0.001\n",
      "2025-06-03 22:07:58.866 INFO: ScaleShiftMACE(\n",
      "  (node_embedding): LinearNodeEmbeddingBlock(\n",
      "    (linear): Linear(3x0e -> 64x0e | 192 weights)\n",
      "  )\n",
      "  (radial_embedding): RadialEmbeddingBlock(\n",
      "    (bessel_fn): BesselBasis(r_max=4.0, num_basis=8, trainable=False)\n",
      "    (cutoff_fn): PolynomialCutoff(p=5.0, r_max=4.0)\n",
      "  )\n",
      "  (spherical_harmonics): SphericalHarmonics()\n",
      "  (atomic_energies_fn): AtomicEnergiesBlock(energies=[-10.7072, -48.8474, -102.5712])\n",
      "  (interactions): ModuleList(\n",
      "    (0): RealAgnosticInteractionBlock(\n",
      "      (linear_up): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "      (conv_tp): TensorProduct(64x0e x 1x0e+1x1o+1x2e -> 64x0e+64x1o+64x2e | 192 paths | 192 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 192]\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (1): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e+64x1o+64x2e | 36864 paths | 36864 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "    (2): RealAgnosticResidualInteractionBlock(\n",
      "      (linear_up): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "      (conv_tp): TensorProduct(64x0e+64x1o+64x2e x 1x0e+1x1o+1x2e -> 192x0e+256x1o+256x2e | 704 paths | 704 weights)\n",
      "      (conv_tp_weights): FullyConnectedNet[8, 64, 64, 64, 704]\n",
      "      (linear): Linear(192x0e+256x1o+256x2e -> 64x0e+64x1o+64x2e | 45056 weights)\n",
      "      (skip_tp): FullyConnectedTensorProduct(64x0e+64x1o+64x2e x 3x0e -> 64x0e | 12288 paths | 12288 weights)\n",
      "      (reshape): reshape_irreps()\n",
      "    )\n",
      "  )\n",
      "  (products): ModuleList(\n",
      "    (0-1): 2 x EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "          (1-2): 2 x Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x4x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 64x0e+64x1o+64x2e | 12288 weights)\n",
      "    )\n",
      "    (2): EquivariantProductBasisBlock(\n",
      "      (symmetric_contractions): SymmetricContraction(\n",
      "        (contractions): ModuleList(\n",
      "          (0): Contraction(\n",
      "            (contractions_weighting): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (contractions_features): ModuleList(\n",
      "              (0-1): 2 x GraphModule()\n",
      "            )\n",
      "            (weights): ParameterList(\n",
      "                (0): Parameter containing: [torch.float64 of size 3x3x64 (cuda:0)]\n",
      "                (1): Parameter containing: [torch.float64 of size 3x1x64 (cuda:0)]\n",
      "            )\n",
      "            (graph_opt_main): GraphModule()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (linear): Linear(64x0e -> 64x0e | 4096 weights)\n",
      "    )\n",
      "  )\n",
      "  (readouts): ModuleList(\n",
      "    (0-1): 2 x LinearReadoutBlock(\n",
      "      (linear): Linear(64x0e+64x1o+64x2e -> 1x0e | 64 weights)\n",
      "    )\n",
      "    (2): NonLinearReadoutBlock(\n",
      "      (linear_1): Linear(64x0e -> 16x0e | 1024 weights)\n",
      "      (non_linearity): Activation [x] (16x0e -> 16x0e)\n",
      "      (linear_2): Linear(16x0e -> 1x0e | 16 weights)\n",
      "    )\n",
      "  )\n",
      "  (scale_shift): ScaleShiftBlock(scale=2.293835, shift=0.000000)\n",
      ")\n",
      "2025-06-03 22:07:58.869 INFO: Number of parameters: 405008\n",
      "2025-06-03 22:07:58.869 INFO: Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: embedding\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 1\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 2\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: interactions_no_decay\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      "\n",
      "Parameter Group 3\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: products\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 5e-07\n",
      "\n",
      "Parameter Group 4\n",
      "    amsgrad: True\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    initial_lr: 0.01\n",
      "    lr: 0.01\n",
      "    maximize: False\n",
      "    name: readouts\n",
      "    swa_lr: 0.001\n",
      "    weight_decay: 0.0\n",
      ")\n",
      "2025-06-03 22:07:58.869 INFO: Using gradient clipping with tolerance=10.000\n",
      "2025-06-03 22:07:58.869 INFO: Started training\n",
      "2025-06-03 22:08:05.467 INFO: Epoch None: loss=52.9404, RMSE_E_per_atom=6129.9 meV, RMSE_F=2241.3 meV / A\n",
      "2025-06-03 22:09:50.796 INFO: Epoch 0: loss=0.7305, RMSE_E_per_atom=207.0 meV, RMSE_F=269.4 meV / A\n",
      "2025-06-03 22:13:11.889 INFO: Epoch 2: loss=0.3819, RMSE_E_per_atom=126.6 meV, RMSE_F=196.0 meV / A\n",
      "2025-06-03 22:16:32.811 INFO: Epoch 4: loss=0.1999, RMSE_E_per_atom=97.8 meV, RMSE_F=142.1 meV / A\n",
      "2025-06-03 22:19:54.102 INFO: Epoch 6: loss=0.3720, RMSE_E_per_atom=122.1 meV, RMSE_F=194.8 meV / A\n",
      "2025-06-03 22:23:15.448 INFO: Epoch 8: loss=0.2366, RMSE_E_per_atom=106.7 meV, RMSE_F=154.7 meV / A\n",
      "2025-06-03 22:26:36.222 INFO: Epoch 10: loss=0.2776, RMSE_E_per_atom=101.3 meV, RMSE_F=166.8 meV / A\n",
      "2025-06-03 22:29:56.875 INFO: Epoch 12: loss=0.1290, RMSE_E_per_atom=99.1 meV, RMSE_F=114.1 meV / A\n",
      "2025-06-03 22:33:17.878 INFO: Epoch 14: loss=0.2420, RMSE_E_per_atom=84.1 meV, RMSE_F=155.4 meV / A\n",
      "2025-06-03 22:36:39.097 INFO: Epoch 16: loss=0.0906, RMSE_E_per_atom=86.1 meV, RMSE_F=94.7 meV / A\n",
      "2025-06-03 22:40:00.319 INFO: Epoch 18: loss=0.0935, RMSE_E_per_atom=78.7 meV, RMSE_F=96.5 meV / A\n",
      "2025-06-03 22:43:20.933 INFO: Epoch 20: loss=0.0882, RMSE_E_per_atom=69.7 meV, RMSE_F=93.9 meV / A\n",
      "2025-06-03 22:46:42.401 INFO: Epoch 22: loss=0.1096, RMSE_E_per_atom=63.1 meV, RMSE_F=104.3 meV / A\n",
      "2025-06-03 22:50:03.543 INFO: Epoch 24: loss=0.0613, RMSE_E_per_atom=66.7 meV, RMSE_F=78.2 meV / A\n",
      "2025-06-03 22:54:21.469 INFO: Epoch 26: loss=0.1090, RMSE_E_per_atom=70.1 meV, RMSE_F=104.2 meV / A\n",
      "2025-06-03 22:57:41.640 INFO: Epoch 28: loss=0.1805, RMSE_E_per_atom=54.2 meV, RMSE_F=134.2 meV / A\n",
      "2025-06-03 23:01:02.338 INFO: Epoch 30: loss=0.0590, RMSE_E_per_atom=55.6 meV, RMSE_F=76.9 meV / A\n",
      "2025-06-03 23:04:23.517 INFO: Epoch 32: loss=0.1393, RMSE_E_per_atom=53.7 meV, RMSE_F=118.8 meV / A\n",
      "2025-06-03 23:07:44.363 INFO: Epoch 34: loss=0.1299, RMSE_E_per_atom=63.6 meV, RMSE_F=114.7 meV / A\n",
      "2025-06-03 23:09:22.963 INFO: Changing loss based on SWA\n",
      "2025-06-03 23:11:05.435 INFO: Epoch 36: loss=0.0324, RMSE_E_per_atom=4.7 meV, RMSE_F=55.0 meV / A\n",
      "2025-06-03 23:14:26.634 INFO: Epoch 38: loss=0.0279, RMSE_E_per_atom=2.9 meV, RMSE_F=52.1 meV / A\n",
      "2025-06-03 23:17:47.757 INFO: Epoch 40: loss=0.0273, RMSE_E_per_atom=4.4 meV, RMSE_F=50.5 meV / A\n",
      "2025-06-03 23:21:09.253 INFO: Epoch 42: loss=0.0254, RMSE_E_per_atom=2.5 meV, RMSE_F=49.8 meV / A\n",
      "2025-06-03 23:24:30.619 INFO: Epoch 44: loss=0.0248, RMSE_E_per_atom=2.2 meV, RMSE_F=49.3 meV / A\n",
      "2025-06-03 23:27:51.444 INFO: Epoch 46: loss=0.0242, RMSE_E_per_atom=2.1 meV, RMSE_F=48.8 meV / A\n",
      "2025-06-03 23:31:12.702 INFO: Epoch 48: loss=0.0239, RMSE_E_per_atom=2.3 meV, RMSE_F=48.4 meV / A\n",
      "2025-06-03 23:32:51.551 INFO: Training complete\n",
      "2025-06-03 23:32:51.552 INFO: Computing metrics for training, validation, and test sets\n",
      "2025-06-03 23:32:52.016 INFO: Loading checkpoint: MACE_models/mace_learncurve_train4000_run-123_epoch-30.pt\n",
      "2025-06-03 23:32:52.062 INFO: Loaded model from epoch 30\n",
      "2025-06-03 23:32:52.063 INFO: Evaluating train ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/mace/tools/checkpoint.py:187: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  torch.load(f=checkpoint_info.path, map_location=device),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-03 23:33:29.133 INFO: Evaluating valid ...\n",
      "2025-06-03 23:33:33.028 INFO: Evaluating Default ...\n",
      "2025-06-03 23:33:42.768 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         58.7        |       79.8       |        3.48       |\n",
      "|    valid    |         55.6        |       76.9       |        3.50       |\n",
      "|   Default   |         57.6        |       80.9       |        3.53       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 23:33:42.769 INFO: Saving model to MACE_models/mace_learncurve_train4000_run-123.model\n",
      "2025-06-03 23:33:43.002 INFO: Compiling model, saving metadata to MACE_models/mace_learncurve_train4000_compiled.model\n",
      "2025-06-03 23:33:44.252 INFO: Loading checkpoint: MACE_models/mace_learncurve_train4000_run-123_epoch-48_swa.pt\n",
      "2025-06-03 23:33:44.437 INFO: Loaded model from epoch 48\n",
      "2025-06-03 23:33:44.438 INFO: Evaluating train ...\n",
      "2025-06-03 23:34:21.241 INFO: Evaluating valid ...\n",
      "2025-06-03 23:34:25.122 INFO: Evaluating Default ...\n",
      "2025-06-03 23:34:34.849 INFO: \n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "| config_type | RMSE E / meV / atom | RMSE F / meV / A | relative F RMSE % |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "|    train    |         2.4         |       38.8       |        1.69       |\n",
      "|    valid    |         2.3         |       48.4       |        2.21       |\n",
      "|   Default   |         2.5         |       47.8       |        2.09       |\n",
      "+-------------+---------------------+------------------+-------------------+\n",
      "2025-06-03 23:34:34.850 INFO: Saving model to MACE_models/mace_learncurve_train4000_run-123_swa.model\n",
      "2025-06-03 23:34:35.086 INFO: Compiling model, saving metadata MACE_models/mace_learncurve_train4000_swa_compiled.model\n",
      "2025-06-03 23:34:36.309 INFO: Done\n",
      "âœ… Found: RMSE_E = 2.3 meV, RMSE_F = 48.4 meV/Ã…\n"
     ]
    }
   ],
   "source": [
    "# Save results\n",
    "results = []\n",
    "\n",
    "for size in train_sizes:\n",
    "    name = f\"train{size}\"\n",
    "    config = base_config.copy()\n",
    "    config['name'] = f\"mace_learncurve_{name}\"\n",
    "    config['train_file'] = f\"data/solvent_xtb_train_{size}.xyz\"\n",
    "\n",
    "    cfg_path = f\"benchmark_results/{config['name']}.yaml\"\n",
    "    Path(\"benchmark_results\").mkdir(exist_ok=True)\n",
    "\n",
    "    with open(cfg_path, 'w') as f:\n",
    "        yaml.dump(config, f)\n",
    "\n",
    "    print(f\"\\nðŸš€ Training started: {config['name']}\")\n",
    "    start = time.time()\n",
    "    subprocess.run(['mace_run_train', '--config', cfg_path])\n",
    "    runtime = time.time() - start\n",
    "\n",
    "    log_path = f\"MACE_models/{config['name']}_run-123.log\"\n",
    "    rmse_e, rmse_f = parse_log_file(log_path)\n",
    "\n",
    "    results.append({\n",
    "        'config': config['name'],\n",
    "        'train_size': size,\n",
    "        'train_time_s': round(runtime, 2),\n",
    "        'rmse_e_meV': rmse_e,\n",
    "        'rmse_f_meV': rmse_f\n",
    "    })\n",
    "\n",
    "# Save\n",
    "df_lcurve = pd.DataFrame(results)\n",
    "df_lcurve.sort_values(\"train_size\")\n",
    "df_lcurve.to_csv(\"benchmark_results/learning_curve_results.csv\", index=False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f56fafa-373b-49e3-8a4b-3704084f0b70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df_lcurve = pd.read_csv(\"benchmark_results/learning_curve_results.csv\")\n",
    "\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(df_lcurve[\"train_size\"], df_lcurve[\"rmse_f_meV\"], marker='o', label=\"Force RMSE\")\n",
    "plt.plot(df_lcurve[\"train_size\"], df_lcurve[\"rmse_e_meV\"], marker='s', label=\"Energy RMSE\")\n",
    "plt.xlabel(\"Training set size\")\n",
    "plt.ylabel(\"RMSE [meV]\")\n",
    "plt.title(\"Learning curve: test error vs. training set size\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(\"benchmark_results/learning_curve.png\", dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6f6394-9be6-42cc-9c8f-7bd78691e72f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
